[
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "AWS services scale to new heights for Prime Day 2025: key metrics and milestones Amazon Prime Day 2025 was the biggest Amazon Prime Day shopping event ever, setting records for both sales volume and total items sold during the 4-day event. Prime members saved billions while shopping Amazon‚Äôs millions of deals during the event.\nThis year marked a significant transformation in the Prime Day experience through advancements in the generative AI offerings from Amazon and AWS. Customers used Alexa+‚Äîthe Amazon next-generation personal assistant now available in early access to millions of customers‚Äîalong with the AI-powered shopping assistant, Rufus, and AI Shopping Guides. These features, built on more than 15 years of cloud innovation and machine learning expertise from AWS, combined with deep retail and consumer experience from Amazon, helped customers quickly discover deals and get product information, complementing the fast, free delivery that Prime members enjoy year-round.\nAs part of our annual tradition to tell you about how AWS powered Prime Day for record-breaking sales, I want to share the services and chart-topping metrics from AWS that made your amazing shopping experience possible.\nPrime Day 2025 ‚Äì all the numbers During the weeks leading up to big shopping events like Prime Day, Amazon fulfillment centers and delivery stations work to get ready and ensure operations run efficiently and safely. For example, the Amazon automated storage and retrieval system (ASRS) operates a global fleet of industrial mobile robots that move goods around Amazon fulfillment centers.\nAWS Outposts, a fully managed service that extends the AWS experience on-premises, powers software applications that manage the command-and-control of Amazon ASRS and supports same-day and next-day deliveries through low-latency processing of critical robotic commands.\nDuring Prime Day 2025, AWS Outposts at one of the largest Amazon fulfillment centers sent more than 524 million commands to over 7,000 robots, reaching peak volumes of 8 million commands per hour‚Äîa 160 percent increase compared to Prime Day 2024.\nHere are some more interesting, mind-blowing metrics:\nAmazon Elastic Compute Cloud (Amazon EC2) ‚Äì During Prime Day 2025, AWS Graviton, a family of processors designed to deliver the best price performance for cloud workloads running in Amazon EC2, powered more than 40 percent of the Amazon EC2 compute used by Amazon.com. Amazon also deployed over 87,000 AWS Inferentia and AWS Trainium chips ‚Äì custom silicon chips for deep learning and generative AI training and inference ‚Äì to power Amazon Rufus for Prime Day.\nAmazon SageMaker AI ‚Äî Amazon SageMaker AI, a fully managed service that brings together a broad set of tools to enable high-performance, low-cost machine learning (ML), processed more than 626 billion inference requests during Prime Day 2025.\nAmazon Elastic Container Service (Amazon ECS) and AWS Fargate‚Äì Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that works seamlessly with AWS Fargate, a serverless compute engine for containers. During Prime Day 2025, Amazon ECS launched an average of 18.4 million tasks per day on AWS Fargate, representing a 77 percent increase from the previous year‚Äôs Prime Day average.\nAWS Fault Injection Service (AWS FIS) ‚Äì We ran over 6,800 AWS FIS experiments‚Äîover eight times more than we conducted in 2024‚Äîto test resilience and ensure Amazon.com remains highly available on Prime Day. This significant increase was made possible by two improvements: new Amazon ECS support for network fault injection experiments on AWS Fargate, and the integration of FIS testing in continuous integration and continuous delivery (CI/CD) pipelines.\nAWS Lambda ‚Äì AWS Lambda, a serverless compute service that lets you run code without managing infrastructure, handled over 1.7 trillion invocations per day during Prime Day 2025.\nAmazon API Gateway ‚Äì During Prime Day 2025, Amazon API Gateway, a fully managed service that makes it easy to create, maintain, and secure APIs at any scale, processed over 1 trillion internal service requests‚Äîa 30 percent increase in requests on average per day compared to Prime Day 2024.\nAmazon CloudFront ‚Äì Amazon CloudFront, a content delivery network (CDN) service that securely delivers content with low latency and high transfer speeds, delivered over 3 trillion HTTP requests during the global week of Prime Day 2025, a 43 percent increase in requests compared to Prime Day 2024.\nAmazon Elastic Block Store (Amazon EBS) ‚Äì During Prime Day 2025, Amazon EBS, our high-performance block storage service, peaked at 20.3 trillion I/O operations, moving up to an exabyte of data daily.\nAmazon Aurora ‚Äì On Prime Day, Amazon Aurora, a relational database management system (RDBMS) built for high performance and availability at global scale for PostgreSQL, MySQL, and DSQL, processed 500 billion transactions, stored 4,071 terabytes of data, and transferred 999 terabytes of data.\nAmazon DynamoDB ‚Äì Amazon DynamoDB, a serverless, fully managed, distributed NoSQL database, powers multiple high-traffic Amazon properties and systems including Alexa, the Amazon.com sites, and all Amazon fulfillment centers. Over the course of Prime Day, these sources made tens of trillions of calls to the DynamoDB API. DynamoDB maintained high availability while delivering single-digit millisecond responses and peaking at 151 million requests per second.\nAmazon ElastiCache ‚Äì During Prime Day, Amazon ElastiCache, a fully managed caching service delivering microsecond latency, peaked at serving over 1.5 quadrillion daily requests and over 1.4 trillion requests in a minute.\nAmazon Kinesis Data Streams ‚Äì Amazon Kinesis Data Streams, a fully managed serverless data streaming service, processed a peak of 807 million records per second during Prime Day 2025.\nAmazon Simple Queue Service (Amazon SQS) ‚Äì During Prime Day 2025, Amazon SQS ‚Äì a fully managed message queuing service for microservices, distributed systems, and serverless applications ‚Äì set a new peak traffic record of 166 million messages per second.\nAmazon GuardDuty ‚Äì During Prime Day 2025, Amazon GuardDuty, an intelligent threat detection service, monitored an average of 8.9 trillion log events per hour, a 48.9 percent increase from last year‚Äôs Prime Day.\nAWS CloudTrail ‚Äì AWS CloudTrail, which tracks user activity and API usage on AWS, as well as in hybrid and multicloud environments, processed over 2.5 trillion events during Prime Day 2025, compared to 976 billion events in 2024.\nPrepared to scale\nIf you‚Äôre preparing for similar business-critical events, product launches, and migrations, I recommend that you take advantage of our newly branded AWS Countdown (formerly known as AWS Infrastructure Event Management, or IEM). This comprehensive support program helps assess operational readiness, identify and mitigate risks, and plan capacity, using proven playbooks developed by AWS experts. We‚Äôve expanded to include: generative AI implementation support to help you confidently launch and scale AI initiatives; migration and modernization support, including mainframe modernization; and infrastructure optimization for specialized sectors including election systems, retail operations, healthcare services, and sports and gaming events.\nI look forward to seeing what other records will be broken next year!\n‚Äî Channy\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\rAWS named as a Leader in 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms and Container Management A month ago, I shared that Amazon Web Services (AWS) is recognized as a Leader in 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services (SCPS), with Gartner naming AWS a Leader for the fifteenth consecutive year.\nIn 2024, AWS was named as a Leader in the Gartner Magic Quadrant for AI Code Assistants, Cloud-Native Application Platforms, Cloud Database Management Systems, Container Management, Data Integration Tools, Desktop as a Service (DaaS), and Data Science and Machine Learning Platforms as well as the SCPS. In 2025, we were also recognized as a Leader in the Gartner Magic Quadrant for Contact Center as a Service (CCaaS), Desktop as a Service and Data Science and Machine Learning (DSML) platforms. We strongly believe this means AWS provides the broadest and deepest range of services to customers.\nToday, I‚Äôm happy to share recent Magic Quadrant reports that named AWS as a Leader in more cloud technology markets: Cloud-Native Application Platforms (aka Cloud Application Platforms) and Container Management.\n2025 Gartner Magic Quadrant for Cloud-Native Application Platforms AWS has been named a Leader in the Gartner Magic Quadrant for Cloud-Native Application Platforms for 2 consecutive years. AWS was positioned highest on ‚ÄúAbility to Execute‚Äù. Gartner defines cloud-native application platforms as those that provide managed application runtime environments for applications and integrated capabilities to manage the lifecycle of an application or application component in the cloud environment. The following image is the graphical representation of the 2025 Magic Quadrant for Cloud-Native Application Platforms.\nOur comprehensive cloud-native application portfolio‚ÄîAWS Lambda, AWS App Runner, AWS Amplify, and AWS Elastic Beanstalk‚Äîoffers flexible options for building modern applications with strong AI capabilities, demonstrated through continued innovation and deep integration across our broader AWS service portfolio.\nYou can simplify the service selection through comprehensive documentation, reference architectures, and prescriptive guidance available in the AWS Solutions Library, along with AI-powered, contextual recommendations from Amazon Q based on your specific requirements. While AWS Lambda is optimized for AWS to provide the best possible serverless experience, it follows industry standards for serverless computing and supports common programming languages and frameworks. You can find all necessary capabilities within AWS, including advanced features for AI/ML, edge computing, and enterprise integration.\nYou can build, deploy, and scale generative AI agents and applications by integrating these compute offerings with Amazon Bedrock for serverless inferences and Amazon SageMaker for artificial intelligence and machine learning (AI/ML) training and management.\nAccess the complete 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms to learn more.\n2025 Gartner Magic Quadrant for Container Management In the 2025 Gartner Magic Quadrant for Container Management, AWS has been named as a Leader for three years and was positioned furthest for ‚ÄúCompleteness of Vision‚Äù. Gartner defines container management as offerings that support the deployment and operation of containerized workloads. This process involves orchestrating and overseeing the entire lifecycle of containers, covering deployment, scaling, and operations, to ensure their efficient and consistent performance across different environments.\nThe following image is the graphical representation of the 2025 Magic Quadrant for Container Management.\nAWS container services offer fully managed container orchestration with AWS native solutions and open-source technologies to focus on providing a wide range of deployment options, from Kubernetes to our native orchestrator.\nYou can use Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS). Both can be used with AWS Fargate for serverless container deployment. Additionally, EKS Auto Mode simplifies Kubernetes management by automatically provisioning infrastructure, selecting optimal compute instances, and dynamically scaling resources for containerized applications.\nYou can connect on-premises and edge infrastructure back to AWS container services with EKS Hybrid Nodes and ECS Anywhere, or use EKS Anywhere for a fully disconnected Kubernetes experience supported by AWS. With flexible compute and deployment options, you can reduce operational overhead and focus on innovation and drive business value faster.\nAccess the complete 2025 Gartner Magic Quadrant for Container Management to learn more.\n‚Äî Channy\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúData Science on AWS‚Äù Event Objectives Explore how data challenges can be solved through AWS Services Overview of Managed AI Services and their use cases Data preparation with Amazon SageMaker Using XGBoost with SageMaker Studio Notebooks No-code AutoML with SageMaker Canvas Speakers Van Hoang Kha ‚Äì Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong ‚Äì Cloud DevOps Engineer, AWS Community Builder Doan Nguyen Thanh Hoa ‚Äì CF Lecturer, FPT University HCMC Key Highlights Amazon Comprehend and Amazon Translate Deep learning-powered text analysis and translation\nProcesses various document types (emails, chats, social media, phone calls, etc.) and extracts insights automatically. Typical Comprehend use cases: Intelligent document processing Automated email workflows Customer support ticket routing Document and media tagging Sentiment analysis Contact center call analysis PII (Personally Identifiable Information) detection and redaction Amazon Translate Neural machine translation service\nKey Features:\nBroad language coverage: 4970 X‚ÜîY translation combinations Low latency: \u0026lt;150 ms/sentence on average Data security: Full encryption and ownership retention Regional coverage: Available in 17 AWS regions Customizable translation: Custom Terminologies and Active Custom Translation Batch translation: Supports DOCX, PPTX, XLSX, XML, HTML, and text files Broad domain coverage: Trained on 11 domains Pay-per-use: Simple API model Use Cases:\nLocalization: Enterprise content, media subtitling, archives Communication: Customer engagement, in-game chat, social media Text analytics: Voice of Customer, media analysis, eDiscovery Amazon Polly Text-to-speech (TTS) service\nAmazon Polly converts text into lifelike speech using deep learning.\nFeatures:\nText-to-speech (TTS) Speech Synthesis Markup Language (SSML) Custom Lexicons Speech Marks Brand Voice Common Use Cases:\nVoiced news articles and training Telephony/IVR systems Podcasts and language learning Navigation, reminders, accessibility tools Amazon Transcribe Automatic Speech Recognition (ASR) service\nConverts recorded media into text Supports real-time transcription for live streaming content Amazon Lex Conversational AI service\nBuilds voice and text-based chatbots.\nHighlights:\nEasy to use High-quality natural language understanding Built-in integration with AWS services Cost-effective Amazon Rekognition Provides image and video analysis for object detection, facial recognition, and scene understanding.\nAmazon Personalize Delivering personalized customer experiences\nFast deployment of recommendation systems Real-time response to user behavior Easy integration with existing systems Managed ML reduces time-to-market Feature Engineering Data Preparation with Amazon SageMaker Canvas Applying to Work ‚Äî Lessons Derived from Key Highlights 1. Data Understanding \u0026amp; Automation (Amazon Comprehend, Translate) Lesson learned:\nThe ability to extract insights from unstructured data is crucial for smarter decision-making.\nApplication:\nUse Amazon Comprehend for customer feedback sentiment analysis and document classification. Automate email or ticket routing based on detected intent and sentiment. Apply Amazon Translate for multilingual projects and global content localization. 2. Customer Engagement \u0026amp; Voice Interface (Amazon Polly, Lex, Transcribe) Lesson learned:\nVoice and conversational AI make digital experiences more accessible and user-friendly.\nApplication:\nCombine Lex, Polly, and Transcribe to build a voice-enabled chatbot for 24/7 support. Use Polly to generate natural-sounding voiceovers for e-learning materials. Leverage Transcribe to capture and analyze customer service calls and meeting notes. 3. Image \u0026amp; Video Intelligence (Amazon Rekognition) Lesson learned:\nComputer vision enables automation and deeper media insights.\nApplication:\nUse Rekognition to auto-tag and moderate multimedia content. Implement facial recognition for access control or customer analysis in retail environments. 4. Personalized Experience (Amazon Personalize) Lesson learned:\nPersonalization drives engagement and retention.\nApplication:\nIntegrate Personalize to recommend content, services, or products based on user behavior. Build real-time recommendation systems that evolve as user intent changes. 5. Machine Learning Simplification (Amazon SageMaker, Canvas) Lesson learned:\nMachine learning is no longer limited to experts‚ÄîAWS simplifies the end-to-end ML lifecycle.\nApplication:\nUse SageMaker Canvas for no-code model training and predictions. Apply SageMaker Studio for algorithm experimentation (e.g., XGBoost). Improve data quality through feature engineering to enhance model performance. 6. Data-driven Modernization Mindset Lesson learned:\nAI and AWS services promote a shift toward data-centric and event-driven architecture.\nApplication:\nCombine DDD (Domain-Driven Design) with AI workflows to design modular, adaptive systems. Use serverless computing (Lambda, API Gateway) for scalable AI-driven processes. Introduce event-driven patterns to handle asynchronous data flows efficiently. üåü Overall Reflection Through this AWS Data Science event, I learned not just about individual AI services but also about how to connect them into a cohesive, data-driven ecosystem.\nEach tool‚ÄîComprehend, Translate, Polly, Rekognition, Personalize, and SageMaker‚Äîrepresents a piece of a larger puzzle: building intelligent, automated, and human-centered applications.\nThese lessons are directly applicable to my projects, helping me move from manual workflows to AI-assisted automation and smarter user experiences.\nEvent Photos "
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúGenerative AI with Amazon Bedrock‚Äù Event Objectives Learn what Amazon Bedrock is and how many model that AI used Learn more Prompting Engineering like: Zero-Shot, Few-Shot, Chain of Thought(CoT) Learn what Retrieval Augmented Generation is and RAG use cases More pretrained AI Services Learn what Amazon Bedrock agent core and demo Speakers Lam Tuan Kiet ‚Äì Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi ‚Äì AI Engineer, Renova Cloud Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee, First Cloud AI Journey Key Highlights Foundation Models Prompt Engineering Retrieval-Augmented Generation(RAG) Bedrock AgentCore Foundation Models Tradional ML models Foundation models Purpose Narrow, task-specific(Classification, regression,forecasting,clustering,etc\u0026hellip;) General-purpose, multi-domain, multi-task Training data Usually structured/tabular or domain-specific Massive datasets (text, images, code, etc.) Training required Must be trained manually using labeled data None ‚Äî already pretrained Customization You need to adjust hyperparameters, features, datasets Optional ‚Äî fine-tuning, RAG, prompt engineering Example Random Forest, XGBoost, SVM, Linear Regression Claude, Llama, Cohere Command, Titan Skill Needed High ML expertise (feature engineering, training pipelines) Very low ‚Äî prompt-based usage AWS Services SageMaker, Amazon ML, custom training Amazon Bedrock, Bedrock Studio Use cases Predicting, Forecasting sales, Classifying images, Fraud training, Anomaly detection Chatbots, Text summarization, Code generation, Image generation, Conversational apps, Document search with RAG, Sentiment analysis, Multi-language translation Supported foundation models in Amazon Bedrock Prompt Engineering - Crafting and refining instructions(prompts) What is a prompt? Is the input you give to an AI model (like ChatGPT, Claude, Gemini, or models on Amazon Bedrock) to get a specific output. Example What is difference between Zero-Shot, Few-Shot and Chain of Thought in Prompting Techniques? Technique Description When to Use Advantages Limitations Example Prompt Style Zero-Shot Model gets only the instruction with no examples. Simple tasks; model already understands domain. Short prompts, fast, minimal setup. Lower accuracy for complex or domain-specific tasks. \u0026ldquo;Classify the text into neutral, negative or positive\u0026rdquo; Few-Shot Provide a few examples to teach the pattern. Custom formats; domain tasks; pattern learning. Higher accuracy; model learns your desired style. Longer prompt; can become costly at scale. \u0026ldquo;A whatpu is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus. \u0026quot; Chain-of-Thought Ask model to explain reasoning step-by-step. Math, logic, coding, planning, reasoning tasks. Better reasoning, more accurate complex outputs. Slower; may generate unnecessarily long answers. \u0026ldquo;Show your step-by-step reasoning.\u0026rdquo; Retrieval Augmented Generation (RAG)- Retrieving relevant information from a data source What is Retrieval Augmented Generation? Retrieval: Fetches the relevant content from the external knowledge base or data sources based on a user query Augmentation: Adding the retrieved relevant context to the user prompt, which goes as an input to the foundation model Generation: Response from the foundation model based on the augmented prompt RAG Use cases Improved content quality: E.g Helps in reducing hallucinations and connecting with recent knowledge including enterprise data Contextual chatbots and question answering: E.g Enhance chatbot capabilities by integrating with real-time data Personalized search: E.g Searching based on user previous search history and persona Real-time data summarization: E.g Retrieving and summarizing transactional data from databases, or API calls\nWhat are embeddings? Numerical representation of text(vectors) that captures semantics and relationships between words\nEmbedding models capture features and nuances of the text\nRich embeddings can be used to compare text similarity\nMultilingual Text Embeddings can identity meaning in different languages\nAmazon Titan Embedding:\nTitan Text Embeddings: Translates text inputs(words,phrases) into numerical representations(Embeddings). Comparing embeddings produces more relevant and contextual responses that word matching Highlights:\nAmazon Titan Text Embeddings V2 is a light weight, efficient model ideal for high accuracy retrieval tasks at different dimensions. The model supports flexible embeddings sizes and prioritizes accuracy maintenance at smaller dimension sizes. Support for 100+ language in pre-training as well unit vector normalization for improving accuracy of measuring vector similarity. RAG in Action Data Ingestion Workflow RetrieveAndGenerate API\nOther Pretrained AI Services (hinh anh amazon polly) (hinh anh amazon comprehend) (hinh anh amazon kendra) (hinh anh amazon lookout family) (hinh anh amazon personalize) Amazon Bedrock AgentCore The evolution into Agentic Generative AI assistants: Follow a set of rules, Automate repetitive tasks Generative AI agents: Achieve a singular goal, address broader range of tasks, automate entire workflows Agentic AI systems: Fully autonomous, multi-agent systems, mimic human logic and reasoning Frameworks for building agents Strands agents SDK Langgraph, langchain OpenAi Agents SDK Crew.AI Google ADK Llamaindex The protype to production \u0026ldquo;chasm\u0026rdquo; Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Getting agents to productions in still too hard Securely execute and scale agent code Remember past interaction + learning Identity and access controls for all agents and tools Agentic tool use for executing complex workflows Discover and connect with custom tools and resources Understand and audit every interaction Agent core services enabling agents at scale Event Experience Attending the ‚ÄúGenerative AI with Amazon Bedrock‚Äù workshop was a highly valuable experience, giving me a comprehensive understanding of how modern AI and cloud technologies are applied in real-world scenarios. The event combined expert insights, hands-on demonstrations, and practical guidance that helped connect theory with implementation.\nLearning from industry experts Speakers from AWS, Renova Cloud, and FPT Software shared real-world best practices for AI adoption and cloud modernization. Their case studies demonstrated how organizations apply foundation models, RAG, and agent-based architectures in enterprise systems. The discussions helped clarify not just how these technologies work, but why they matter for scalability and business innovation. Hands-on technical exposure Practical sessions on Zero-Shot, Few-Shot, and Chain-of-Thought prompting improved my understanding of effective prompt engineering. The walkthrough of RAG workflows showed how grounding AI responses in real data reduces hallucinations and improves accuracy. Live demos of Bedrock AgentCore helped me visualize how AI agents plan tasks, execute tools, and integrate securely with AWS services. Understanding modernization and architecture The workshop highlighted the shift from traditional ML to foundation-model-driven AI and explained the differences in training, customization, and usage. I learned how organizations modernize systems using: Event-driven architecture Domain-driven design (DDD) AI-augmented workflows These approaches helped me understand how to design applications that are scalable, resilient, and intelligent. Exposure to modern AI tools Demonstrations of Amazon Titan Embeddings, Textract, Comprehend, Kendra, Polly, and other AI services showed how to quickly integrate intelligence into applications. Learning about Amazon Q Developer highlighted how AI can support the entire software lifecycle‚Äîfrom planning to coding and refactoring. Networking and knowledge exchange Discussions with cloud engineers, AI specialists, and developers broadened my understanding of industry use cases and challenges. These conversations reinforced the importance of aligning business objectives, data modeling, and technology decisions. Key takeaways RAG enables enterprise-grade AI by grounding answers in verified data. Prompt engineering matters‚Äîwell-structured prompts lead to more reliable results. Agentic AI systems represent the next evolution of automation, capable of reasoning and executing multi-step workflows. System modernization requires a phased roadmap and clear ROI measurement. Integrating AI tools into the developer workflow can significantly boost productivity and reduce operational overhead. "
},
{
	"uri": "http://localhost:1313/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Tao Bao Thanh\nPhone Number: 0901452366\nEmail: taobaothanh365@gmail.com\nUniversity: Ho Chi Minh FPT University\nMajor: Software Engineering\nClass: OJT202\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services and how to use the AWS Console \u0026amp; CLI. Tasks to be Carried Out This Week Day Task Start Date Completion Date Reference Material 2 Make friends with FCJ members.\nPay attention to rules for job training.\nPractice:\n+ Create AWS Free Tier account 08/09/2025 09/09/2025 ‚Äî 3 Learn AWS \u0026amp; AWS Services:\n+ EC2\n+ Lambda\n+ SQS\n+ SNS\n+ CLI \u0026amp; SDKs 09/09/2025 10/09/2025 ‚Äî 4 Learn AWS ECS, AWS EKS, VPC, CloudFormation.\nLearn Cost Management.\nPractice:\n+ Create EC2 instance\n+ Create Lambda function\n+ Use AWS CLI 10/09/2025 11/09/2025 ‚Äî 5 Learn VPC Basics and S3.\nPractice:\n+ Create Security Group\n+ Create Internet Gateway\n+ Create Subnet\n+ Create Route Table 11/09/2025 12/09/2025 ‚Äî 6 Practice:\n+ Launch S3 Bucket 12/09/2025 13/09/2025 ‚Äî Week 1 Achievements 1. AWS Foundations Understood the core AWS service groups: Compute, Storage, Networking, Database. Successfully created and configured an AWS Free Tier account. Became familiar with the AWS Management Console and learned how to navigate services. 2. AWS CLI Skills Performed basic operations using AWS CLI: Check account \u0026amp; configuration details Retrieve list of regions View EC2 information Create/manage Key Pairs Check running services "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives Learn about Amazon RDS, AWS S3, AWS EC2 Auto Scaling, AWS DynamoDB, and Amazon Lightsail. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Learned AWS DynamoDB, AWS RDS, and Amazon Aurora. - Practice: + Created AWS CloudFront and AWS S3. 14/09/2025 15/09/2025 https://render.skillbuilder.aws/?module_id=B1DGAV8S16%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX/ 3 Practice: + Created AWS RDS. + Created DynamoDB using Python and AWS CLI. 15/09/2025 16/09/2025 https://render.skillbuilder.aws/?module_id=B1DGAV8S16%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 4 Translated blog articles. 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learned fundamentals of EC2 Auto Scaling and Amazon Lightsail.\n- Practice: + Created a database in AWS RDS.\n+ Used Amazon Lightsail. 19/09/2025 20/09/2025 https://000005.awsstudygroup.com/ https://000045.awsstudygroup.com/ 6 Practice: + Created RDS, VPC, and EC2 with AWS EC2 Auto Scaling. 20/09/2025 21/09/2025 https://000006.awsstudygroup.com/ Week 2 Achievements 1. Static Website Hosting with Amazon S3 Learned how to create an S3 bucket, upload files, and manage access. Enabled Static Website Hosting on an S3 bucket. Configured public access so the website can be accessed over the Internet. Set up an appropriate Bucket Policy to allow public read-only access. 2. Database Essentials with Amazon RDS Understand that RDS supports many popular engines such as MySQL, PostgreSQL, Oracle, and SQL Server. Learned how to connect an EC2 instance to an RDS database. Identified key connection information: endpoint, port, username, and password. Created and configured VPC, subnets, and security groups to secure an RDS instance. 3. Scaling Applications with EC2 Auto Scaling Understood that Auto Scaling automatically increases or decreases EC2 instances based on demand. Learned core components of Auto Scaling: Launch Configuration / Launch Template Auto Scaling Group (ASG) Scaling Policies: Dynamic Scaling and Scheduled Scaling Health Checks Used Amazon CloudWatch to monitor metrics such as CPU utilization, network traffic, and request count to support scaling decisions. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives Understand AWS security fundamentals. Learn key security-related services such as AWS KMS, Amazon Macie, and AWS Certificate Manager. Understand how to protect AWS infrastructure. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Learned data protection using AWS built-in capabilities (Amazon S3, Amazon EBS, Amazon DynamoDB) and AWS security services (AWS KMS, Amazon Macie, AWS Certificate Manager). 22/09/2025 23/09/2025 https://render.skillbuilder.aws/?module_id=TNAPD78T9R%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 3 Learned how to protect networks and applications using AWS infrastructure security (Security Groups, ELB, AWS Regions) and AWS protection services (AWS Shield, AWS WAF). 23/09/2025 24/09/2025 Same as above 4 Studied threat detection and incident response using Amazon Inspector, Amazon GuardDuty, Amazon Detective, and AWS Security Hub. 24/09/2025 25/09/2025 Same as above 5 Learned how to prevent unauthorized access using additional services such as IAM Identity Center, AWS Secrets Manager, and AWS Systems Manager. 25/09/2025 26/09/2025 Same as above 6 Practice: + Implemented Identity Federation using AWS Single Sign-On (AWS IAM Identity Center). 26/09/2025 29/09/2025 https://000012.awsstudygroup.com/ Week 3 Achievements 1. Data Protection and Governance Understood: Encryption \u0026amp; key management using AWS KMS Sensitive data discovery using Amazon Macie Certificate lifecycle management with AWS Certificate Manager Built-in security features of S3, EBS, and DynamoDB 2. Network and Application Security Learned layered security concepts: Security Groups Elastic Load Balancers (ELB) Multi-Region protection strategies Understood how AWS Shield and AWS WAF mitigate DDoS and web attacks. 3. Threat Detection \u0026amp; Incident Response Understood the roles of: Amazon Inspector (vulnerability scanning) Amazon GuardDuty (threat detection) Amazon Detective (investigation \u0026amp; analysis) AWS Security Hub (centralized security posture) 4. Preventing Unauthorized Access Learned fundamentals of: IAM Identity Center (SSO) AWS Secrets Manager AWS Systems Manager "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives Practice creating, managing, and deploying AWS resources using various tools (Console, CLI, SDK, Elastic Beanstalk, etc.). Build a prototype web application using Lambda, S3, DynamoDB, and API Gateway. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Practice: Create a table in AWS DynamoDB using the AWS CLI or Python SDK (boto3) with Access Keys. 28/09/2025 29/09/2025 3 Practice: Build a Book Store application using AWS Lambda, S3, and DynamoDB. 30/09/2025 01/10/2025 https://000078.awsstudygroup.com/ 4 Learn JSON \u0026amp; Document data models. 01/10/2025 02/10/2025 5 Practice:\n+ Deploy and verify AWS resources using an AWS CloudFormation template.\n+ Use AWS Tools for Eclipse to deploy a Java application to an Elastic Beanstalk environment.\n+ Install and configure the Elastic Beanstalk CLI.\n+ Use the EB CLI to deploy updates to an existing environment.\n+ Use the AWS SDK to query and modify AWS resources programmatically. 03/10/2025 04/10/2025 https://000050.awsstudygroup.com/ 6 Practice: Build a frontend web application that interacts with the database through Lambda and API Gateway. 04/10/2025 05/10/2025 https://000079.awsstudygroup.com/ Week 4 Achievements 1. Working with AWS DynamoDB Created and managed DynamoDB tables using AWS CLI and Python SDK (boto3). Defined Primary Keys, Sort Keys, and configured Read/Write Capacity Modes. 2. Built a Book Store Application (Lambda + S3 + DynamoDB) Created Lambda functions to perform CRUD operations on book data. Integrated API Gateway with Lambda to build a RESTful backend. Stored book images and static web assets in Amazon S3. 3. Understood JSON \u0026amp; Document Data Models Learned how DynamoDB stores semi-structured data. Queried and updated data using JSON-based structures. 4. Deployed a Java Application with Elastic Beanstalk Deployed infrastructure using AWS Console and CloudFormation templates. Installed and configured the Elastic Beanstalk CLI. Deployed updates to an existing environment. Used the AWS SDK for Java to interact programmatically with AWS. 5. Built a Frontend Web App with Serverless Backend Created a simple UI for adding, editing, and deleting items in DynamoDB. Understood the workflow: Frontend ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB. Learned basic approaches to securing serverless APIs. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives Explore and practice authentication and storage using AWS Amplify. Understand and deploy serverless applications using AWS SAM (Serverless Application Model). Learn and implement user authentication using Amazon Cognito. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Learn AWS Amplify. 05/10/2025 06/10/2025 https://aws.amazon.com/vi/amplify/ 3 Practice: Use Amplify Authentication and Amplify Storage for serverless applications. 06/10/2025 09/10/2025 https://000134.awsstudygroup.com/ 4 Learn AWS SAM: + SAM templates and SAM CLI + SAM Accelerate and SAM CLI integration. 09/10/2025 10/10/2025 https://aws.amazon.com/vi/serverless/sam/ 5 Practice: + Install SAM CLI + Deploy front-end and Lambda functions + Configure API Gateway and test API via Postman. 10/10/2025 11/10/2025 https://000080.awsstudygroup.com/ 6 Learn Amazon Cognito and Practice: Implement authentication using Cognito. 11/10/2025 13/10/2025 https://000081.awsstudygroup.com/ Week 5 Achievements 1. Worked with AWS Amplify Learned Amplify architecture and how it integrates with serverless systems. Implemented Amplify Authentication for user registration and login flows. Used Amplify Storage to manage file uploads and access through Amazon S3. Managed Amplify environments using CLI and integrated with front-end applications. 2. Studied and Applied AWS SAM (Serverless Application Model) Learned structure of SAM templates; understood CloudFormation integration. Installed and configured SAM CLI for local development and deployment. Practiced deploying serverless applications using SAM Accelerate. Deployed Lambda functions and tested API endpoints via API Gateway using Postman. Understood workflow: SAM ‚Üí Lambda ‚Üí API Gateway ‚Üí CloudFormation. 3. Learned and Practiced Amazon Cognito Understood Cognito User Pools (authentication) and Identity Pools (authorization + AWS service access). Implemented user authentication workflows using Cognito. Integrated Cognito with AWS Amplify for seamless front-end authentication. Successfully tested sign-up, sign-in, confirmation, and token verification flows. 4. Combined AWS Amplify, SAM, and Cognito Built a fully functional serverless application supporting authentication, API interaction, and cloud deployment. Strengthened overall understanding of serverless architecture, preparing for advanced cloud-native development tasks. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives Understand and practice AWS Backup, including creating Backup Vaults, Backup Plans, and deploying backup configurations using AWS CloudFormation. Learn how AWS WAF (Web Application Firewall) and AWS PrivateLink work, including key components such as ACLs, Rules, Rule Groups, VPC Endpoint Services, and Network Load Balancers. Study AWS KMS (Key Management Service) ‚Äî covering symmetric and asymmetric key management and their role in data encryption. Gain a solid understanding of Containerization with Docker, including how to build and deploy applications using Docker Images and Docker Compose. Strengthen skills in Infrastructure as Code (IaC) and container-based deployment for secure and scalable cloud environments. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS Backup: Backup vault, Backup plan, Using CloudFormation to create backup plan 13/10/2025 15/10/2025 https://000013.awsstudygroup.com/ 3 Practice: Create backup plan, on-demand backup, backup vaults 13/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn AWS WAF and AWS PrivateLink: ACL, Rules, Rule Groups, VPC Endpoint Service, VPC Endpoint, Network Load Balancer 15/10/2025 17/10/2025 https://000026.awsstudygroup.com/ https://000111.awsstudygroup.com/ 5 Learn AWS KMS: Symmetric Key, Asymmetric Key; Practice: Create ACLs, rules and rule groups 17/10/2025 19/10/2025 https://000033.awsstudygroup.com/ 6 Learn Containerization with Docker: What is Docker, Deploy using Docker Image, Deploy with Docker Compose and push image 19/10/2025 21/10/2025 https://000015.awsstudygroup.com/ Week 6 Achievements Learned and practiced AWS Backup, created Backup Vaults, Backup Plans, and On-demand Backups via AWS Console and CloudFormation. Configured and tested AWS WAF, created ACLs, Rules, and Rule Groups to protect web applications. Practiced AWS PrivateLink, implemented VPC Endpoint Services and VPC Endpoints with Network Load Balancer. Used AWS KMS, created and managed both Symmetric and Asymmetric Keys for secure data encryption. Built and deployed containerized applications with Docker, including Docker Images, containers, Docker Compose, and pushed images to Docker Hub. Strengthened understanding of Cloud Security, Encryption, and Containerization, preparing for real-world AWS deployments. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives Learn the AWS services used to optimize performance. Understand necessary services for performance optimization: ECS, EKS, CodePipeline, Storage Gateway. Learn about Docker, Kubernetes, and the relationship between Docker and Kubernetes. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS EKS: Control Plane (AWS managed), Worker Nodes (user-managed), Components: Cluster, Node Group, Pod, Deployment, Service 20/10/2025 21/10/2025 https://000126.awsstudygroup.com/1-introduce/ 3 Practice EKS: Create network (VPC, subnets, Internet Gateway), Auth for control plane, Create EKS cluster, Install addons (vpc-cni, kube-proxy), Auth for worker node, Create worker node, Install addon coredns, Test Nginx deployment 21/10/2025 22/10/2025 https://000126.awsstudygroup.com/1-introduce/ 4 Learn AWS ECS: Cluster, Task Definition, Task, Service, Container Agent, ECS Launch Types, Networking in ECS, Integrate with other services, ECS Auto Scaling 22/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn AWS Storage Gateway and Practice ECS: Create ECS cluster, Configure Docker image, Create Task Definition, Register namespace in Cloud Map 23/10/2025 25/10/2025 https://000016.awsstudygroup.com/1-introduction/ 6 Learn AWS CodePipeline: Source stage (GitHub, CodeCommit, S3), Build stage (CodeBuild), Deploy stage (CodeDeploy) 25/10/2025 27/10/2025 https://000017.awsstudygroup.com/ Week 7 Achievements 1. AWS EKS Understand EKS architecture: Control Plane (AWS-managed) and Worker Nodes (self-managed). Learned definitions: Cluster, Node Group, Pod, Deployment, Service. Connected kubectl to EKS Cluster and managed workloads. 2. Practice on EKS Created VPC, Subnets, Internet Gateway, Route Table. Provided authentication for EKS Control Plane (IAM Role). Created EKS Cluster, installed addons: vpc-cni, kube-proxy. Created Node Group for Worker Nodes, installed addon coredns. Deployed Nginx successfully on EKS, accessible via LoadBalancer. 3. AWS ECS Understood ECS Cluster, Task Definition, Service, Container Agent. Differentiated ECS Launch Types: EC2 vs Fargate. Learned ECS networking modes: bridge, host, awsvpc. Integrated ECS with CloudWatch, Load Balancer, Auto Scaling. 4. Practice ECS Created ECS Cluster (Fargate), built and pushed Docker images to ECR. Created Task Definition and Service to run containers. Registered namespace in Cloud Map for service discovery. Learned AWS Storage Gateway basics and hybrid storage model. 5. AWS CodePipeline Learned CI/CD pipeline stages: Source ‚Üí Build ‚Üí Deploy. Integrated CodePipeline with GitHub, CodeBuild, CodeDeploy. Automated build and deployment of Spring Boot or React applications to EC2/S3. Wrote template files: buildspec.yml and appspec.yml. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives Understand the fundamentals and use cases of AWS Step Functions, including its 7 core state types and how to orchestrate complex workflows. Gain hands-on experience creating and testing workflows in AWS Cloud9, focusing on task orchestration and error handling. Learn the key features and deployment steps of Amazon FSx, including its different variants (Windows File Server, Lustre, NetApp ONTAP, OpenZFS). Practice configuring an FSx file system integrated with AWS Managed Microsoft AD, ensuring proper setup of networking, authentication, and file sharing. Explore AWS X-Ray to understand how to trace and visualize requests across distributed applications for performance optimization and debugging. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS Step Functions: + 7 states: Task, Choice, Fail/Success, Pass, Wait, Parallel, Map + Use cases of AWS Step Functions + Benefits of AWS Step Functions 26/10/2025 27/10/2025 https://000047.awsstudygroup.com/1-intro/ 3 Practice: + Create Cloud9 environment + Create sample services + Initialize workflow + Implement error handling 27/10/2025 28/10/2025 https://000047.awsstudygroup.com/1-intro/ 4 Learn Amazon FSx: + FSx for Windows File Server + FSx for Lustre + FSx for NetApp ONTAP + FSx for OpenZFS 28/10/2025 29/10/2025 https://000025.awsstudygroup.com/ 5 Practice: + Configure file system details + Choose existing VPC + Choose AWS Managed Microsoft AD (Provide DNS, Service Account username/password) + Define Windows file share name + Review and create 29/10/2025 30/10/2025 https://000025.awsstudygroup.com/ 6 Learn AWS X-Ray: + Trace + Segment + Subsegment + Annotation / Metadata + Service Map 30/10/2025 31/10/2025 https://000140.awsstudygroup.com/ Week 8 Achievements Learned and explained the 7 states of AWS Step Functions, including their role in workflow automation and state transitions. Practiced building a sample workflow in Cloud9 with Step Functions, including error handling and execution validation. Gained understanding of different Amazon FSx options and their ideal use cases for Windows workloads, HPC, and enterprise storage. Completed hands-on setup of FSx for Windows File Server, including file system configuration, VPC/AD integration, and file share creation. Learned how AWS X-Ray collects traces, segments, subsegments, and visualized a Service Map to identify performance bottlenecks and errors. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives Understand and apply AWS AppSync for building GraphQL APIs with various data sources. Learn and configure AWS EBS Data Lifecycle Manager to automate snapshots and backups. Explore AWS GuardDuty for intelligent threat detection using ML and behavior analytics. Study AWS Macie to identify and protect sensitive data in S3 buckets using machine learning. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS AppSync: + GraphQL APIs: Query, Mutation, Subscription + Data Sources: DynamoDB, RDS/Aurora, AWS Lambda, HTTP Endpoints, OpenSearch + Authentication and Authorization: AWS IAM, API Keys, Cognito User Pools, OpenID Connect + Realtime Subscriptions, Caching, Offline support Practice: + Create GraphQL API + Define schema and attach data source + Configure request/response mapping templates 02/11/2025 03/11/2025 https://000086.awsstudygroup.com/1-introduction/ 3 Learn AWS EBS Data Lifecycle Manager: + Automate backup and recovery + Reduce storage costs + Ensure compliance and data protection + Understand Lifecycle Policies: EBS Snapshot Policy, EBS-backed AMI Policy, Cross-region/Cross-account Copy Policy 03/11/2025 05/11/2025 https://000088.awsstudygroup.com/ 4 Practice: + Launch EC2 instance and configure lifecycle policies + Define which resources to back up, schedule, and retention Learn AWS GuardDuty: + ML-based threat detection and behavioral analysis 03/11/2025 06/11/2025 https://000098.awsstudygroup.com/ 5 Learn AWS Macie: + Key Features: Data Discovery, Classification, Bucket-level Visibility + How Macie works internally + Use Cases and Pricing Model 14/08/2025 15/08/2025 https://000090.awsstudygroup.com/ 6 Practice on AWS Macie: + Create and configure an S3 bucket + Enable Macie service + Create Macie jobs to scan and classify data 15/08/2025 15/08/2025 https://000090.awsstudygroup.com/ Week 9 Achievements AWS AppSync: Built a simple GraphQL API integrated with DynamoDB and Lambda, understanding schema design, resolvers, and mapping templates. AWS EBS Lifecycle Manager: Configured automated backup policies to improve reliability and cost-efficiency in EC2 environments. AWS GuardDuty: Gained knowledge of continuous threat detection using ML and anomaly-based analysis. AWS Macie: Enabled Macie for S3, ran data classification jobs, and reviewed findings on sensitive data exposure and compliance monitoring. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn Amazon RDS,AWS DynamoDB, LightSail and EC2 Auto Scaling Week 3: Learn security\nWeek 4: Learn AWS DynamoDB and Relational Database Services\nWeek 5: authentication and storage\nWeek 6: Data encrytion\nWeek 7: optimized performance\nWeek 8: setup of networking, authentication, and file sharing\nWeek 9: Practice with security and operations\nWeek 10: Learn monitoring performance\nWeek 11: Learn Redis and more AWS Security Week 12: Learn QuickSight, Athena, and data lake\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "URBAN RAIL TRANSIT SERVICE SYSTEM ON AWS Executive Summary Rapid urban development has increased the need for intelligent and sustainable public transportation. A modern Urban Rail Transit System requires a reliable, scalable, and highly available digital platform operating 24/7.\nThis proposal presents an AWS architecture built on Amazon EC2, ensuring full control, high performance, and flexibility for mission-critical workloads.\nThe platform supports:\nTicket booking, scheduling, payments, and traffic monitoring Real-time passenger data ingestion through Amazon Kinesis BI dashboards and predictive analytics with QuickSight and SageMaker Full CI/CD automation and comprehensive monitoring Key Highlights EC2-based architecture with Auto Scaling \u0026amp; ALB End-to-end security: Route53 ‚Üí CloudFront ‚Üí WAF ‚Üí ALB ‚Üí Private EC2 Fully automated CI/CD using CodePipeline, CodeBuild, CodeDeploy Real-time processing using Kinesis Data Streams Analytics with QuickSight High availability \u0026amp; scalability across multiple AZs 1. Project Objectives Primary Objective Build a digital platform for the Urban Rail Transit System with scalability, security, and long-term operational stability.\nSpecific Objectives Implement ticketing, scheduling, payment, and notification services Enable automated operations and system-wide monitoring Establish a full CI/CD pipeline with zero downtime deployment to EC2 Support real-time passenger data ingestion and analytics Provide multi-AZ architecture with ‚â• 99.95% availability 2. Project Scope Component Description Region AWS Singapore (ap-southeast-1) Users Passengers, operators, administrators Architecture Style Multi-tier architecture on EC2 Phase 1 Ticketing, scheduling, notifications Phase 2 Analytics, BI dashboards 3. Proposed AWS Architecture 3.1 Architecture Overview A multi-tier architecture will be deployed, including:\nEdge Layer: Route53, CloudFront, AWS WAF Application Layer: ALB ‚Üí EC2 Auto Scaling Group Data Layer: RDS SQL Server, ElastiCache Redis Event Layer: EventBridge, SNS, SQS Analytics Layer: Kinesis ‚Üí S3 ‚Üí QuickSight ‚Üí SageMaker Monitoring Layer: CloudWatch, CloudTrail CI/CD Layer: CodePipeline, CodeBuild, CodeDeploy 3.2 Networking \u0026amp; Access Layer Route 53: Global DNS routing CloudFront: CDN caching and global acceleration AWS WAF: Security filtering (SQLi, XSS, bots) Application Load Balancer: Routes traffic to EC2 Traffic Flow:\nUser ‚Üí Route53 ‚Üí CloudFront ‚Üí WAF ‚Üí ALB ‚Üí Private Subnet ‚Üí EC2 Instances\n3.3 Application Layer ‚Äî EC2 Auto Scaling Why EC2? Full control over OS, runtime, and application environment Suitable for monolithic or microservice deployments Reliable for long-running backend services Auto Scaling based on CPU, network, request count EC2 roles for secure access to AWS resources Application Deployment Model Backend components deployed on EC2 Auto Scaling Group EC2 instances in Private Subnets for security Deployed via CodeDeploy (Blue/Green or Rolling) Backend Services Include: Booking Service Schedule Service Payment Service Notification Service User Service Operation \u0026amp; Reporting Service 3.4 Data Layer Amazon RDS (SQL Server) Stores tickets, schedules, user accounts, payments Multi-AZ deployment ensures HA Automated backups and failover IAM authentication + encryption Amazon S3 Stores documents, reports, logs, analytics data Destination for Kinesis stream data Lifecycle storage tiering for cost reduction 3.5 Event \u0026amp; Messaging Layer Amazon EventBridge Automates operations, e.g.:\nPaymentSuccess ‚Üí CreateInvoice ‚Üí NotifyUser TrainDelay ‚Üí PushNotifications ‚Üí Dashboard Update Amazon SQS Buffer queue for heavy workloads Prevents overload during peak hours Amazon SNS Sends multi-channel notifications (SMS, email, push) 3.6 Real-Time Analytics Kinesis Data Streams Real-time ingestion of passenger flow data App logs ‚Üí Kinesis ‚Üí S3 for analytics QuickSight Business dashboards: ticket sales, peak traffic, service delays 3.7 Monitoring \u0026amp; Observability CloudWatch Metrics: EC2 performance, ALB latency, RDS metrics CloudWatch Logs: Application logs from EC2 via CloudWatch Agent SNS Alerts: Critical system alerts CloudTrail: Governance and audit logs 3.8 CI/CD Pipeline Developer Commit\n‚Üí CodePipeline\n‚Üí CodeBuild (build + test)\n‚Üí Artifact Storage\n‚Üí CodeDeploy\n‚Üí EC2 Auto Scaling Group\nDeployment Features Blue/Green or Rolling EC2 deployment Health check via ALB Automatic rollback on failure Zero downtime upgrade capability 4. Deployment Plan Phase Duration Deliverables 1 1 week Route53, CloudFront, WAF, VPC, ALB 2 3 weeks EC2 ASG, RDS 3 1 week EventBridge, SQS, SNS 4 3 weeks Kinesis, S3, QuickSight 5 2 weeks CI/CD Pipeline (CodeDeploy) 6 2 weeks Hardening, cost optimization 5. Estimated Monthly Operational Cost (EC2 Architecture) Service Cost/Month CloudFront + Route53 + WAF $24 EC2 ASG (t3.micro x 2 AZ) $38 RDS $19 S3 + Kinesis $1 Monitoring $37 CI/CD $18 Total Estimated Cost $210 6. Expected Outcomes Reliable 24/7 transit system operations High-availability EC2 backend with autoscaling Secure payment processing and protected endpoints Real-time passenger analytics Optimized operational cost with correctly sized instances Zero-downtime deployment and easy maintenance Appendix A ‚Äî AWS Services Summary Category AWS Services Edge Route53, CloudFront, WAF Networking VPC, ALB Compute EC2, Auto Scaling Group Database RDS SQL Server Event EventBridge, SNS, SQS Analytics Kinesis, S3, QuickSight Monitoring CloudWatch, CloudTrail CI/CD CodePipeline, CodeBuild, CodeDeploy "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives Practice with AWS core services (VPC, EC2, EBS, Postgres) Learn AWS SageMaker for AI/ML workflow Practice AI/ML data analysis with SageMaker Learn AWS Bedrock AgentCore components Learn AWS Glue and Amazon Athena for ETL and analytics Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Practice: + Creating VPCs + Initializing EC2 instances + Creating an EBS volume + Assigning EBS volume + Installing Postgres + Mount EBS on EC2 production + Postgres backup + Mount EBS on EC2 test + Data recovery 08/11/2025 10/11/2025 https://100000.awsstudygroup.com/ 3 Learn AWS SageMaker AI: + What is SageMaker + Components + End-to-End Workflow 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 4 Practice: Use Amazon SageMaker AI to analyze data, import Excel files, choose data types, and export results including charts 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 5 Learn AWS Bedrock AgentCore: + Identity, Memory, Code Interpreter, Browser, Gateway, Observability + Common use cases: equip agents with built-in tools, deploy securely at scale, test and monitor agents 12/11/2025 14/11/2025 https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html 6 Learn AWS Glue and Amazon Athena 14/11/2025 16/11/2025 https://000040.awsstudygroup.com/ Week 10 Achievements 1. AWS Core Services Practice: Created VPCs with proper subnets, route tables, and Internet Gateways. Launched EC2 instances for production and test environments. Created and attached EBS volumes to EC2 instances. Installed and configured PostgreSQL on EC2 instances. Mounted EBS volumes on both production and test instances. Performed PostgreSQL backup and data recovery successfully. 2. AWS SageMaker AI Learning \u0026amp; Practice: Understood SageMaker components: Notebook, Training, Endpoint, Model, Pipelines. Completed end-to-end AI/ML workflow: imported Excel datasets, analyzed data, visualized charts. Learned to choose proper data types, clean data, and export results for further use. 3. AWS Bedrock AgentCore Learning: Learned core components: Identity, Memory, Code Interpreter, Browser, Gateway, Observability. Explored common use cases: equipping agents with tools, secure deployment at scale, monitoring and testing agents. 4. AWS Glue \u0026amp; Amazon Athena Learning: Practiced building ETL pipelines using AWS Glue. Learned to catalog datasets and transform data for analytics. Queried S3 data using Amazon Athena and verified results. Connected Athena queries to QuickSight dashboards for reporting and visualization. 5. AWS CLI \u0026amp; Management Console Proficiency: Configured AWS CLI with Access Key, Secret Key, and Default Region. Used CLI to retrieve regions, list services, and check configurations. Managed AWS resources via CLI and Web Console interchangeably. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives Understand and practice AWS services: ElastiCache (Redis), Certificate Manager, Inspector, Detective, Systems Manager Learn the structure, operation, and benefits of each service Practice connecting, granting permissions, deploying, and performing security checks on AWS services Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn Amazon ElastiCache - Redis: + What is Redis + Components: Clusters, ElastiCache nodes, Redis shards + How it works + Benefits of using ElastiCache 17/11/2025 18/11/2025 https://000061.awsstudygroup.com/1-introduce/ 3 Learn AWS Certificate Manager Practice: + Create cluster + Grant access to the cluster + Connect to cluster node 18/11/2025 19/11/2025 https://000061.awsstudygroup.com/ https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html 4 Learn AWS Inspector: + Identify security gaps + Detect misconfigurations + Apply remediation steps 19/11/2025 20/11/2025 https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html 5 Learn AWS Detective 21/11/2025 22/11/2025 https://docs.aws.amazon.com/detective/latest/userguide/what-is-detective.html 6 Practice with AWS Systems Manager 22/11/2025 24/11/2025 https://000031.awsstudygroup.com/1-introduce/ Week 11 Achievements Completed theory and hands-on practice with ElastiCache (Redis): learned about Redis, clusters, nodes, shards, how it works, and its benefits. Practiced creating Redis clusters, granting access, and connecting to nodes. Learned and practiced AWS Certificate Manager: creating and deploying certificates on clusters. Learned and practiced AWS Inspector: identifying security gaps, misconfigurations, and remediation steps. Learned AWS Detective: detecting and investigating security events. Practiced AWS Systems Manager: centralized management and operation of AWS resources. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives Understand and practice AWS QuickSight and AWS Athena. Build a data lake from multiple data sources. Deploy a web application using Elastic Beanstalk and CDK CI/CD pipelines. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS QuickSight:\n+ Overview + Architecture + Components + SPICE 25/11/2025 26/11/2025 https://000073.awsstudygroup.com/ 3 Practice with AWS QuickSight 26/11/2025 27/11/2025 https://000073.awsstudygroup.com/ 4 Learn and Practice AWS Athena 27/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Practice building a data lake 29/11/2025 30/11/2025 https://000070.awsstudygroup.com/ 6 Practice deploying web app with Elastic Beanstalk and CDK pipeline 01/12/2025 02/12/2025 https://000113.awsstudygroup.com/ Week 12 Achievements AWS QuickSight\nLearned overview, architecture, and main components (Dashboard, Analysis, Dataset). Understood SPICE engine to accelerate queries. Practiced creating a basic dashboard and connecting datasets from S3/Athena. AWS Athena\nLearned how to query data stored on S3 using SQL. Practiced creating tables, views, and running sample queries. Data Lake\nBuilt a small data lake on S3 to consolidate multiple data sources. Performed data cleaning and transformation before analysis. Elastic Beanstalk \u0026amp; CDK\nDeployed a demo web application on Elastic Beanstalk. Configured a basic CDK pipeline for automatic deployment. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 -AWS services scale to new heights for Prime Day 2025: key metrics and milestones Prime Day 2025 set new records powered by AWS. More than 7,000 ASRS robots at Amazon fulfillment centers executed over 524 million commands, peaking at 8 million per hour‚Äîa 160% increase over 2024. With AWS Outposts ensuring low-latency operations and Amazon‚Äôs retail expertise, customers quickly found deals, accessed product information, and enjoyed fast same-day or next-day delivery.\nBlog 2 - AWS named as a Leader in 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms and Container Management In the era of booming cloud computing and artificial intelligence, businesses are constantly seeking technology platforms that enable faster innovation, greater agility, and better cost optimization. With its comprehensive ecosystem of services, AWS not only supports millions of customers worldwide on their digital transformation journey but also continues to affirm its pioneering position by being recognized by Gartner as a Leader in several key categories of the 2025 Magic Quadrant.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Event 1 Event Name: Data science on AWS Workshop\nDate \u0026amp; Time: 09:00, October 16, 2025\nLocation: Academic hall, FPT HCM University, High Tech,Thu Duc City, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Generative AI with Amazon Bedrock\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Company from 09/2025 to 12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Learning how to use all services and how they work on AWS, through which I improved my skills in practicing workshop, apply the AWS services in my application. Also i have to optimize the cost, performance and increased security layers\nWhen i am in the company, i always to check my to do list and check all the task that i haven\u0026rsquo;t done yesterday. Besides that i have chance to meet my mentor and ask them help when i have an issue or something. In the progress of study, me with my teammates have to build the websites and using AWS services.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚òê ‚úÖ ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚òê ‚úÖ 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚òê ‚úÖ ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚òê ‚úÖ ‚òê Needs Improvement I am internal person so i barely communicated with my mentor, i need to change my how to communicate with my mentor and spend time to meet them to ask them about my problem. I need improve my project management skill because i have to train my teammates and give them a task but i don\u0026rsquo;t have a method like Agile or Scrum to develop a software like coworker. I always to think about how the services work and how they apply to my application but sometime i don\u0026rsquo;t understand them, so i need to spend time to improve my logic mindset to understand them. "
},
{
	"uri": "http://localhost:1313/workshop-template/7-feedback/",
	"title": "Sharing &amp; Feedback",
	"tags": [],
	"description": "",
	"content": " Here you can freely provide personal feedback about your experience participating in the First Cloud Journey program, helping the FCJ team improve areas that need attention based on the following categories:\nGeneral Evaluation 1. Work Environment\nThe work environment is very friendly and open. FCJ members are always willing to support me whenever I face difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think additional team bonding or networking sessions could help everyone understand each other more.\n2. Support from Mentors / Team Admins\nMentors provide detailed guidance, explain clearly when I don‚Äôt understand, and always encourage me to ask questions. The admin team supports administrative tasks, documents, and creates conditions for smooth work. I appreciate that mentors allow me to try and solve problems on my own instead of just giving answers.\n3. Job Fit with Academic Background\nThe tasks assigned match my knowledge from school while also expanding into new areas I had not encountered before. This allowed me to reinforce my foundational knowledge while learning practical skills.\n4. Learning Opportunities \u0026amp; Skill Development\nDuring the internship, I learned many new skills such as using project management tools, teamwork, and professional communication in a corporate environment. Mentors also shared practical experiences, helping me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: people respect each other, work seriously but remain cheerful. When urgent projects arise, everyone helps each other regardless of position. This made me feel part of the team even as an intern.\n6. Intern Benefits / Policies\nThe company provides internship allowances and offers flexible working hours when needed. Additionally, participating in internal training sessions is a great advantage.\nOther Questions What are you most satisfied with during the internship?\nDuring the learning process, I felt comfortable adapting to the company‚Äôs work culture and was very satisfied with the guidance from mentors. What do you think the company needs to improve for future interns?\nThe company could improve opportunities for students to come to the office daily. If recommending to friends, would you advise them to intern here? Why?\nIf I were to introduce this to friends, I would recommend it because Amazon Web Services Vietnam is not only a cloud computing company but also has a large AWS study group community in Vietnam, allowing students to learn about cloud computing. Suggestions \u0026amp; Wishes Any advice to enhance the internship experience?\nI would like to join the company club to learn more from mentors and see how they lead a team. Would you like to continue this program in the future?\nI would like to be more closely associated with the company. Other feedback (free sharing)\nNo additional feedback at this time. "
},
{
	"uri": "http://localhost:1313/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]