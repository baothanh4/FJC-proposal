[
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "AWS services scale to new heights for Prime Day 2025: key metrics and milestones Amazon Prime Day 2025 was the biggest Amazon Prime Day shopping event ever, setting records for both sales volume and total items sold during the 4-day event. Prime members saved billions while shopping Amazon‚Äôs millions of deals during the event.\nThis year marked a significant transformation in the Prime Day experience through advancements in the generative AI offerings from Amazon and AWS. Customers used Alexa+‚Äîthe Amazon next-generation personal assistant now available in early access to millions of customers‚Äîalong with the AI-powered shopping assistant, Rufus, and AI Shopping Guides. These features, built on more than 15 years of cloud innovation and machine learning expertise from AWS, combined with deep retail and consumer experience from Amazon, helped customers quickly discover deals and get product information, complementing the fast, free delivery that Prime members enjoy year-round.\nAs part of our annual tradition to tell you about how AWS powered Prime Day for record-breaking sales, I want to share the services and chart-topping metrics from AWS that made your amazing shopping experience possible.\nPrime Day 2025 ‚Äì all the numbers During the weeks leading up to big shopping events like Prime Day, Amazon fulfillment centers and delivery stations work to get ready and ensure operations run efficiently and safely. For example, the Amazon automated storage and retrieval system (ASRS) operates a global fleet of industrial mobile robots that move goods around Amazon fulfillment centers.\nAWS Outposts, a fully managed service that extends the AWS experience on-premises, powers software applications that manage the command-and-control of Amazon ASRS and supports same-day and next-day deliveries through low-latency processing of critical robotic commands.\nDuring Prime Day 2025, AWS Outposts at one of the largest Amazon fulfillment centers sent more than 524 million commands to over 7,000 robots, reaching peak volumes of 8 million commands per hour‚Äîa 160 percent increase compared to Prime Day 2024.\nHere are some more interesting, mind-blowing metrics:\nAmazon Elastic Compute Cloud (Amazon EC2) ‚Äì During Prime Day 2025, AWS Graviton, a family of processors designed to deliver the best price performance for cloud workloads running in Amazon EC2, powered more than 40 percent of the Amazon EC2 compute used by Amazon.com. Amazon also deployed over 87,000 AWS Inferentia and AWS Trainium chips ‚Äì custom silicon chips for deep learning and generative AI training and inference ‚Äì to power Amazon Rufus for Prime Day.\nAmazon SageMaker AI ‚Äî Amazon SageMaker AI, a fully managed service that brings together a broad set of tools to enable high-performance, low-cost machine learning (ML), processed more than 626 billion inference requests during Prime Day 2025.\nAmazon Elastic Container Service (Amazon ECS) and AWS Fargate‚Äì Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that works seamlessly with AWS Fargate, a serverless compute engine for containers. During Prime Day 2025, Amazon ECS launched an average of 18.4 million tasks per day on AWS Fargate, representing a 77 percent increase from the previous year‚Äôs Prime Day average.\nAWS Fault Injection Service (AWS FIS) ‚Äì We ran over 6,800 AWS FIS experiments‚Äîover eight times more than we conducted in 2024‚Äîto test resilience and ensure Amazon.com remains highly available on Prime Day. This significant increase was made possible by two improvements: new Amazon ECS support for network fault injection experiments on AWS Fargate, and the integration of FIS testing in continuous integration and continuous delivery (CI/CD) pipelines.\nAWS Lambda ‚Äì AWS Lambda, a serverless compute service that lets you run code without managing infrastructure, handled over 1.7 trillion invocations per day during Prime Day 2025.\nAmazon API Gateway ‚Äì During Prime Day 2025, Amazon API Gateway, a fully managed service that makes it easy to create, maintain, and secure APIs at any scale, processed over 1 trillion internal service requests‚Äîa 30 percent increase in requests on average per day compared to Prime Day 2024.\nAmazon CloudFront ‚Äì Amazon CloudFront, a content delivery network (CDN) service that securely delivers content with low latency and high transfer speeds, delivered over 3 trillion HTTP requests during the global week of Prime Day 2025, a 43 percent increase in requests compared to Prime Day 2024.\nAmazon Elastic Block Store (Amazon EBS) ‚Äì During Prime Day 2025, Amazon EBS, our high-performance block storage service, peaked at 20.3 trillion I/O operations, moving up to an exabyte of data daily.\nAmazon Aurora ‚Äì On Prime Day, Amazon Aurora, a relational database management system (RDBMS) built for high performance and availability at global scale for PostgreSQL, MySQL, and DSQL, processed 500 billion transactions, stored 4,071 terabytes of data, and transferred 999 terabytes of data.\nAmazon DynamoDB ‚Äì Amazon DynamoDB, a serverless, fully managed, distributed NoSQL database, powers multiple high-traffic Amazon properties and systems including Alexa, the Amazon.com sites, and all Amazon fulfillment centers. Over the course of Prime Day, these sources made tens of trillions of calls to the DynamoDB API. DynamoDB maintained high availability while delivering single-digit millisecond responses and peaking at 151 million requests per second.\nAmazon ElastiCache ‚Äì During Prime Day, Amazon ElastiCache, a fully managed caching service delivering microsecond latency, peaked at serving over 1.5 quadrillion daily requests and over 1.4 trillion requests in a minute.\nAmazon Kinesis Data Streams ‚Äì Amazon Kinesis Data Streams, a fully managed serverless data streaming service, processed a peak of 807 million records per second during Prime Day 2025.\nAmazon Simple Queue Service (Amazon SQS) ‚Äì During Prime Day 2025, Amazon SQS ‚Äì a fully managed message queuing service for microservices, distributed systems, and serverless applications ‚Äì set a new peak traffic record of 166 million messages per second.\nAmazon GuardDuty ‚Äì During Prime Day 2025, Amazon GuardDuty, an intelligent threat detection service, monitored an average of 8.9 trillion log events per hour, a 48.9 percent increase from last year‚Äôs Prime Day.\nAWS CloudTrail ‚Äì AWS CloudTrail, which tracks user activity and API usage on AWS, as well as in hybrid and multicloud environments, processed over 2.5 trillion events during Prime Day 2025, compared to 976 billion events in 2024.\nPrepared to scale\nIf you‚Äôre preparing for similar business-critical events, product launches, and migrations, I recommend that you take advantage of our newly branded AWS Countdown (formerly known as AWS Infrastructure Event Management, or IEM). This comprehensive support program helps assess operational readiness, identify and mitigate risks, and plan capacity, using proven playbooks developed by AWS experts. We‚Äôve expanded to include: generative AI implementation support to help you confidently launch and scale AI initiatives; migration and modernization support, including mainframe modernization; and infrastructure optimization for specialized sectors including election systems, retail operations, healthcare services, and sports and gaming events.\nI look forward to seeing what other records will be broken next year!\n‚Äî Channy\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\rAWS named as a Leader in 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms and Container Management A month ago, I shared that Amazon Web Services (AWS) is recognized as a Leader in 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services (SCPS), with Gartner naming AWS a Leader for the fifteenth consecutive year.\nIn 2024, AWS was named as a Leader in the Gartner Magic Quadrant for AI Code Assistants, Cloud-Native Application Platforms, Cloud Database Management Systems, Container Management, Data Integration Tools, Desktop as a Service (DaaS), and Data Science and Machine Learning Platforms as well as the SCPS. In 2025, we were also recognized as a Leader in the Gartner Magic Quadrant for Contact Center as a Service (CCaaS), Desktop as a Service and Data Science and Machine Learning (DSML) platforms. We strongly believe this means AWS provides the broadest and deepest range of services to customers.\nToday, I‚Äôm happy to share recent Magic Quadrant reports that named AWS as a Leader in more cloud technology markets: Cloud-Native Application Platforms (aka Cloud Application Platforms) and Container Management.\n2025 Gartner Magic Quadrant for Cloud-Native Application Platforms AWS has been named a Leader in the Gartner Magic Quadrant for Cloud-Native Application Platforms for 2 consecutive years. AWS was positioned highest on ‚ÄúAbility to Execute‚Äù. Gartner defines cloud-native application platforms as those that provide managed application runtime environments for applications and integrated capabilities to manage the lifecycle of an application or application component in the cloud environment. The following image is the graphical representation of the 2025 Magic Quadrant for Cloud-Native Application Platforms.\nOur comprehensive cloud-native application portfolio‚ÄîAWS Lambda, AWS App Runner, AWS Amplify, and AWS Elastic Beanstalk‚Äîoffers flexible options for building modern applications with strong AI capabilities, demonstrated through continued innovation and deep integration across our broader AWS service portfolio.\nYou can simplify the service selection through comprehensive documentation, reference architectures, and prescriptive guidance available in the AWS Solutions Library, along with AI-powered, contextual recommendations from Amazon Q based on your specific requirements. While AWS Lambda is optimized for AWS to provide the best possible serverless experience, it follows industry standards for serverless computing and supports common programming languages and frameworks. You can find all necessary capabilities within AWS, including advanced features for AI/ML, edge computing, and enterprise integration.\nYou can build, deploy, and scale generative AI agents and applications by integrating these compute offerings with Amazon Bedrock for serverless inferences and Amazon SageMaker for artificial intelligence and machine learning (AI/ML) training and management.\nAccess the complete 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms to learn more.\n2025 Gartner Magic Quadrant for Container Management In the 2025 Gartner Magic Quadrant for Container Management, AWS has been named as a Leader for three years and was positioned furthest for ‚ÄúCompleteness of Vision‚Äù. Gartner defines container management as offerings that support the deployment and operation of containerized workloads. This process involves orchestrating and overseeing the entire lifecycle of containers, covering deployment, scaling, and operations, to ensure their efficient and consistent performance across different environments.\nThe following image is the graphical representation of the 2025 Magic Quadrant for Container Management.\nAWS container services offer fully managed container orchestration with AWS native solutions and open-source technologies to focus on providing a wide range of deployment options, from Kubernetes to our native orchestrator.\nYou can use Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS). Both can be used with AWS Fargate for serverless container deployment. Additionally, EKS Auto Mode simplifies Kubernetes management by automatically provisioning infrastructure, selecting optimal compute instances, and dynamically scaling resources for containerized applications.\nYou can connect on-premises and edge infrastructure back to AWS container services with EKS Hybrid Nodes and ECS Anywhere, or use EKS Anywhere for a fully disconnected Kubernetes experience supported by AWS. With flexible compute and deployment options, you can reduce operational overhead and focus on innovation and drive business value faster.\nAccess the complete 2025 Gartner Magic Quadrant for Container Management to learn more.\n‚Äî Channy\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-secrets-manager/5.3.1-create-secrets-manager/",
	"title": "Create secrets manager",
	"tags": [],
	"description": "",
	"content": " Open the Amazon Secrets manager console In the navigation pane, choose Secrets, then click Store a new secret: In the Create endpoint console: Choose \u0026lsquo;Other type of secret\u0026rsquo; In Key/value pairs, we will have 5 secret names All the next is default. After we create all 5 secrets manager: "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúData Science on AWS‚Äù Event Objectives Explore how data challenges can be solved through AWS Services Overview of Managed AI Services and their use cases Data preparation with Amazon SageMaker Using XGBoost with SageMaker Studio Notebooks No-code AutoML with SageMaker Canvas Speakers Van Hoang Kha ‚Äì Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong ‚Äì Cloud DevOps Engineer, AWS Community Builder Doan Nguyen Thanh Hoa ‚Äì CF Lecturer, FPT University HCMC Key Highlights Amazon Comprehend and Amazon Translate Deep learning-powered text analysis and translation\nProcesses various document types (emails, chats, social media, phone calls, etc.) and extracts insights automatically. Typical Comprehend use cases: Intelligent document processing Automated email workflows Customer support ticket routing Document and media tagging Sentiment analysis Contact center call analysis PII (Personally Identifiable Information) detection and redaction Amazon Translate Neural machine translation service\nKey Features:\nBroad language coverage: 4970 X‚ÜîY translation combinations Low latency: \u0026lt;150 ms/sentence on average Data security: Full encryption and ownership retention Regional coverage: Available in 17 AWS regions Customizable translation: Custom Terminologies and Active Custom Translation Batch translation: Supports DOCX, PPTX, XLSX, XML, HTML, and text files Broad domain coverage: Trained on 11 domains Pay-per-use: Simple API model Use Cases:\nLocalization: Enterprise content, media subtitling, archives Communication: Customer engagement, in-game chat, social media Text analytics: Voice of Customer, media analysis, eDiscovery Amazon Polly Text-to-speech (TTS) service\nAmazon Polly converts text into lifelike speech using deep learning.\nFeatures:\nText-to-speech (TTS) Speech Synthesis Markup Language (SSML) Custom Lexicons Speech Marks Brand Voice Common Use Cases:\nVoiced news articles and training Telephony/IVR systems Podcasts and language learning Navigation, reminders, accessibility tools Amazon Transcribe Automatic Speech Recognition (ASR) service\nConverts recorded media into text Supports real-time transcription for live streaming content Amazon Lex Conversational AI service\nBuilds voice and text-based chatbots.\nHighlights:\nEasy to use High-quality natural language understanding Built-in integration with AWS services Cost-effective Amazon Rekognition Provides image and video analysis for object detection, facial recognition, and scene understanding.\nAmazon Personalize Delivering personalized customer experiences\nFast deployment of recommendation systems Real-time response to user behavior Easy integration with existing systems Managed ML reduces time-to-market Feature Engineering Data Preparation with Amazon SageMaker Canvas Applying to Work ‚Äî Lessons Derived from Key Highlights 1. Data Understanding \u0026amp; Automation (Amazon Comprehend, Translate) Lesson learned:\nThe ability to extract insights from unstructured data is crucial for smarter decision-making.\nApplication:\nUse Amazon Comprehend for customer feedback sentiment analysis and document classification. Automate email or ticket routing based on detected intent and sentiment. Apply Amazon Translate for multilingual projects and global content localization. 2. Customer Engagement \u0026amp; Voice Interface (Amazon Polly, Lex, Transcribe) Lesson learned:\nVoice and conversational AI make digital experiences more accessible and user-friendly.\nApplication:\nCombine Lex, Polly, and Transcribe to build a voice-enabled chatbot for 24/7 support. Use Polly to generate natural-sounding voiceovers for e-learning materials. Leverage Transcribe to capture and analyze customer service calls and meeting notes. 3. Image \u0026amp; Video Intelligence (Amazon Rekognition) Lesson learned:\nComputer vision enables automation and deeper media insights.\nApplication:\nUse Rekognition to auto-tag and moderate multimedia content. Implement facial recognition for access control or customer analysis in retail environments. 4. Personalized Experience (Amazon Personalize) Lesson learned:\nPersonalization drives engagement and retention.\nApplication:\nIntegrate Personalize to recommend content, services, or products based on user behavior. Build real-time recommendation systems that evolve as user intent changes. 5. Machine Learning Simplification (Amazon SageMaker, Canvas) Lesson learned:\nMachine learning is no longer limited to experts‚ÄîAWS simplifies the end-to-end ML lifecycle.\nApplication:\nUse SageMaker Canvas for no-code model training and predictions. Apply SageMaker Studio for algorithm experimentation (e.g., XGBoost). Improve data quality through feature engineering to enhance model performance. 6. Data-driven Modernization Mindset Lesson learned:\nAI and AWS services promote a shift toward data-centric and event-driven architecture.\nApplication:\nCombine DDD (Domain-Driven Design) with AI workflows to design modular, adaptive systems. Use serverless computing (Lambda, API Gateway) for scalable AI-driven processes. Introduce event-driven patterns to handle asynchronous data flows efficiently. üåü Overall Reflection Through this AWS Data Science event, I learned not just about individual AI services but also about how to connect them into a cohesive, data-driven ecosystem.\nEach tool‚ÄîComprehend, Translate, Polly, Rekognition, Personalize, and SageMaker‚Äîrepresents a piece of a larger puzzle: building intelligent, automated, and human-centered applications.\nThese lessons are directly applicable to my projects, helping me move from manual workflows to AI-assisted automation and smarter user experiences.\nEvent Photos "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúGenerative AI with Amazon Bedrock‚Äù Event Objectives Learn what Amazon Bedrock is and how many model that AI used Learn more Prompting Engineering like: Zero-Shot, Few-Shot, Chain of Thought(CoT) Learn what Retrieval Augmented Generation is and RAG use cases More pretrained AI Services Learn what Amazon Bedrock agent core and demo Speakers Lam Tuan Kiet ‚Äì Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi ‚Äì AI Engineer, Renova Cloud Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee, First Cloud AI Journey Key Highlights Foundation Models Prompt Engineering Retrieval-Augmented Generation(RAG) Bedrock AgentCore Foundation Models Tradional ML models Foundation models Purpose Narrow, task-specific(Classification, regression,forecasting,clustering,etc\u0026hellip;) General-purpose, multi-domain, multi-task Training data Usually structured/tabular or domain-specific Massive datasets (text, images, code, etc.) Training required Must be trained manually using labeled data None ‚Äî already pretrained Customization You need to adjust hyperparameters, features, datasets Optional ‚Äî fine-tuning, RAG, prompt engineering Example Random Forest, XGBoost, SVM, Linear Regression Claude, Llama, Cohere Command, Titan Skill Needed High ML expertise (feature engineering, training pipelines) Very low ‚Äî prompt-based usage AWS Services SageMaker, Amazon ML, custom training Amazon Bedrock, Bedrock Studio Use cases Predicting, Forecasting sales, Classifying images, Fraud training, Anomaly detection Chatbots, Text summarization, Code generation, Image generation, Conversational apps, Document search with RAG, Sentiment analysis, Multi-language translation Supported foundation models in Amazon Bedrock Prompt Engineering - Crafting and refining instructions(prompts) What is a prompt? Is the input you give to an AI model (like ChatGPT, Claude, Gemini, or models on Amazon Bedrock) to get a specific output. Example What is difference between Zero-Shot, Few-Shot and Chain of Thought in Prompting Techniques? Technique Description When to Use Advantages Limitations Example Prompt Style Zero-Shot Model gets only the instruction with no examples. Simple tasks; model already understands domain. Short prompts, fast, minimal setup. Lower accuracy for complex or domain-specific tasks. \u0026ldquo;Classify the text into neutral, negative or positive\u0026rdquo; Few-Shot Provide a few examples to teach the pattern. Custom formats; domain tasks; pattern learning. Higher accuracy; model learns your desired style. Longer prompt; can become costly at scale. \u0026ldquo;A whatpu is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus. \u0026quot; Chain-of-Thought Ask model to explain reasoning step-by-step. Math, logic, coding, planning, reasoning tasks. Better reasoning, more accurate complex outputs. Slower; may generate unnecessarily long answers. \u0026ldquo;Show your step-by-step reasoning.\u0026rdquo; Retrieval Augmented Generation (RAG)- Retrieving relevant information from a data source What is Retrieval Augmented Generation? Retrieval: Fetches the relevant content from the external knowledge base or data sources based on a user query Augmentation: Adding the retrieved relevant context to the user prompt, which goes as an input to the foundation model Generation: Response from the foundation model based on the augmented prompt RAG Use cases Improved content quality: E.g Helps in reducing hallucinations and connecting with recent knowledge including enterprise data Contextual chatbots and question answering: E.g Enhance chatbot capabilities by integrating with real-time data Personalized search: E.g Searching based on user previous search history and persona Real-time data summarization: E.g Retrieving and summarizing transactional data from databases, or API calls\nWhat are embeddings? Numerical representation of text(vectors) that captures semantics and relationships between words\nEmbedding models capture features and nuances of the text\nRich embeddings can be used to compare text similarity\nMultilingual Text Embeddings can identity meaning in different languages\nAmazon Titan Embedding:\nTitan Text Embeddings: Translates text inputs(words,phrases) into numerical representations(Embeddings). Comparing embeddings produces more relevant and contextual responses that word matching Highlights:\nAmazon Titan Text Embeddings V2 is a light weight, efficient model ideal for high accuracy retrieval tasks at different dimensions. The model supports flexible embeddings sizes and prioritizes accuracy maintenance at smaller dimension sizes. Support for 100+ language in pre-training as well unit vector normalization for improving accuracy of measuring vector similarity. RAG in Action Data Ingestion Workflow RetrieveAndGenerate API\nOther Pretrained AI Services (hinh anh amazon polly) (hinh anh amazon comprehend) (hinh anh amazon kendra) (hinh anh amazon lookout family) (hinh anh amazon personalize) Amazon Bedrock AgentCore The evolution into Agentic Generative AI assistants: Follow a set of rules, Automate repetitive tasks Generative AI agents: Achieve a singular goal, address broader range of tasks, automate entire workflows Agentic AI systems: Fully autonomous, multi-agent systems, mimic human logic and reasoning Frameworks for building agents Strands agents SDK Langgraph, langchain OpenAi Agents SDK Crew.AI Google ADK Llamaindex The protype to production \u0026ldquo;chasm\u0026rdquo; Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Getting agents to productions in still too hard Securely execute and scale agent code Remember past interaction + learning Identity and access controls for all agents and tools Agentic tool use for executing complex workflows Discover and connect with custom tools and resources Understand and audit every interaction Agent core services enabling agents at scale Event Experience Attending the ‚ÄúGenerative AI with Amazon Bedrock‚Äù workshop was a highly valuable experience, giving me a comprehensive understanding of how modern AI and cloud technologies are applied in real-world scenarios. The event combined expert insights, hands-on demonstrations, and practical guidance that helped connect theory with implementation.\nLearning from industry experts Speakers from AWS, Renova Cloud, and FPT Software shared real-world best practices for AI adoption and cloud modernization. Their case studies demonstrated how organizations apply foundation models, RAG, and agent-based architectures in enterprise systems. The discussions helped clarify not just how these technologies work, but why they matter for scalability and business innovation. Hands-on technical exposure Practical sessions on Zero-Shot, Few-Shot, and Chain-of-Thought prompting improved my understanding of effective prompt engineering. The walkthrough of RAG workflows showed how grounding AI responses in real data reduces hallucinations and improves accuracy. Live demos of Bedrock AgentCore helped me visualize how AI agents plan tasks, execute tools, and integrate securely with AWS services. Understanding modernization and architecture The workshop highlighted the shift from traditional ML to foundation-model-driven AI and explained the differences in training, customization, and usage. I learned how organizations modernize systems using: Event-driven architecture Domain-driven design (DDD) AI-augmented workflows These approaches helped me understand how to design applications that are scalable, resilient, and intelligent. Exposure to modern AI tools Demonstrations of Amazon Titan Embeddings, Textract, Comprehend, Kendra, Polly, and other AI services showed how to quickly integrate intelligence into applications. Learning about Amazon Q Developer highlighted how AI can support the entire software lifecycle‚Äîfrom planning to coding and refactoring. Networking and knowledge exchange Discussions with cloud engineers, AI specialists, and developers broadened my understanding of industry use cases and challenges. These conversations reinforced the importance of aligning business objectives, data modeling, and technology decisions. Key takeaways RAG enables enterprise-grade AI by grounding answers in verified data. Prompt engineering matters‚Äîwell-structured prompts lead to more reliable results. Agentic AI systems represent the next evolution of automation, capable of reasoning and executing multi-step workflows. System modernization requires a phased roadmap and clear ROI measurement. Integrating AI tools into the developer workflow can significantly boost productivity and reduce operational overhead. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Tao Bao Thanh\nPhone Number: 0901452366\nEmail: taobaothanh365@gmail.com\nUniversity: Ho Chi Minh FPT University\nMajor: Software Engineering\nClass: OJT202\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop overview Here is a proposed overview of how AWS services are used to set up the Metropolitano system, ensuring the criteria of \u0026ldquo;high availability - secure - scalable\u0026rdquo;:Overview of Metropolitano System Architecture on AWS\nThe system is designed to manage, monitor and coordinate urban railway operations, using a range of AWS services for each functional layer:\nLayer AWS Services Role on the Metropolitano system Network \u0026amp; Security Route 53 Manage DNS, route user (operator, management) traffic to applications efficiently and reliably. CloudFront Content delivery network (CDN), which helps deliver static content (user interface, reports) with low latency and enhanced security. WAF (Web Application Firewall) Protect web applications from common network attacks, filtering malicious traffic before it reaches the application server. Secrets Manager Securely manage, rotate, and control access to sensitive information such as database (RDS) passwords, API keys, and other credentials. Helps strengthen application security and minimize the risk of information leakage. Data storage \u0026amp; Application S3(Simple Storage Service) Store all unstructured and rarely accessed data, such as logs, backups, and project documents EC2(Elastic Compute Cloud) Provides compute resources to run back-end applications, orchestration services, and user interfaces. Can be used in Auto Scaling Groups to ensure high availability. RDS (Relational Database Service) - MSSQL Provides a fully managed relational database running on the Microsoft SQL Server platform. Used to store structured, mission-critical business data (route information, schedules, equipment status). Collect \u0026amp; process Kinesis Build event-driven architectures, automate workflows by connecting different AWS services when events occur (e.g., when data reaches an alert threshold) EventBridge Build event-driven architectures, automate workflows by connecting different AWS services when events occur (e.g., when data reaches an alert threshold) SQS (Simple Queue Service) Message queues to decouple system components, handle asynchronous tasks (e.g., sending mass notifications, handling dispatch commands). SNS (Simple Notification Service) Push notification service for important events, used to send alerts to other systems or operators via email/SMS. CloudWatch Monitor resources and applications, collect logs, and set up alarms to ensure system stability and detect problems early. Analyze \u0026amp; Visualization Quicksight Business intelligence (BI) services to visualize data from RDS and Kinesis (after it has been processed), helping management monitor system status in real-time and make decisions. Automation develop CodePipeline Continuous Integration and Delivery (CI/CD) services to automate the entire process of building, testing, and deploying application source code. CodeBuild The service builds source code, runs automated tests, and generates deployment-ready packages. CodeDeploy The service automates the deployment of source code to EC2 instances or other environments, ensuring smooth, uninterrupted updates. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "Part 1: Networking \u0026amp; Security 1.1 Create VPC Steps:\nGo to VPC Console ‚Üí Your VPCs ‚Üí Create VPC Select VPC and more Name: metropolitano Create public/private subnets across at least 2 Availability Zones VPC Endpoints: None (baseline) 1.2 Enable Auto-assign Public IPv4 for Public Subnet Steps:\nVPC ‚Üí Subnets ‚Üí Select public subnet ‚Üí Edit subnet settings Enable Auto-assign Public IPv4 ‚Üí Save 1.3 Security Groups 1.3.1 Public Web/EC2 Security Group Name: public-web-sg\nInbound rules:\nHTTP/HTTPS from Internet SSH from admin IP only 1.3.2 Private Database Security Group Name: private-db-sg Inbound rules: MS SQL Server (1433) only from public-web-sg Part 2: Database Setup (RDS SQL Server) 2.1 Create DB Subnet Group RDS ‚Üí Subnet Groups ‚Üí Create Name: private-db-metropolitano VPC: metropolitano-vpc Subnets: private subnets across 2 AZs 2.2 Create RDS SQL Server Instance Steps:\nRDS ‚Üí Databases ‚Üí Create database\nEngine: Microsoft SQL Server Template: Dev/Test Credentials: Self-managed Public access: No Security group: private-db-sg Part 3: Compute ‚Äì EC2 Application Server Steps:\nGo to EC2 ‚Üí Instances ‚Üí Launch instance 2. Name: metropolitano-version-1\n3. AMI: Amazon Linux\n4. Instance type: t3.medium\n5. Key pair: myKey.pm\n6. Network:\nVPC: metropolitano-vpc Subnet: Public subnet Security Group: public-web-sg 7. Wait until instance status = 3/3 checks passed Part 4: Storage ‚Äì S3 Bucket 4.1 Create S3 Bucket Go to S3 ‚Üí Buckets ‚Üí Create bucket\nName: metropolitano-2025 Object Ownership: ACLs disabled (recommended) Block Public Access: Disable Block all public access Click Create bucket Upload the dist folder from frontend Part 5: CloudFront Steps:\nCloudFront Console ‚Üí Distributions ‚Üí Create distribution Plan: Free tier Name: Metropolitano Origin type: Amazon S3 Origin: metropolitano-2025 Origin path: /dist Cache settings: Viewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE Wait 3‚Äì5 minutes for deployment.\n![Create cloudfront](/images/5-Workshop/5.4-S3-onprem/image_19.png)\r![Websites](/images/5-Workshop/5.4-S3-onprem/image_20.png)\rPart 6: Kinesis ‚Äì Event Stream Steps:\nConsole ‚Üí Kinesis ‚Üí Create Data Stream 2. Name: metropolitano-stream\n3. Shards: Select based on expected data volume\n4. Producers (EC2) will send data; Consumers will process data Part 7: EventBridge ‚Äì Event Automation Steps:\nConsole ‚Üí EventBridge ‚Üí Create Rule Name: metropolitano-event-rule Event source: AWS services (CloudWatch Alarm, S3 Object Created) Target: SQS, SNS Part 8: SQS ‚Äì Message Queue Steps:\nConsole ‚Üí SQS ‚Üí Create Queue Name: metropolitano-queue Queue type: Standard or FIFO Part 9: SNS ‚Äì Notification Service Steps:\nConsole ‚Üí SNS ‚Üí Create Topic Name: metropolitano-alerts Add subscriptions: Email, SMS Part 10: CloudWatch ‚Äì Monitoring \u0026amp; Alerts Steps:\nConsole ‚Üí CloudWatch ‚Üí Create Alarm Metrics: EC2 CPU utilization RDS instance status Kinesis throughput Actions: Send notification via SNS when threshold is exceeded Part 11: Analytics \u0026amp; Visualization Steps:\nConsole ‚Üí QuickSight ‚Üí Sign up / Create account Dataset sources: RDS, S3, or Athena Create Analysis ‚Üí Build dashboards \u0026amp; visual reports "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services and how to use the AWS Console \u0026amp; CLI. Tasks to be Carried Out This Week Day Task Start Date Completion Date Reference Material 2 Make friends with FCJ members.\nPay attention to rules for job training.\nPractice:\n+ Create AWS Free Tier account 08/09/2025 09/09/2025 ‚Äî 3 Learn AWS \u0026amp; AWS Services:\n+ EC2\n+ Lambda\n+ SQS\n+ SNS\n+ CLI \u0026amp; SDKs 09/09/2025 10/09/2025 ‚Äî 4 Learn AWS ECS, AWS EKS, VPC, CloudFormation.\nLearn Cost Management.\nPractice:\n+ Create EC2 instance\n+ Create Lambda function\n+ Use AWS CLI 10/09/2025 11/09/2025 ‚Äî 5 Learn VPC Basics and S3.\nPractice:\n+ Create Security Group\n+ Create Internet Gateway\n+ Create Subnet\n+ Create Route Table 11/09/2025 12/09/2025 ‚Äî 6 Practice:\n+ Launch S3 Bucket 12/09/2025 13/09/2025 ‚Äî Week 1 Achievements 1. AWS Foundations Understood the core AWS service groups: Compute, Storage, Networking, Database. Successfully created and configured an AWS Free Tier account. Became familiar with the AWS Management Console and learned how to navigate services. 2. AWS CLI Skills Performed basic operations using AWS CLI: Check account \u0026amp; configuration details Retrieve list of regions View EC2 information Create/manage Key Pairs Check running services "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives Learn about Amazon RDS, AWS S3, AWS EC2 Auto Scaling, AWS DynamoDB, and Amazon Lightsail. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Learned AWS DynamoDB, AWS RDS, and Amazon Aurora. - Practice: + Created AWS CloudFront and AWS S3. 14/09/2025 15/09/2025 https://render.skillbuilder.aws/?module_id=B1DGAV8S16%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX/ 3 Practice: + Created AWS RDS. + Created DynamoDB using Python and AWS CLI. 15/09/2025 16/09/2025 https://render.skillbuilder.aws/?module_id=B1DGAV8S16%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 4 Translated blog articles. 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learned fundamentals of EC2 Auto Scaling and Amazon Lightsail.\n- Practice: + Created a database in AWS RDS.\n+ Used Amazon Lightsail. 19/09/2025 20/09/2025 https://000005.awsstudygroup.com/ https://000045.awsstudygroup.com/ 6 Practice: + Created RDS, VPC, and EC2 with AWS EC2 Auto Scaling. 20/09/2025 21/09/2025 https://000006.awsstudygroup.com/ Week 2 Achievements 1. Static Website Hosting with Amazon S3 Learned how to create an S3 bucket, upload files, and manage access. Enabled Static Website Hosting on an S3 bucket. Configured public access so the website can be accessed over the Internet. Set up an appropriate Bucket Policy to allow public read-only access. 2. Database Essentials with Amazon RDS Understand that RDS supports many popular engines such as MySQL, PostgreSQL, Oracle, and SQL Server. Learned how to connect an EC2 instance to an RDS database. Identified key connection information: endpoint, port, username, and password. Created and configured VPC, subnets, and security groups to secure an RDS instance. 3. Scaling Applications with EC2 Auto Scaling Understood that Auto Scaling automatically increases or decreases EC2 instances based on demand. Learned core components of Auto Scaling: Launch Configuration / Launch Template Auto Scaling Group (ASG) Scaling Policies: Dynamic Scaling and Scheduled Scaling Health Checks Used Amazon CloudWatch to monitor metrics such as CPU utilization, network traffic, and request count to support scaling decisions. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives Understand AWS security fundamentals. Learn key security-related services such as AWS KMS, Amazon Macie, and AWS Certificate Manager. Understand how to protect AWS infrastructure. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Learned data protection using AWS built-in capabilities (Amazon S3, Amazon EBS, Amazon DynamoDB) and AWS security services (AWS KMS, Amazon Macie, AWS Certificate Manager). 22/09/2025 23/09/2025 https://render.skillbuilder.aws/?module_id=TNAPD78T9R%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 3 Learned how to protect networks and applications using AWS infrastructure security (Security Groups, ELB, AWS Regions) and AWS protection services (AWS Shield, AWS WAF). 23/09/2025 24/09/2025 Same as above 4 Studied threat detection and incident response using Amazon Inspector, Amazon GuardDuty, Amazon Detective, and AWS Security Hub. 24/09/2025 25/09/2025 Same as above 5 Learned how to prevent unauthorized access using additional services such as IAM Identity Center, AWS Secrets Manager, and AWS Systems Manager. 25/09/2025 26/09/2025 Same as above 6 Practice: + Implemented Identity Federation using AWS Single Sign-On (AWS IAM Identity Center). 26/09/2025 29/09/2025 https://000012.awsstudygroup.com/ Week 3 Achievements 1. Data Protection and Governance Understood: Encryption \u0026amp; key management using AWS KMS Sensitive data discovery using Amazon Macie Certificate lifecycle management with AWS Certificate Manager Built-in security features of S3, EBS, and DynamoDB 2. Network and Application Security Learned layered security concepts: Security Groups Elastic Load Balancers (ELB) Multi-Region protection strategies Understood how AWS Shield and AWS WAF mitigate DDoS and web attacks. 3. Threat Detection \u0026amp; Incident Response Understood the roles of: Amazon Inspector (vulnerability scanning) Amazon GuardDuty (threat detection) Amazon Detective (investigation \u0026amp; analysis) AWS Security Hub (centralized security posture) 4. Preventing Unauthorized Access Learned fundamentals of: IAM Identity Center (SSO) AWS Secrets Manager AWS Systems Manager "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives Practice creating, managing, and deploying AWS resources using various tools (Console, CLI, SDK, Elastic Beanstalk, etc.). Build a prototype web application using Lambda, S3, DynamoDB, and API Gateway. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Practice: Create a table in AWS DynamoDB using the AWS CLI or Python SDK (boto3) with Access Keys. 28/09/2025 29/09/2025 3 Practice: Build a Book Store application using AWS Lambda, S3, and DynamoDB. 30/09/2025 01/10/2025 https://000078.awsstudygroup.com/ 4 Learn JSON \u0026amp; Document data models. 01/10/2025 02/10/2025 5 Practice:\n+ Deploy and verify AWS resources using an AWS CloudFormation template.\n+ Use AWS Tools for Eclipse to deploy a Java application to an Elastic Beanstalk environment.\n+ Install and configure the Elastic Beanstalk CLI.\n+ Use the EB CLI to deploy updates to an existing environment.\n+ Use the AWS SDK to query and modify AWS resources programmatically. 03/10/2025 04/10/2025 https://000050.awsstudygroup.com/ 6 Practice: Build a frontend web application that interacts with the database through Lambda and API Gateway. 04/10/2025 05/10/2025 https://000079.awsstudygroup.com/ Week 4 Achievements 1. Working with AWS DynamoDB Created and managed DynamoDB tables using AWS CLI and Python SDK (boto3). Defined Primary Keys, Sort Keys, and configured Read/Write Capacity Modes. 2. Built a Book Store Application (Lambda + S3 + DynamoDB) Created Lambda functions to perform CRUD operations on book data. Integrated API Gateway with Lambda to build a RESTful backend. Stored book images and static web assets in Amazon S3. 3. Understood JSON \u0026amp; Document Data Models Learned how DynamoDB stores semi-structured data. Queried and updated data using JSON-based structures. 4. Deployed a Java Application with Elastic Beanstalk Deployed infrastructure using AWS Console and CloudFormation templates. Installed and configured the Elastic Beanstalk CLI. Deployed updates to an existing environment. Used the AWS SDK for Java to interact programmatically with AWS. 5. Built a Frontend Web App with Serverless Backend Created a simple UI for adding, editing, and deleting items in DynamoDB. Understood the workflow: Frontend ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB. Learned basic approaches to securing serverless APIs. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives Explore and practice authentication and storage using AWS Amplify. Understand and deploy serverless applications using AWS SAM (Serverless Application Model). Learn and implement user authentication using Amazon Cognito. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 Learn AWS Amplify. 05/10/2025 06/10/2025 https://aws.amazon.com/vi/amplify/ 3 Practice: Use Amplify Authentication and Amplify Storage for serverless applications. 06/10/2025 09/10/2025 https://000134.awsstudygroup.com/ 4 Learn AWS SAM: + SAM templates and SAM CLI + SAM Accelerate and SAM CLI integration. 09/10/2025 10/10/2025 https://aws.amazon.com/vi/serverless/sam/ 5 Practice: + Install SAM CLI + Deploy front-end and Lambda functions + Configure API Gateway and test API via Postman. 10/10/2025 11/10/2025 https://000080.awsstudygroup.com/ 6 Learn Amazon Cognito and Practice: Implement authentication using Cognito. 11/10/2025 13/10/2025 https://000081.awsstudygroup.com/ Week 5 Achievements 1. Worked with AWS Amplify Learned Amplify architecture and how it integrates with serverless systems. Implemented Amplify Authentication for user registration and login flows. Used Amplify Storage to manage file uploads and access through Amazon S3. Managed Amplify environments using CLI and integrated with front-end applications. 2. Studied and Applied AWS SAM (Serverless Application Model) Learned structure of SAM templates; understood CloudFormation integration. Installed and configured SAM CLI for local development and deployment. Practiced deploying serverless applications using SAM Accelerate. Deployed Lambda functions and tested API endpoints via API Gateway using Postman. Understood workflow: SAM ‚Üí Lambda ‚Üí API Gateway ‚Üí CloudFormation. 3. Learned and Practiced Amazon Cognito Understood Cognito User Pools (authentication) and Identity Pools (authorization + AWS service access). Implemented user authentication workflows using Cognito. Integrated Cognito with AWS Amplify for seamless front-end authentication. Successfully tested sign-up, sign-in, confirmation, and token verification flows. 4. Combined AWS Amplify, SAM, and Cognito Built a fully functional serverless application supporting authentication, API interaction, and cloud deployment. Strengthened overall understanding of serverless architecture, preparing for advanced cloud-native development tasks. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives Understand and practice AWS Backup, including creating Backup Vaults, Backup Plans, and deploying backup configurations using AWS CloudFormation. Learn how AWS WAF (Web Application Firewall) and AWS PrivateLink work, including key components such as ACLs, Rules, Rule Groups, VPC Endpoint Services, and Network Load Balancers. Study AWS KMS (Key Management Service) ‚Äî covering symmetric and asymmetric key management and their role in data encryption. Gain a solid understanding of Containerization with Docker, including how to build and deploy applications using Docker Images and Docker Compose. Strengthen skills in Infrastructure as Code (IaC) and container-based deployment for secure and scalable cloud environments. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS Backup: Backup vault, Backup plan, Using CloudFormation to create backup plan 13/10/2025 15/10/2025 https://000013.awsstudygroup.com/ 3 Practice: Create backup plan, on-demand backup, backup vaults 13/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 4 Learn AWS WAF and AWS PrivateLink: ACL, Rules, Rule Groups, VPC Endpoint Service, VPC Endpoint, Network Load Balancer 15/10/2025 17/10/2025 https://000026.awsstudygroup.com/ https://000111.awsstudygroup.com/ 5 Learn AWS KMS: Symmetric Key, Asymmetric Key; Practice: Create ACLs, rules and rule groups 17/10/2025 19/10/2025 https://000033.awsstudygroup.com/ 6 Learn Containerization with Docker: What is Docker, Deploy using Docker Image, Deploy with Docker Compose and push image 19/10/2025 21/10/2025 https://000015.awsstudygroup.com/ Week 6 Achievements Learned and practiced AWS Backup, created Backup Vaults, Backup Plans, and On-demand Backups via AWS Console and CloudFormation. Configured and tested AWS WAF, created ACLs, Rules, and Rule Groups to protect web applications. Practiced AWS PrivateLink, implemented VPC Endpoint Services and VPC Endpoints with Network Load Balancer. Used AWS KMS, created and managed both Symmetric and Asymmetric Keys for secure data encryption. Built and deployed containerized applications with Docker, including Docker Images, containers, Docker Compose, and pushed images to Docker Hub. Strengthened understanding of Cloud Security, Encryption, and Containerization, preparing for real-world AWS deployments. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives Learn the AWS services used to optimize performance. Understand necessary services for performance optimization: ECS, EKS, CodePipeline, Storage Gateway. Learn about Docker, Kubernetes, and the relationship between Docker and Kubernetes. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS EKS: Control Plane (AWS managed), Worker Nodes (user-managed), Components: Cluster, Node Group, Pod, Deployment, Service 20/10/2025 21/10/2025 https://000126.awsstudygroup.com/1-introduce/ 3 Practice EKS: Create network (VPC, subnets, Internet Gateway), Auth for control plane, Create EKS cluster, Install addons (vpc-cni, kube-proxy), Auth for worker node, Create worker node, Install addon coredns, Test Nginx deployment 21/10/2025 22/10/2025 https://000126.awsstudygroup.com/1-introduce/ 4 Learn AWS ECS: Cluster, Task Definition, Task, Service, Container Agent, ECS Launch Types, Networking in ECS, Integrate with other services, ECS Auto Scaling 22/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn AWS Storage Gateway and Practice ECS: Create ECS cluster, Configure Docker image, Create Task Definition, Register namespace in Cloud Map 23/10/2025 25/10/2025 https://000016.awsstudygroup.com/1-introduction/ 6 Learn AWS CodePipeline: Source stage (GitHub, CodeCommit, S3), Build stage (CodeBuild), Deploy stage (CodeDeploy) 25/10/2025 27/10/2025 https://000017.awsstudygroup.com/ Week 7 Achievements 1. AWS EKS Understand EKS architecture: Control Plane (AWS-managed) and Worker Nodes (self-managed). Learned definitions: Cluster, Node Group, Pod, Deployment, Service. Connected kubectl to EKS Cluster and managed workloads. 2. Practice on EKS Created VPC, Subnets, Internet Gateway, Route Table. Provided authentication for EKS Control Plane (IAM Role). Created EKS Cluster, installed addons: vpc-cni, kube-proxy. Created Node Group for Worker Nodes, installed addon coredns. Deployed Nginx successfully on EKS, accessible via LoadBalancer. 3. AWS ECS Understood ECS Cluster, Task Definition, Service, Container Agent. Differentiated ECS Launch Types: EC2 vs Fargate. Learned ECS networking modes: bridge, host, awsvpc. Integrated ECS with CloudWatch, Load Balancer, Auto Scaling. 4. Practice ECS Created ECS Cluster (Fargate), built and pushed Docker images to ECR. Created Task Definition and Service to run containers. Registered namespace in Cloud Map for service discovery. Learned AWS Storage Gateway basics and hybrid storage model. 5. AWS CodePipeline Learned CI/CD pipeline stages: Source ‚Üí Build ‚Üí Deploy. Integrated CodePipeline with GitHub, CodeBuild, CodeDeploy. Automated build and deployment of Spring Boot or React applications to EC2/S3. Wrote template files: buildspec.yml and appspec.yml. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives Understand the fundamentals and use cases of AWS Step Functions, including its 7 core state types and how to orchestrate complex workflows. Gain hands-on experience creating and testing workflows in AWS Cloud9, focusing on task orchestration and error handling. Learn the key features and deployment steps of Amazon FSx, including its different variants (Windows File Server, Lustre, NetApp ONTAP, OpenZFS). Practice configuring an FSx file system integrated with AWS Managed Microsoft AD, ensuring proper setup of networking, authentication, and file sharing. Explore AWS X-Ray to understand how to trace and visualize requests across distributed applications for performance optimization and debugging. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS Step Functions: + 7 states: Task, Choice, Fail/Success, Pass, Wait, Parallel, Map + Use cases of AWS Step Functions + Benefits of AWS Step Functions 26/10/2025 27/10/2025 https://000047.awsstudygroup.com/1-intro/ 3 Practice: + Create Cloud9 environment + Create sample services + Initialize workflow + Implement error handling 27/10/2025 28/10/2025 https://000047.awsstudygroup.com/1-intro/ 4 Learn Amazon FSx: + FSx for Windows File Server + FSx for Lustre + FSx for NetApp ONTAP + FSx for OpenZFS 28/10/2025 29/10/2025 https://000025.awsstudygroup.com/ 5 Practice: + Configure file system details + Choose existing VPC + Choose AWS Managed Microsoft AD (Provide DNS, Service Account username/password) + Define Windows file share name + Review and create 29/10/2025 30/10/2025 https://000025.awsstudygroup.com/ 6 Learn AWS X-Ray: + Trace + Segment + Subsegment + Annotation / Metadata + Service Map 30/10/2025 31/10/2025 https://000140.awsstudygroup.com/ Week 8 Achievements Learned and explained the 7 states of AWS Step Functions, including their role in workflow automation and state transitions. Practiced building a sample workflow in Cloud9 with Step Functions, including error handling and execution validation. Gained understanding of different Amazon FSx options and their ideal use cases for Windows workloads, HPC, and enterprise storage. Completed hands-on setup of FSx for Windows File Server, including file system configuration, VPC/AD integration, and file share creation. Learned how AWS X-Ray collects traces, segments, subsegments, and visualized a Service Map to identify performance bottlenecks and errors. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives Understand and apply AWS AppSync for building GraphQL APIs with various data sources. Learn and configure AWS EBS Data Lifecycle Manager to automate snapshots and backups. Explore AWS GuardDuty for intelligent threat detection using ML and behavior analytics. Study AWS Macie to identify and protect sensitive data in S3 buckets using machine learning. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS AppSync: + GraphQL APIs: Query, Mutation, Subscription + Data Sources: DynamoDB, RDS/Aurora, AWS Lambda, HTTP Endpoints, OpenSearch + Authentication and Authorization: AWS IAM, API Keys, Cognito User Pools, OpenID Connect + Realtime Subscriptions, Caching, Offline support Practice: + Create GraphQL API + Define schema and attach data source + Configure request/response mapping templates 02/11/2025 03/11/2025 https://000086.awsstudygroup.com/1-introduction/ 3 Learn AWS EBS Data Lifecycle Manager: + Automate backup and recovery + Reduce storage costs + Ensure compliance and data protection + Understand Lifecycle Policies: EBS Snapshot Policy, EBS-backed AMI Policy, Cross-region/Cross-account Copy Policy 03/11/2025 05/11/2025 https://000088.awsstudygroup.com/ 4 Practice: + Launch EC2 instance and configure lifecycle policies + Define which resources to back up, schedule, and retention Learn AWS GuardDuty: + ML-based threat detection and behavioral analysis 03/11/2025 06/11/2025 https://000098.awsstudygroup.com/ 5 Learn AWS Macie: + Key Features: Data Discovery, Classification, Bucket-level Visibility + How Macie works internally + Use Cases and Pricing Model 14/08/2025 15/08/2025 https://000090.awsstudygroup.com/ 6 Practice on AWS Macie: + Create and configure an S3 bucket + Enable Macie service + Create Macie jobs to scan and classify data 15/08/2025 15/08/2025 https://000090.awsstudygroup.com/ Week 9 Achievements AWS AppSync: Built a simple GraphQL API integrated with DynamoDB and Lambda, understanding schema design, resolvers, and mapping templates. AWS EBS Lifecycle Manager: Configured automated backup policies to improve reliability and cost-efficiency in EC2 environments. AWS GuardDuty: Gained knowledge of continuous threat detection using ML and anomaly-based analysis. AWS Macie: Enabled Macie for S3, ran data classification jobs, and reviewed findings on sensitive data exposure and compliance monitoring. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn Amazon RDS,AWS DynamoDB, LightSail and EC2 Auto Scaling Week 3: Learn security\nWeek 4: Learn AWS DynamoDB and Relational Database Services\nWeek 5: authentication and storage\nWeek 6: Data encrytion\nWeek 7: optimized performance\nWeek 8: setup of networking, authentication, and file sharing\nWeek 9: Practice with security and operations\nWeek 10: Learn monitoring performance\nWeek 11: Learn Redis and more AWS Security Week 12: Learn QuickSight, Athena, and data lake\nWeek 13: Learn how to migrate to AWS\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "3.1 AWS account, Region \u0026amp; IAM Region + Choose region to deploy(ap-southeast-1) IAM baseline + Infra/Admin group: Administrator, Prefer to separate by environment (dev/stg/prod). + Stakeholders: ReadOnlyAccess + CI/CD (service roles): CodePipeline, CodeBuild, CodeDeploy + EC2 instance profile rule(run time): Read secrets from Secrets Manager follow prefix(example: metro//*),push log/metrics into CloudWatch, S3/Kinesis/SNS for function Security Notes + Do not give AdministratorAccess for EC2 Role + Using least privilege: policy for ARN resource + prefix + Turn the MFA for importmant IAM Users/roles + Activate the baseline guardrails {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS First Cloud AI Journey ‚Äì Metropolitano Railways Project Team: FPT HCM University\nClient: Metropolitano Railway Systems\nDocument Date: 12/09/2025\nTABLE OF CONTENTS BACKGROUND AND MOTIVATION\n1.1 EXECUTIVE SUMMARY\n1.2 PROJECT SUCCESS CRITERIA\n1.3 ASSUMPTIONS SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 TECHNICAL ARCHITECTURE DIAGRAM\n2.2 TECHNICAL PLAN\n2.3 PROJECT PLAN\n2.4 SECURITY CONSIDERATIONS ACTIVITIES AND DELIVERABLES\n3.1 ACTIVITIES AND DELIVERABLES\n3.2 OUT OF SCOPE\n3.3 PATH TO PRODUCTION EXPECTED AWS COST BREAKDOWN BY SERVICES TEAM RESOURCES \u0026amp; COST ESTIMATES ACCEPTANCE 1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Client background:\nRapid urbanization is increasing pressure on public transportation systems. Metropolitano Railway Systems is modernizing operations to improve reliability, passenger experience, and operational efficiency. Current on-premises systems face challenges in scalability, high availability, and real-time data processing.\nBusiness \u0026amp; Technical Objectives:\nEnsure high availability and 24/7 operations for mission-critical rail services. Enable elastic scalability to handle peak passenger loads and seasonal demand. Improve system reliability and disaster recovery capabilities. Support real-time data processing for ticketing and payment transaction monitoring, including payment status tracking and reconciliation. Strengthen security \u0026amp; compliance aligned with public-sector transportation standards. Reduce operational costs through cloud-native automation. Accelerate innovation cycles for digital services such as mobile ticketing and predictive maintenance. Use cases (POC scope):\nDigital ticketing \u0026amp; fare collection (web/app booking, QR/IC card integration). Train scheduling \u0026amp; dispatch management (contextual use case; not the primary focus of this POC). Real-time payment/transaction event analytics through Amazon Kinesis (e.g., payment status updates, settlement events, anomaly detection). BI dashboards for management decision-making using Amazon QuickSight. Incident response and alerting through centralized monitoring and logging. Summary of Professional Services (Project Team):\nDesign an AWS architecture that is secure, scalable, and fault-tolerant based on Amazon EC2 and managed services. Migrate selected workloads from on-premises infrastructure to AWS (POC scope). Implement real-time payment/ticketing event pipelines using Amazon Kinesis, with storage and analytics datasets delivered to Amazon S3. Deploy analytics using QuickSight. Set up CI/CD using CodePipeline, CodeBuild, and CodeDeploy. Provide knowledge transfer/training to ensure operational readiness for the Client‚Äôs technical team. 1.2 PROJECT SUCCESS CRITERIA Service Reliability \u0026amp; Availability\nThe deployed system must achieve ‚â• 99.9% uptime across all mission-critical services in scope (ticketing, payment monitoring, analytics). Scalability \u0026amp; Performance\nThe platform must automatically scale to handle peak passenger traffic without performance degradation. End-to-end response time for booking and payment APIs must remain under 300 ms during peak load tests (target). Real-Time Data Processing\nReal-time payment and ticketing transaction events must be processed with a latency under 5 seconds using Amazon Kinesis. Data ingestion pipelines must support at least 10,000 events/second with auto-scaling (target). Analytics \u0026amp; Insights Delivery\nManagement dashboards (QuickSight) must provide accurate, refreshed datasets within ‚â§ 5 minutes of data arrival in S3. CI/CD \u0026amp; Operational Excellence\nAll application deployments must be executed via automated CI/CD pipelines with rollback capability. Cost Efficiency\nAWS cost optimization mechanisms (Auto Scaling, RI/Savings Plans, lifecycle policies) target a reduction of at least 20% compared to on-premises operations. Cost Explorer and Billing Alarms must be configured to prevent overruns. 1.3 ASSUMPTIONS Prerequisites \u0026amp; Dependencies\nThe Client provides timely access to required environments, systems, and personnel. The Client supplies necessary credentials, API documentation, and integration endpoints for on-premises/third-party systems. Existing operational data (ticketing, scheduling, payment transactions) is available and accessible for migration and integration. Third-party vendors (payment gateways, fare systems, transit card systems) offer stable APIs and complete documentation. Required AWS accounts/organizations/billing structures are set up prior to project start. The Client identifies Subject Matter Experts (SMEs) for each domain (operations, IT, ticketing, scheduling). Technical Constraints\nSome legacy systems may remain on-premises, requiring hybrid connectivity via VPN or Direct Connect. Existing applications may have non-cloud-optimized architectures, limiting modernization within the POC scope. Legacy data quality issues may affect migration accuracy and analytics output. Operational networks and payment/ticketing systems must support secure and reliable cloud connectivity. Real-time analytics performance depends on ingestion latency and stability from payment gateways and ticketing transaction sources. Business Constraints\nProject timelines may depend on internal Client approvals/procurement processes. The Client‚Äôs organizational readiness and staff availability may impact progress. Budget limitations may restrict scope. Certain regulatory/compliance requirements may limit data residency/retention options. Risks (high-level)\nIntegration risk (legacy undocumented behaviors). Data migration risk (inconsistent/incomplete legacy data). Operational risk (hybrid/on-prem hardware failures). Security risk (misconfigured third-party endpoints). Timeline \u0026amp; dependency risk (vendor approvals, API throughput). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM Figure 1. AWS Technical Architecture Diagram ‚Äì Metropolitano Railways (POC)\nFigure 1 (POC Outcome \u0026amp; Architecture Summary):\nThe POC enables near real-time monitoring and reconciliation of ticketing/payment transactions. Payment status events are streamed into Amazon Kinesis and delivered to Amazon S3 (data lake) for analytics. Amazon QuickSight dashboards provide revenue insights (daily/monthly) and highlight unpaid/incomplete payments for follow-up. User traffic is routed via Route 53 ‚Üí CloudFront ‚Üí AWS WAF to the application on Amazon EC2, with transactional data stored in Amazon RDS (private subnet). Amazon CloudWatch supports monitoring and alerting. Asynchronous workflows are decoupled using SNS/SQS (and EventBridge where needed). Source control uses GitLab; CI/CD is implemented using CodePipeline/CodeBuild/CodeDeploy.\nProduction note (recommendations for production readiness):\nEnable Multi-AZ and place workloads in private subnets behind an ALB. Restrict egress using NAT Gateway and/or VPC Endpoints. Enforce TLS (ACM), least-privilege IAM, and store secrets in Secrets Manager/SSM. Enable CloudTrail/Config and strengthen logging (WAF/CloudFront logs). Use blue/green or canary deployments with rollback. 2.2 TECHNICAL PLAN Key activities include:\nInfrastructure provisioning using IaC templates to deploy: Amazon EC2 clusters for application workloads Amazon RDS for database services Amazon S3 for static content and data lake storage Amazon Route 53 for DNS routing Amazon CloudFront and AWS WAF for secure global content delivery Amazon Kinesis for real-time data ingestion Amazon SQS/SNS for asynchronous messaging Amazon EventBridge for event-driven workflows Amazon CloudWatch for logging, monitoring, alarms, and dashboards Amazon QuickSight for analytics and BI reporting This POC does not include AWS Lambda; streaming consumers and asynchronous/background processing are implemented on EC2-based services (or containers) and integrated via SNS/SQS/EventBridge where applicable. Application build and release processes will be automated using CodePipeline, CodeBuild, and CodeDeploy, enabling blue/green or rolling deployments with automated rollback. Configuration items requiring approvals (production DNS changes, WAF rule modifications, RDS parameter adjustments, security group updates) will follow the Client‚Äôs Change Management Process (including CAB approvals and tracked deployment records). Critical paths (ticketing workflows, payment integrations, real-time ingestion, analytics refresh flows) will undergo unit, integration, load, and failover testing. Test scenarios, acceptance criteria, and validation procedures are provided in Appendix X. 2.3 PROJECT PLAN Stakeholder Participation:\nClient stakeholders (Operations, IT, Data, Security teams) are required for: Sprint Reviews (demo and feedback) Sprint Retrospectives (continuous improvement) UAT and sign-off sessions Technical design workshops Team Responsibilities (high-level):\nCloud Architect ‚Äì AWS solution design, security architecture, scalability \u0026amp; HA patterns DevOps Engineer ‚Äì CI/CD pipelines, IaC, automated deployments Application Engineer ‚Äì application refactoring and integration Client IT Lead ‚Äì access provisioning, governance alignment Client Operations Team ‚Äì process validation, UAT testing Client Security Officer ‚Äì compliance and security controls review Data Engineer ‚Äì Kinesis pipelines, SQS/SNS integration, data modeling Analytics/BI Engineer ‚Äì QuickSight dashboards and dataset automation Communication Cadence:\nDaily: standups with Project Team Weekly: project status updates to Client stakeholders Bi-weekly: sprint review + sprint planning Monthly: steering committee meeting Ad-hoc: incident response, change approvals, design deep-dives Knowledge Transfer:\nAWS architecture overview CI/CD pipeline management Monitoring and incident response using CloudWatch Data pipeline operations (Kinesis ‚Üí S3 ‚Üí QuickSight) Infrastructure lifecycle \u0026amp; IaC updates Security and IAM operations 2.4 SECURITY CONSIDERATIONS Access Security\nImplement least-privilege IAM, role-based access control, and best-practice IAM policies. Enable MFA for privileged accounts. Restrict CI/CD access using scoped IAM roles. Apply change approvals for Route 53 DNS, CloudFront updates, and WAF rule modifications. Infrastructure Security\nDeploy Amazon EC2 in private subnets within a Multi-AZ VPC architecture (production target). Use AWS WAF to protect CloudFront distributions and application endpoints against common exploits. Enforce traffic boundaries via Security Groups and NACLs. Run Amazon RDS in Multi-AZ with encrypted storage and automated backups. Use EventBridge for security-trigger automation (e.g., config rule violations), where applicable. Data Security\nEncrypt data at rest in S3, RDS, Kinesis, and SQS using AWS KMS CMKs. Protect data in transit using TLS 1.2+ across all services. Configure S3 lifecycle policies and object versioning for retention compliance. Data lake structure follows separation of raw, processed, curated layers. Detection \u0026amp; Monitoring\nEnable AWS CloudTrail and AWS Config for full API auditing and compliance tracking. CloudWatch collects logs, metrics, application traces, alarms, and dashboards. Deliver WAF logs and CloudFront access logs to S3 for security analytics. Incident Management\nDesign incident response playbooks for system failures, security breaches, data exposure, and pipeline errors. Trigger automated alerts using SNS/EventBridge to the Client‚Äôs operations team. Implement disaster recovery procedures for critical EC2 and RDS workloads. Ensure backup snapshots comply with Client-defined retention policies. Route operational alarms to the correct on-call teams for faster MTTR. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Timeline basis: Internship duration 08/09/2025 ‚Äì 22/11/2025 (~11 weeks)\nProject Phase Timeline Activities Deliverables / Milestones Total man-day Assessment Week 1‚Äì2 (08/09‚Äì21/09) Requirements workshops (business/technical/security); current-state analysis; identify integration points (payments, ticketing, analytics); environment readiness validation Assessment report; Architecture blueprint v1; Backlog \u0026amp; sprint plan TBD Setup base infrastructure Week 3‚Äì4 (22/09‚Äì05/10) VPC/subnets/routing/SGs; IAM baseline; S3 data lake foundation; CloudFront + WAF + Route53; RDS setup; CloudWatch Infrastructure provisioned; IaC templates delivered; Networking \u0026amp; security baseline TBD Setup component 1 Week 5‚Äì6 (06/10‚Äì19/10) EC2 Auto Scaling setup; deploy application backend; configure CI/CD (CodePipeline/CodeBuild/CodeDeploy); observability dashboards Application deployed; CI/CD pipelines operational; Monitoring dashboard TBD Setup component 2 Week 7‚Äì8 (20/10‚Äì02/11) Kinesis streaming pipelines; SQS/SNS messaging; ETL to S3 data lake; QuickSight dashboards; EventBridge workflows Data pipeline operational; Event-driven architecture; Revenue dashboards (daily/monthly); Unpaid/incomplete payments report (with alert thresholds) TBD Testing \u0026amp; Go-live (POC) Week 9‚Äì10 (03/11‚Äì16/11) Unit/integration/load testing; go-live readiness review; (POC) DNS cutover; monitoring \u0026amp; rozllback plans UAT sign-off; Go-live checklist; POC launch TBD Handover Week 11 (17/11‚Äì22/11) Final documentation; operations training; knowledge transfer sessions; transition to BAU Runbooks \u0026amp; SOPs; Admin training completion; Final acceptance TBD How to calculate man-day (guideline):\nMan-day = number of people √ó number of working days actually spent on the phase. Example: 5 people work 3 days on ‚ÄúAssessment‚Äù ‚Üí 5 √ó 3 = 15 man-days. If you track by hours: 1 man-day ‚âà 8 working hours (common convention). Example: total 120 hours ‚Üí 120 / 8 = 15 man-days. 3.2 OUT OF SCOPE Custom application code development or feature enhancements not listed in the Scope of Work. On-premises infrastructure upgrades, network redesign, or hardware procurement. Performance tuning of third-party vendor systems. Mobile app development or UI/UX redesign outside the scope. Machine Learning model development beyond QuickSight \u0026amp; basic SageMaker patterns. Penetration testing or third-party security audits (unless contracted separately). Post go-live 24/7 operations unless contracted separately. Support for legacy networks or non-cloud-compatible components. Migration of data sources not included in the initial assessment. AWS Lambda‚Äìbased serverless compute (event consumers/functions) is out of scope for this POC. 3.3 PATH TO PRODUCTION The Proof of Concept (POC) environment will demonstrate selected use cases defined in Section 2.2.\nThe POC environment will not contain all production-grade capabilities.\nKey gaps requiring enhancements before production deployment include:\nFull resilience design (multi-AZ, failover, scaling policies). Complete observability coverage (CloudWatch metrics, distributed tracing, WAF logs). Hardened security baselines (advanced IAM controls, WAF tuning, encryption policies). Comprehensive testing (integration, performance, DR simulation). CI/CD hardening and automated rollback. Production-approved DNS, WAF, and network change processes. Enhanced error handling and exception flows for edge cases. 4. EXPECTED AWS COST BREAKDOWN BY SERVICES Costing considerations (high-level):\nEC2 estimated using a mix of On-Demand + Reserved Instances/Savings Plans (production). Multi-AZ RDS with automated backups. CloudFront cost includes WAF usage (rule groups + request filtering). S3 includes storage tiers (Standard, Intelligent-Tiering) and lifecycle policies. Log storage and metrics in CloudWatch. Kinesis ingest \u0026amp; processing units. Data transfer cost between components. QuickSight Reader \u0026amp; Author licenses. Messaging (SQS, SNS, EventBridge) based on estimated throughput. Route 53 DNS queries + health checks. Assumptions (for accurate pricing):\nDaily ingestion volume: X GB/day via Kinesis. EC2 sizing based on projected user load for ticketing and payment workloads. S3 storage baseline calculated for 12 months retention. Moderate WAF rule usage and CloudFront regional edge charges. SQS/SNS/EventBridge traffic estimated from projected workflow processes. QuickSight usage includes 1 Author and X Readers. 5. TEAM Name Title Description Email / Contact Info TaÃÄo BaÃâo ThaÃÄnh Team Leader Manage the project, configure AWS services, code Backend, write documents, design architecture taobaothanh365@gmail.com Nguy√™ÃÉn ThiÃ£ NhaÃÉ Uy√™n Member Design the UI and code Frontend uyenntnse183774@fpt.edu.vn Nguy√™ÃÉn BaÃâo KhaÃÅnh Member Design the UI and code Frontend baokhanhtcv2005@gmail.com Tr√¢ÃÄn VƒÉn Quy√™ÃÅt Member Code Backend quyettvse181574@fpt.edu.vn Nguy·ªÖn VƒÉn C∆∞·ªùng Member Code Backend cuongnvse183645@fpt.edu.vn Download Proposal Template\r"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives Practice with AWS core services (VPC, EC2, EBS, Postgres) Learn AWS SageMaker for AI/ML workflow Practice AI/ML data analysis with SageMaker Learn AWS Bedrock AgentCore components Learn AWS Glue and Amazon Athena for ETL and analytics Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Practice: + Creating VPCs + Initializing EC2 instances + Creating an EBS volume + Assigning EBS volume + Installing Postgres + Mount EBS on EC2 production + Postgres backup + Mount EBS on EC2 test + Data recovery 08/11/2025 10/11/2025 https://100000.awsstudygroup.com/ 3 Learn AWS SageMaker AI: + What is SageMaker + Components + End-to-End Workflow 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 4 Practice: Use Amazon SageMaker AI to analyze data, import Excel files, choose data types, and export results including charts 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 5 Learn AWS Bedrock AgentCore: + Identity, Memory, Code Interpreter, Browser, Gateway, Observability + Common use cases: equip agents with built-in tools, deploy securely at scale, test and monitor agents 12/11/2025 14/11/2025 https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html 6 Learn AWS Glue and Amazon Athena 14/11/2025 16/11/2025 https://000040.awsstudygroup.com/ Week 10 Achievements 1. AWS Core Services Practice: Created VPCs with proper subnets, route tables, and Internet Gateways. Launched EC2 instances for production and test environments. Created and attached EBS volumes to EC2 instances. Installed and configured PostgreSQL on EC2 instances. Mounted EBS volumes on both production and test instances. Performed PostgreSQL backup and data recovery successfully. 2. AWS SageMaker AI Learning \u0026amp; Practice: Understood SageMaker components: Notebook, Training, Endpoint, Model, Pipelines. Completed end-to-end AI/ML workflow: imported Excel datasets, analyzed data, visualized charts. Learned to choose proper data types, clean data, and export results for further use. 3. AWS Bedrock AgentCore Learning: Learned core components: Identity, Memory, Code Interpreter, Browser, Gateway, Observability. Explored common use cases: equipping agents with tools, secure deployment at scale, monitoring and testing agents. 4. AWS Glue \u0026amp; Amazon Athena Learning: Practiced building ETL pipelines using AWS Glue. Learned to catalog datasets and transform data for analytics. Queried S3 data using Amazon Athena and verified results. Connected Athena queries to QuickSight dashboards for reporting and visualization. 5. AWS CLI \u0026amp; Management Console Proficiency: Configured AWS CLI with Access Key, Secret Key, and Default Region. Used CLI to retrieve regions, list services, and check configurations. Managed AWS resources via CLI and Web Console interchangeably. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives Understand and practice AWS services: ElastiCache (Redis), Certificate Manager, Inspector, Detective, Systems Manager Learn the structure, operation, and benefits of each service Practice connecting, granting permissions, deploying, and performing security checks on AWS services Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn Amazon ElastiCache - Redis: + What is Redis + Components: Clusters, ElastiCache nodes, Redis shards + How it works + Benefits of using ElastiCache 17/11/2025 18/11/2025 https://000061.awsstudygroup.com/1-introduce/ 3 Learn AWS Certificate Manager Practice: + Create cluster + Grant access to the cluster + Connect to cluster node 18/11/2025 19/11/2025 https://000061.awsstudygroup.com/ https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html 4 Learn AWS Inspector: + Identify security gaps + Detect misconfigurations + Apply remediation steps 19/11/2025 20/11/2025 https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html 5 Learn AWS Detective 21/11/2025 22/11/2025 https://docs.aws.amazon.com/detective/latest/userguide/what-is-detective.html 6 Practice with AWS Systems Manager 22/11/2025 24/11/2025 https://000031.awsstudygroup.com/1-introduce/ Week 11 Achievements Completed theory and hands-on practice with ElastiCache (Redis): learned about Redis, clusters, nodes, shards, how it works, and its benefits. Practiced creating Redis clusters, granting access, and connecting to nodes. Learned and practiced AWS Certificate Manager: creating and deploying certificates on clusters. Learned and practiced AWS Inspector: identifying security gaps, misconfigurations, and remediation steps. Learned AWS Detective: detecting and investigating security events. Practiced AWS Systems Manager: centralized management and operation of AWS resources. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives Understand and practice AWS QuickSight and AWS Athena. Build a data lake from multiple data sources. Deploy a web application using Elastic Beanstalk and CDK CI/CD pipelines. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS QuickSight:\n+ Overview + Architecture + Components + SPICE 25/11/2025 26/11/2025 https://000073.awsstudygroup.com/ 3 Practice with AWS QuickSight 26/11/2025 27/11/2025 https://000073.awsstudygroup.com/ 4 Learn and Practice AWS Athena 27/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Practice building a data lake 29/11/2025 30/11/2025 https://000070.awsstudygroup.com/ 6 Practice deploying web app with Elastic Beanstalk and CDK pipeline 01/12/2025 02/12/2025 https://000113.awsstudygroup.com/ Week 12 Achievements AWS QuickSight\nLearned overview, architecture, and main components (Dashboard, Analysis, Dataset). Understood SPICE engine to accelerate queries. Practiced creating a basic dashboard and connecting datasets from S3/Athena. AWS Athena\nLearned how to query data stored on S3 using SQL. Practiced creating tables, views, and running sample queries. Data Lake\nBuilt a small data lake on S3 to consolidate multiple data sources. Performed data cleaning and transformation before analysis. Elastic Beanstalk \u0026amp; CDK\nDeployed a demo web application on Elastic Beanstalk. Configured a basic CDK pipeline for automatic deployment. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.13-week13/",
	"title": "Week 13 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 13 Objectives This week focuses on learning and practicing AWS services related to infrastructure migration, database conversion, and disaster recovery. Specifically:\nLearn and practice AWS VM Import/Export. Study and use AWS Database Migration Service (DMS) and AWS Schema Conversion Tool (SCT). Learn and practice AWS Elastic Disaster Recovery (DRS). Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Learn AWS VM Import/Export 02/12/2025 03/12/2025 https://000014.awsstudygroup.com/ 3 Practice AWS VM Import/Export 03/12/2025 04/12/2025 https://000014.awsstudygroup.com/ 4 Learn \u0026amp; Practice AWS DMS and SCT 04/12/2025 05/12/2025 https://000043.awsstudygroup.com/ 5 Learn AWS Elastic Disaster Recovery 05/12/2025 06/12/2025 https://000100.awsstudygroup.com/ 6 Practice AWS Elastic Disaster Recovery 06/12/2025 07/12/2025 https://000113.awsstudygroup.com/ Achievements 1. AWS VM Import/Export Learned how to import VMs (OVA/VMDK) to AWS. Created the vmimport IAM role and configured S3 for VM storage. Successfully imported an AMI and exported it back to OVA format. 2. AWS Database Migration Service + Schema Conversion Tool Used SCT to analyze and convert schema across different database engines (ex: SQL Server ‚Üí PostgreSQL). Fixed incompatibilities reported by SCT. Configured DMS replication instance and endpoints. Performed full-load and CDC (Change Data Capture) replication with continuous synchronization. 3. AWS Elastic Disaster Recovery Understood architecture: replication agent, replication server, staging area subnet. Installed the DRS agent on source machines and replicated to AWS. Executed test failover and validated RPO/RTO performance. Completed successful failback to the original environment. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-secrets-manager/",
	"title": "Create a secret manager",
	"tags": [],
	"description": "",
	"content": "Using AWS Secrets Manager In this section, you will create secrets manager to store a secret. The secrets manager will store the secrets key from your project like username and password from database, jwt secret key, and vnpay. This will useful to store on AWS and don\u0026rsquo;t need to worry about all secret will publish\nContent Create secret manager "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 -AWS services scale to new heights for Prime Day 2025: key metrics and milestones Prime Day 2025 set new records powered by AWS. More than 7,000 ASRS robots at Amazon fulfillment centers executed over 524 million commands, peaking at 8 million per hour‚Äîa 160% increase over 2024. With AWS Outposts ensuring low-latency operations and Amazon‚Äôs retail expertise, customers quickly found deals, accessed product information, and enjoyed fast same-day or next-day delivery.\nBlog 2 - AWS named as a Leader in 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms and Container Management In the era of booming cloud computing and artificial intelligence, businesses are constantly seeking technology platforms that enable faster innovation, greater agility, and better cost optimization. With its comprehensive ecosystem of services, AWS not only supports millions of customers worldwide on their digital transformation journey but also continues to affirm its pioneering position by being recognized by Gartner as a Leader in several key categories of the 2025 Magic Quadrant.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Cloud Mastery Series #2 ‚Äì DevOps on AWS Event Objectives Understand the core principles of DevOps culture and key performance metrics (DORA). Master the AWS DevOps ecosystem for building end-to-end CI/CD pipelines. Compare and select appropriate Infrastructure as Code (IaC) tools (CloudFormation vs. AWS CDK). Explore containerization strategies using Amazon ECS, EKS, and App Runner. Implement full-stack observability using CloudWatch and AWS X-Ray. Time \u0026amp; Venue Time: 08:30 ‚Äì 17:00, Monday, November 17, 2025\nVenue: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nSpeakers Truong Quang Tinh - DevOps Engineer, TymeX | AWS Community Builder Kha Van - Cloud Security Engineer | AWS Community Builders Bao Huynh ‚Äì AWS Community Builder Thinh Nguyen ‚Äì AWS Community Builder Vi Tran ‚Äì AWS Community Builder Key Highlights 1. DevOps Culture \u0026amp; Metrics Cultural Shift: Moving from siloed development and operations to a unified culture of shared responsibility. Key Metrics (DORA): Focusing on four critical indicators to measure performance: Deployment Frequency (DF) Lead Time for Changes (LT) Mean Time to Restore (MTTR) Change Failure Rate (CFR) 2. AWS DevOps Services ‚Äì CI/CD Pipeline Source Control: Strategies for Git management (GitFlow vs. Trunk-based development) using AWS CodeCommit. Orchestration: Automating the release process with AWS CodePipeline. Deployment Strategies: Blue/Green: Reduces downtime and risk by running two identical environments. Canary: Rolls out changes to a small subset of users first. Rolling: Updates instances incrementally. 3. Infrastructure as Code (IaC) AWS CloudFormation: The declarative approach using JSON/YAML templates to define infrastructure; utilizes drift detection to maintain state consistency. AWS CDK (Cloud Development Kit): The imperative approach allowing developers to define cloud resources using familiar programming languages (TypeScript, Python, Java). CDK constructs allow for reusable infrastructure patterns. 4. Container Services \u0026amp; Observability Compute Evolution: Selection criteria between Amazon ECS (tight control), Amazon EKS (Kubernetes standard), and AWS App Runner (simplified for developers). Observability: Going beyond simple monitoring by integrating CloudWatch (Logs, Metrics, Alarms) with AWS X-Ray for distributed tracing, enabling root-cause analysis in microservices. Key Takeaways Design Mindset Automation First: Manual processes are error-prone; everything from testing to infrastructure provisioning should be automated. Shift-Left Security: Integrating security checks early in the CI/CD pipeline rather than at the end. Measure Everything: You cannot improve what you do not measure. DORA metrics are the compass for DevOps maturity. Technical Architecture Immutable Infrastructure: Servers are never modified after deployment; they are replaced. Pipeline Orchestration: A robust pipeline must include build, unit test, integration test, and approval gates before production deployment. Traceability: In distributed systems, correlating logs and traces (via X-Ray) is mandatory for debugging. Modernization Strategy Right-sizing Compute: Use App Runner for simple web apps to reduce operational overhead, and reserve EKS for complex, orchestrated microservices. IaC Adoption: Moving away from manual console clicks to code-defined infrastructure to ensure reproducibility and disaster recovery. Applying to Work Adopt Trunk-based Development: Streamline the git workflow in the team project to reduce merge conflicts and accelerate integration. Implement CI/CD: Build a pipeline using AWS CodePipeline that automatically deploys the student management backend upon pushing to the main branch. Migrate to IaC: Refactor the current manual setup of DynamoDB and Lambda functions into AWS CDK scripts for better maintainability. Enhance Monitoring: Set up CloudWatch Alarms for critical API errors (5xx) to proactively detect issues before the demo. Event Experience Attending the ‚ÄúDevOps on AWS‚Äù workshop was instrumental in bridging the gap between coding and operations. It provided a roadmap for building reliable, scalable systems. Key experiences included:\nLearning from practitioners: The speakers, being active AWS Community Builders, shared \u0026ldquo;war stories\u0026rdquo; and real-world pitfalls in DevOps, not just textbook theory. I gained clarity on the Shared Responsibility Model within a DevOps context. Hands-on technical exposure: Participated in a live CI/CD walkthrough, witnessing raw code transform into a deployed application via an automated pipeline in minutes. Experienced the power of AWS CDK by deploying a VPC and ECS cluster with significantly fewer lines of code compared to raw CloudFormation templates. Visualized distributed tracing with AWS X-Ray, understanding how to pinpoint latency bottlenecks in microservices. Networking and discussions: Engaged in discussions about the trade-offs between GitFlow and Trunk-based development for small teams and received guidance on the AWS Certified DevOps Engineer ‚Äì Professional certification roadmap. Lessons learned IaC is non-negotiable: For any long-term project, infrastructure must be code. Observability is crucial: Building a system is half the battle; knowing it is healthy is the other half. Culture over tools: Tools like Jenkins or CodePipeline are useless without a culture of collaboration and continuous improvement. Some event photos Overall, the event not only provided technical skills in AWS tools but also instilled a professional DevOps mindset, preparing me to build production-grade systems efficiently.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Event 1 Event Name: Data science on AWS Workshop\nDate \u0026amp; Time: 09:00, October 16, 2025\nLocation: Academic hall, FPT HCM University, High Tech,Thu Duc City, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Generative AI with Amazon Bedrock\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 **Event Name:**AWS Cloud Mastery Series #2 ‚Äì DevOps on AWS\n**Date \u0026amp; Time:**9:00, November 17, 2025\n**Location:**26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\n**Role\u0026quot;**Attendee\nEvent 4 **Event Name:**AWS Cloud Mastery Series #3 - ‚ÄãTheo AWS Well-Architected Security Pillar\n**Date \u0026amp; Time:**9:00, November 29. 2025\n**Location:**26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\n**Role\u0026quot;**Attendee\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/",
	"title": "Prepare Environment Configuration",
	"tags": [],
	"description": "",
	"content": "AWS Metropolitano Workshop In this section, you will prepare the entire environment for the Metropolitano system running on AWS. This environment includes real-time data processing services, data storage, backend API, data analytics, and CI/CD pipeline.\nThe goal of this section is to build a complete architecture for operating the Metropolitano train system, including:\nReal-time event streaming ‚Äî processing passenger count, train status, and ticket transactions Event-driven processing using Amazon SQS and SNS RDS for persistent data storage S3 as a data lake for logs, raw data, and analytical datasets EventBridge for orchestrating events across the system CloudWatch for system monitoring QuickSight for data visualization CodePipeline + CodeBuild + CodeDeploy for automated deployment Metropolitano Architecture Overview The architecture below represents the ticketing system, passenger flow, data streaming, and event processing pipeline:\nWhy Prepare the Environment? The Metropolitano system uses multiple AWS services. Preparing the environment ensures:\nService-to-service connectivity ‚Äî API Gateway ‚áÑ Lambda ‚áÑ RDS ‚áÑ S3 ‚áÑ DynamoDB Real-time data pipelines using Kinesis, SQS, SNS, and EventBridge Network infrastructure (VPC) for internal communication without using public internet Analytics readiness using QuickSight and an S3 Data Lake Automated application deployment using CodePipeline What You Will Configure in This Section During the Prepare Environment phase, you will create and configure the following:\nVPC \u0026amp; Networking\nSubnets, Route Tables, Security Groups Private \u0026amp; public networking for EC2, RDS, and Lambda EC2 Instances\nSimulated backend or worker nodes IAM Roles for S3 / DynamoDB / Kinesis access Amazon RDS\nPrimary database for the Metropolitano system Stores ticketing, user, and historical data S3 Data Lake\nraw/ (raw data) analytics/ (processed/analytical data) app-assets/ (frontend static assets) Kinesis Data Stream\nCollects real-time passenger metrics and train status SQS + SNS\nHandles transaction queues Sends delay or incident notifications EventBridge\nRoutes events between API, Lambda, Payment, and Alerts CloudWatch\nLogs, Metrics, Alarms, Dashboards QuickSight\nProvides analytical dashboards for system monitoring CodeBuild + CodeDeploy + CodePipeline\nAutomates backend build ‚Üí test ‚Üí deployment After Completing This Section You will have a fully prepared AWS environment ready to deploy the entire Metropolitano application, ensuring:\nReliable data storage Real-time event processing Flexible API integration Automated CI/CD pipeline Powerful analytics system "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Cloud Mastery Series #3 - ‚ÄãTheo AWS Well-Architected Security Pillar Event Objectives Deeply understand the Security Pillar within the AWS Well-Architected Framework. Master the core principles: Least Privilege, Zero Trust, and Defense in Depth. Learn how to implement security controls across 5 key areas: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. Identify top security threats in the cloud environment specifically for the Vietnamese market. Time \u0026amp; Venue Time: 08:30 ‚Äì 12:00, Saturday, November 29, 2025\nVenue: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nSpeakers Le Vu Xuan An - Software \u0026amp; Cloud Engineer | AWS Cloud Club Captain HCMUTE Tran Doan Cong Ly - DevOps Engineer | AWS FCAJ Ambassador | AWS Cloud Club Captain PTIT Danh Hoang Hieu Nghi ‚Äì AI Engineer, Renova Cloud | AWS First Cloud AI Journey | AWS Cloud Club Captain HUFLIT Tran Duc Anh - Cloud Security Engineer Trainee | First Cloud AI Journey | AWS Cloud Club Captain SGU Nguyen Tuan Thinh - Cloud Engineer Trainee | First Cloud AI Journey Nguyen Do Thanh Dat - Cloud Engineer Trainee | First Cloud AI Journey ƒêinh Le Hoang Anh ‚Äì Cloud Engineer Trainee | First Cloud AI Journey Kha Van - Cloud Security Engineer | AWS Community Builders Special Guest:\nMendel Grabski - Cloud Security \u0026amp; Solution Architect | Enabling Secure-by-Design Solutions Truong Quang Tinh - DevOps Engineer, TymeX | AWS Community Builder Key Highlights 1. Security Foundations \u0026amp; Identity (IAM) Core Principles: Moved beyond perimeter security to a Zero Trust model where every request must be authenticated and authorized. Modern IAM: Transition from long-term credentials (IAM Users with Access Keys) to temporary credentials via IAM Roles and Identity Center (SSO). Enforcing Least Privilege using Access Analyzer to validate policies. Multi-Account Strategy: Using SCPs (Service Control Policies) to set permission boundaries across the organization. 2. Detection \u0026amp; Continuous Monitoring Centralized Visibility: utilizing Security Hub to aggregate alerts from GuardDuty, Inspector, and Macie. Logging Strategy: Enabling logs at all layers is non-negotiable (VPC Flow Logs for network, CloudTrail for API calls, S3 Server Access Logs). Detection-as-Code: Defining alert rules as code to ensure consistent monitoring across environments. 3. Infrastructure \u0026amp; Data Protection Network Security: Implementing Defense in Depth with VPC segmentation (Public/Private subnets), Security Groups (stateful), and WAF/Shield for edge protection. Encryption: At-rest: Using KMS with automatic key rotation for EBS, RDS, and S3. In-transit: Enforcing TLS 1.2+ for all data movement. Secrets Management: replacing hardcoded credentials in code with AWS Secrets Manager or Parameter Store. 4. Incident Response (IR) Automation IR Lifecycle: Preparation ‚Üí Detection \u0026amp; Analysis ‚Üí Containment, Eradication \u0026amp; Recovery ‚Üí Post-Incident Activity. Playbooks: Detailed walkthroughs for handling common scenarios like compromised IAM keys or public S3 buckets. Automation: Using Lambda and Step Functions to auto-remediate issues (e.g., automatically revoking a compromised user\u0026rsquo;s session). Key Takeaways Security Mindset Security is everyone\u0026rsquo;s responsibility: It is not just for the security team; developers must practice \u0026ldquo;Security by Design\u0026rdquo;. Assume Breach: Always design systems assuming that a component might be compromised, limiting the \u0026ldquo;blast radius\u0026rdquo;. Technical Architecture Identity as the new perimeter: In a cloud-native world, IAM is more critical than the network firewall. Immutable Infrastructure: Patching servers in place is risky; prefer replacing them with new, patched images to prevent malware persistence. Applying to Work Refactor IAM: Immediately review the group project\u0026rsquo;s IAM policies. Remove any *:* permissions and replace hardcoded Access Keys with IAM Roles for EC2/Lambda. Secure Secrets: Migrate database credentials from .env files to AWS Systems Manager Parameter Store. Enable GuardDuty: Activate GuardDuty in the project account to detect anomalies (cryptojacking, unauthorized access) during the development phase. Encryption: Enable default encryption (SSE-S3) for all S3 buckets used in the Student Management System project. Event Experience The workshop provided a concentrated, high-intensity dive into cloud security. Unlike general overviews, this session focused on actionable patterns:\nInteractive Learning: The mini-demo on validating IAM Policies and simulating access helped visualize how easily permissions can be misconfigured. Real-world relevance: Discussing \u0026ldquo;Top threats in Vietnam\u0026rdquo; made the content highly relatable, emphasizing the need to protect against common misconfigurations and credential leaks. Practical Automation: Seeing an automated IR playbook triggered by EventBridge and executed by Lambda changed my perspective on how to handle security incidents at scale. Some Event Photos This event shifted my perspective from \u0026ldquo;building it fast\u0026rdquo; to \u0026ldquo;building it secure.\u0026rdquo; I now have a clear roadmap to harden our applications before they reach production.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Set up the project called Metropolitano Overview The Metropolitano AWS Architecture provides a secure, scalable, and highly available cloud environment for running transportation management workloads. This system leverages multiple AWS services to ensure high performance, strong security controls, fast content delivery, operational monitoring, and continuous deployment. In this workshop, you will learn how the Metropolitano platform is structured on AWS and how different components integrate to deliver a seamless experience for both end-users and internal operators.\nThroughout the lab, you will explore how the system uses core AWS components such as CloudFront, S3, Route 53, EC2, RDS, EventBridge, SQS, SNS, Kinesis, and CodePipeline.\nContent Workshop overview Prerequiste Create a secret manager Prepare enviroment Clean up "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.5-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Clean Up Resources 1. Networking \u0026amp; VPC Clean Up 1.1 Delete Route 53 Hosted Zone \u0026amp; Resolver Rules Navigate to Route 53 ‚Üí Hosted Zones.\nDelete the Private Hosted Zone s3.us-east-1.amazonaws.com.\nGo to Route 53 Resolver ‚Üí Rules.\nDisassociate the rule myS3Rule from VPC Onprem, then delete it.\n1.2 Delete VPC Resources Steps:\nGo to EC2 ‚Üí Network Interfaces\nDelete ENIs created for VPC Endpoints, Resolver Endpoints, or EC2 instances after termination. Go to VPC console:\nDelete Security Groups created for the lab. Delete Subnets (ensure no ENIs or route tables attached). Delete Route Tables. Delete Internet Gateway (detach first). Delete NAT Gateway (if created). Finally, delete the VPC metropolitano. 2. EC2 Clean Up Go to EC2 ‚Üí Instances. Select instance metropolitano-version-1 and Terminate. Delete: Elastic IPs (if allocated) Key pair myKey.pm (local file optional) Security group public-web-sg 3. RDS Clean Up Go to RDS Console ‚Üí Databases. Select SQL Server instance and choose Delete. Options: Disable final snapshot (optional) Delete the DB Subnet Group private-db-metropolitano. Delete the security group private-db-sg if unused. 4. S3 Clean Up Go to S3 Console. Empty the bucket metropolitano-2025. Click Delete bucket. 5. CloudFront Clean Up Go to CloudFront ‚Üí Distributions. Select the distribution. Choose Disable ‚Üí Wait for status ‚ÄúDisabled‚Äù. Click Delete. 6. Kinesis Data Stream Clean Up Go to Kinesis Console ‚Üí Data Streams. Select metropolitano-stream. Click Delete. 7. EventBridge Clean Up Go to EventBridge ‚Üí Rules. Select metropolitano-event-rule. Click Delete. If event buses or targets were created manually, delete them too. 8. SQS Queue Clean Up Go to SQS Console ‚Üí Queues. Select metropolitano-queue. Click Delete. 9. SNS Clean Up Go to SNS Console ‚Üí Topics. Select metropolitano-alerts. Delete subscriptions (email/SMS). Delete topic. 10. CloudWatch Clean Up 10.1 Delete Alarms CloudWatch ‚Üí Alarms ‚Üí Select alarms ‚Üí Delete. 10.2 Delete Logs CloudWatch ‚Üí Log Groups Delete: EC2 log groups Lambda log groups (if any) VPC Flow Logs (if created) 10.3 Delete Metrics (optional) Metrics automatically expire; no action required.\n11. QuickSight Clean Up If enabled, QuickSight can generate monthly cost.\nSteps:\nGo to QuickSight Console ‚Üí Manage QuickSight.\nDelete:\nDatasets SPICE datasets Dashboards Analyses If not needed ‚Üí Unsubscribe QuickSight account.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Company from 09/2025 to 12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Learning how to use all services and how they work on AWS, through which I improved my skills in practicing workshop, apply the AWS services in my application. Also i have to optimize the cost, performance and increased security layers\nWhen i am in the company, i always to check my to do list and check all the task that i haven\u0026rsquo;t done yesterday. Besides that i have chance to meet my mentor and ask them help when i have an issue or something. In the progress of study, me with my teammates have to build the websites and using AWS services.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚òê ‚úÖ ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚òê ‚úÖ 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚òê ‚úÖ ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚òê ‚úÖ ‚òê Needs Improvement I am internal person so i barely communicated with my mentor, i need to change my how to communicate with my mentor and spend time to meet them to ask them about my problem. I need improve my project management skill because i have to train my teammates and give them a task but i don\u0026rsquo;t have a method like Agile or Scrum to develop a software like coworker. I always to think about how the services work and how they apply to my application but sometime i don\u0026rsquo;t understand them, so i need to spend time to improve my logic mindset to understand them. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/7-feedback/",
	"title": "Sharing &amp; Feedback",
	"tags": [],
	"description": "",
	"content": " Here you can freely provide personal feedback about your experience participating in the First Cloud Journey program, helping the FCJ team improve areas that need attention based on the following categories:\nGeneral Evaluation 1. Work Environment\nThe work environment is very friendly and open. FCJ members are always willing to support me whenever I face difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think additional team bonding or networking sessions could help everyone understand each other more.\n2. Support from Mentors / Team Admins\nMentors provide detailed guidance, explain clearly when I don‚Äôt understand, and always encourage me to ask questions. The admin team supports administrative tasks, documents, and creates conditions for smooth work. I appreciate that mentors allow me to try and solve problems on my own instead of just giving answers.\n3. Job Fit with Academic Background\nThe tasks assigned match my knowledge from school while also expanding into new areas I had not encountered before. This allowed me to reinforce my foundational knowledge while learning practical skills.\n4. Learning Opportunities \u0026amp; Skill Development\nDuring the internship, I learned many new skills such as using project management tools, teamwork, and professional communication in a corporate environment. Mentors also shared practical experiences, helping me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: people respect each other, work seriously but remain cheerful. When urgent projects arise, everyone helps each other regardless of position. This made me feel part of the team even as an intern.\n6. Intern Benefits / Policies\nThe company provides internship allowances and offers flexible working hours when needed. Additionally, participating in internal training sessions is a great advantage.\nOther Questions What are you most satisfied with during the internship?\nDuring the learning process, I felt comfortable adapting to the company‚Äôs work culture and was very satisfied with the guidance from mentors. What do you think the company needs to improve for future interns?\nThe company could improve opportunities for students to come to the office daily. If recommending to friends, would you advise them to intern here? Why?\nIf I were to introduce this to friends, I would recommend it because Amazon Web Services Vietnam is not only a cloud computing company but also has a large AWS study group community in Vietnam, allowing students to learn about cloud computing. Suggestions \u0026amp; Wishes Any advice to enhance the internship experience?\nI would like to join the company club to learn more from mentors and see how they lead a team. Would you like to continue this program in the future?\nI would like to be more closely associated with the company. Other feedback (free sharing)\nNo additional feedback at this time. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.14-week14/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]