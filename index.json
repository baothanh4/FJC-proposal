[
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "AWS services scale to new heights for Prime Day 2025: key metrics and milestones Amazon Prime Day 2025 was the biggest Amazon Prime Day shopping event ever, setting records for both sales volume and total items sold during the 4-day event. Prime members saved billions while shopping Amazon‚Äôs millions of deals during the event.\nThis year marked a significant transformation in the Prime Day experience through advancements in the generative AI offerings from Amazon and AWS. Customers used Alexa+‚Äîthe Amazon next-generation personal assistant now available in early access to millions of customers‚Äîalong with the AI-powered shopping assistant, Rufus, and AI Shopping Guides. These features, built on more than 15 years of cloud innovation and machine learning expertise from AWS, combined with deep retail and consumer experience from Amazon, helped customers quickly discover deals and get product information, complementing the fast, free delivery that Prime members enjoy year-round.\nAs part of our annual tradition to tell you about how AWS powered Prime Day for record-breaking sales, I want to share the services and chart-topping metrics from AWS that made your amazing shopping experience possible.\nPrime Day 2025 ‚Äì all the numbers During the weeks leading up to big shopping events like Prime Day, Amazon fulfillment centers and delivery stations work to get ready and ensure operations run efficiently and safely. For example, the Amazon automated storage and retrieval system (ASRS) operates a global fleet of industrial mobile robots that move goods around Amazon fulfillment centers.\nAWS Outposts, a fully managed service that extends the AWS experience on-premises, powers software applications that manage the command-and-control of Amazon ASRS and supports same-day and next-day deliveries through low-latency processing of critical robotic commands.\nDuring Prime Day 2025, AWS Outposts at one of the largest Amazon fulfillment centers sent more than 524 million commands to over 7,000 robots, reaching peak volumes of 8 million commands per hour‚Äîa 160 percent increase compared to Prime Day 2024.\nHere are some more interesting, mind-blowing metrics:\nAmazon Elastic Compute Cloud (Amazon EC2) ‚Äì During Prime Day 2025, AWS Graviton, a family of processors designed to deliver the best price performance for cloud workloads running in Amazon EC2, powered more than 40 percent of the Amazon EC2 compute used by Amazon.com. Amazon also deployed over 87,000 AWS Inferentia and AWS Trainium chips ‚Äì custom silicon chips for deep learning and generative AI training and inference ‚Äì to power Amazon Rufus for Prime Day.\nAmazon SageMaker AI ‚Äî Amazon SageMaker AI, a fully managed service that brings together a broad set of tools to enable high-performance, low-cost machine learning (ML), processed more than 626 billion inference requests during Prime Day 2025.\nAmazon Elastic Container Service (Amazon ECS) and AWS Fargate‚Äì Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that works seamlessly with AWS Fargate, a serverless compute engine for containers. During Prime Day 2025, Amazon ECS launched an average of 18.4 million tasks per day on AWS Fargate, representing a 77 percent increase from the previous year‚Äôs Prime Day average.\nAWS Fault Injection Service (AWS FIS) ‚Äì We ran over 6,800 AWS FIS experiments‚Äîover eight times more than we conducted in 2024‚Äîto test resilience and ensure Amazon.com remains highly available on Prime Day. This significant increase was made possible by two improvements: new Amazon ECS support for network fault injection experiments on AWS Fargate, and the integration of FIS testing in continuous integration and continuous delivery (CI/CD) pipelines.\nAWS Lambda ‚Äì AWS Lambda, a serverless compute service that lets you run code without managing infrastructure, handled over 1.7 trillion invocations per day during Prime Day 2025.\nAmazon API Gateway ‚Äì During Prime Day 2025, Amazon API Gateway, a fully managed service that makes it easy to create, maintain, and secure APIs at any scale, processed over 1 trillion internal service requests‚Äîa 30 percent increase in requests on average per day compared to Prime Day 2024.\nAmazon CloudFront ‚Äì Amazon CloudFront, a content delivery network (CDN) service that securely delivers content with low latency and high transfer speeds, delivered over 3 trillion HTTP requests during the global week of Prime Day 2025, a 43 percent increase in requests compared to Prime Day 2024.\nAmazon Elastic Block Store (Amazon EBS) ‚Äì During Prime Day 2025, Amazon EBS, our high-performance block storage service, peaked at 20.3 trillion I/O operations, moving up to an exabyte of data daily.\nAmazon Aurora ‚Äì On Prime Day, Amazon Aurora, a relational database management system (RDBMS) built for high performance and availability at global scale for PostgreSQL, MySQL, and DSQL, processed 500 billion transactions, stored 4,071 terabytes of data, and transferred 999 terabytes of data.\nAmazon DynamoDB ‚Äì Amazon DynamoDB, a serverless, fully managed, distributed NoSQL database, powers multiple high-traffic Amazon properties and systems including Alexa, the Amazon.com sites, and all Amazon fulfillment centers. Over the course of Prime Day, these sources made tens of trillions of calls to the DynamoDB API. DynamoDB maintained high availability while delivering single-digit millisecond responses and peaking at 151 million requests per second.\nAmazon ElastiCache ‚Äì During Prime Day, Amazon ElastiCache, a fully managed caching service delivering microsecond latency, peaked at serving over 1.5 quadrillion daily requests and over 1.4 trillion requests in a minute.\nAmazon Kinesis Data Streams ‚Äì Amazon Kinesis Data Streams, a fully managed serverless data streaming service, processed a peak of 807 million records per second during Prime Day 2025.\nAmazon Simple Queue Service (Amazon SQS) ‚Äì During Prime Day 2025, Amazon SQS ‚Äì a fully managed message queuing service for microservices, distributed systems, and serverless applications ‚Äì set a new peak traffic record of 166 million messages per second.\nAmazon GuardDuty ‚Äì During Prime Day 2025, Amazon GuardDuty, an intelligent threat detection service, monitored an average of 8.9 trillion log events per hour, a 48.9 percent increase from last year‚Äôs Prime Day.\nAWS CloudTrail ‚Äì AWS CloudTrail, which tracks user activity and API usage on AWS, as well as in hybrid and multicloud environments, processed over 2.5 trillion events during Prime Day 2025, compared to 976 billion events in 2024.\nPrepared to scale\nIf you‚Äôre preparing for similar business-critical events, product launches, and migrations, I recommend that you take advantage of our newly branded AWS Countdown (formerly known as AWS Infrastructure Event Management, or IEM). This comprehensive support program helps assess operational readiness, identify and mitigate risks, and plan capacity, using proven playbooks developed by AWS experts. We‚Äôve expanded to include: generative AI implementation support to help you confidently launch and scale AI initiatives; migration and modernization support, including mainframe modernization; and infrastructure optimization for specialized sectors including election systems, retail operations, healthcare services, and sports and gaming events.\nI look forward to seeing what other records will be broken next year!\n‚Äî Channy\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\rAWS named as a Leader in 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms and Container Management A month ago, I shared that Amazon Web Services (AWS) is recognized as a Leader in 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services (SCPS), with Gartner naming AWS a Leader for the fifteenth consecutive year.\nIn 2024, AWS was named as a Leader in the Gartner Magic Quadrant for AI Code Assistants, Cloud-Native Application Platforms, Cloud Database Management Systems, Container Management, Data Integration Tools, Desktop as a Service (DaaS), and Data Science and Machine Learning Platforms as well as the SCPS. In 2025, we were also recognized as a Leader in the Gartner Magic Quadrant for Contact Center as a Service (CCaaS), Desktop as a Service and Data Science and Machine Learning (DSML) platforms. We strongly believe this means AWS provides the broadest and deepest range of services to customers.\nToday, I‚Äôm happy to share recent Magic Quadrant reports that named AWS as a Leader in more cloud technology markets: Cloud-Native Application Platforms (aka Cloud Application Platforms) and Container Management.\n2025 Gartner Magic Quadrant for Cloud-Native Application Platforms AWS has been named a Leader in the Gartner Magic Quadrant for Cloud-Native Application Platforms for 2 consecutive years. AWS was positioned highest on ‚ÄúAbility to Execute‚Äù. Gartner defines cloud-native application platforms as those that provide managed application runtime environments for applications and integrated capabilities to manage the lifecycle of an application or application component in the cloud environment. The following image is the graphical representation of the 2025 Magic Quadrant for Cloud-Native Application Platforms.\nOur comprehensive cloud-native application portfolio‚ÄîAWS Lambda, AWS App Runner, AWS Amplify, and AWS Elastic Beanstalk‚Äîoffers flexible options for building modern applications with strong AI capabilities, demonstrated through continued innovation and deep integration across our broader AWS service portfolio.\nYou can simplify the service selection through comprehensive documentation, reference architectures, and prescriptive guidance available in the AWS Solutions Library, along with AI-powered, contextual recommendations from Amazon Q based on your specific requirements. While AWS Lambda is optimized for AWS to provide the best possible serverless experience, it follows industry standards for serverless computing and supports common programming languages and frameworks. You can find all necessary capabilities within AWS, including advanced features for AI/ML, edge computing, and enterprise integration.\nYou can build, deploy, and scale generative AI agents and applications by integrating these compute offerings with Amazon Bedrock for serverless inferences and Amazon SageMaker for artificial intelligence and machine learning (AI/ML) training and management.\nAccess the complete 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms to learn more.\n2025 Gartner Magic Quadrant for Container Management In the 2025 Gartner Magic Quadrant for Container Management, AWS has been named as a Leader for three years and was positioned furthest for ‚ÄúCompleteness of Vision‚Äù. Gartner defines container management as offerings that support the deployment and operation of containerized workloads. This process involves orchestrating and overseeing the entire lifecycle of containers, covering deployment, scaling, and operations, to ensure their efficient and consistent performance across different environments.\nThe following image is the graphical representation of the 2025 Magic Quadrant for Container Management.\nAWS container services offer fully managed container orchestration with AWS native solutions and open-source technologies to focus on providing a wide range of deployment options, from Kubernetes to our native orchestrator.\nYou can use Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS). Both can be used with AWS Fargate for serverless container deployment. Additionally, EKS Auto Mode simplifies Kubernetes management by automatically provisioning infrastructure, selecting optimal compute instances, and dynamically scaling resources for containerized applications.\nYou can connect on-premises and edge infrastructure back to AWS container services with EKS Hybrid Nodes and ECS Anywhere, or use EKS Anywhere for a fully disconnected Kubernetes experience supported by AWS. With flexible compute and deployment options, you can reduce operational overhead and focus on innovation and drive business value faster.\nAccess the complete 2025 Gartner Magic Quadrant for Container Management to learn more.\n‚Äî Channy\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúData Science on AWS‚Äù Event Objectives Explore how data challenges can be solved through AWS Services Overview of Managed AI Services and their use cases Data preparation with Amazon SageMaker Using XGBoost with SageMaker Studio Notebooks No-code AutoML with SageMaker Canvas Speakers Van Hoang Kha ‚Äì Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong ‚Äì Cloud DevOps Engineer, AWS Community Builder Doan Nguyen Thanh Hoa ‚Äì CF Lecturer, FPT University HCMC Key Highlights Amazon Comprehend and Amazon Translate Deep learning-powered text analysis and translation\nProcesses various document types (emails, chats, social media, phone calls, etc.) and extracts insights automatically. Typical Comprehend use cases: Intelligent document processing Automated email workflows Customer support ticket routing Document and media tagging Sentiment analysis Contact center call analysis PII (Personally Identifiable Information) detection and redaction Amazon Translate Neural machine translation service\nKey Features:\nBroad language coverage: 4970 X‚ÜîY translation combinations Low latency: \u0026lt;150 ms/sentence on average Data security: Full encryption and ownership retention Regional coverage: Available in 17 AWS regions Customizable translation: Custom Terminologies and Active Custom Translation Batch translation: Supports DOCX, PPTX, XLSX, XML, HTML, and text files Broad domain coverage: Trained on 11 domains Pay-per-use: Simple API model Use Cases:\nLocalization: Enterprise content, media subtitling, archives Communication: Customer engagement, in-game chat, social media Text analytics: Voice of Customer, media analysis, eDiscovery Amazon Polly Text-to-speech (TTS) service\nAmazon Polly converts text into lifelike speech using deep learning.\nFeatures:\nText-to-speech (TTS) Speech Synthesis Markup Language (SSML) Custom Lexicons Speech Marks Brand Voice Common Use Cases:\nVoiced news articles and training Telephony/IVR systems Podcasts and language learning Navigation, reminders, accessibility tools Amazon Transcribe Automatic Speech Recognition (ASR) service\nConverts recorded media into text Supports real-time transcription for live streaming content Amazon Lex Conversational AI service\nBuilds voice and text-based chatbots.\nHighlights:\nEasy to use High-quality natural language understanding Built-in integration with AWS services Cost-effective Amazon Rekognition Provides image and video analysis for object detection, facial recognition, and scene understanding.\nAmazon Personalize Delivering personalized customer experiences\nFast deployment of recommendation systems Real-time response to user behavior Easy integration with existing systems Managed ML reduces time-to-market Feature Engineering Data Preparation with Amazon SageMaker Canvas Applying to Work ‚Äî Lessons Derived from Key Highlights 1. Data Understanding \u0026amp; Automation (Amazon Comprehend, Translate) Lesson learned:\nThe ability to extract insights from unstructured data is crucial for smarter decision-making.\nApplication:\nUse Amazon Comprehend for customer feedback sentiment analysis and document classification. Automate email or ticket routing based on detected intent and sentiment. Apply Amazon Translate for multilingual projects and global content localization. 2. Customer Engagement \u0026amp; Voice Interface (Amazon Polly, Lex, Transcribe) Lesson learned:\nVoice and conversational AI make digital experiences more accessible and user-friendly.\nApplication:\nCombine Lex, Polly, and Transcribe to build a voice-enabled chatbot for 24/7 support. Use Polly to generate natural-sounding voiceovers for e-learning materials. Leverage Transcribe to capture and analyze customer service calls and meeting notes. 3. Image \u0026amp; Video Intelligence (Amazon Rekognition) Lesson learned:\nComputer vision enables automation and deeper media insights.\nApplication:\nUse Rekognition to auto-tag and moderate multimedia content. Implement facial recognition for access control or customer analysis in retail environments. 4. Personalized Experience (Amazon Personalize) Lesson learned:\nPersonalization drives engagement and retention.\nApplication:\nIntegrate Personalize to recommend content, services, or products based on user behavior. Build real-time recommendation systems that evolve as user intent changes. 5. Machine Learning Simplification (Amazon SageMaker, Canvas) Lesson learned:\nMachine learning is no longer limited to experts‚ÄîAWS simplifies the end-to-end ML lifecycle.\nApplication:\nUse SageMaker Canvas for no-code model training and predictions. Apply SageMaker Studio for algorithm experimentation (e.g., XGBoost). Improve data quality through feature engineering to enhance model performance. 6. Data-driven Modernization Mindset Lesson learned:\nAI and AWS services promote a shift toward data-centric and event-driven architecture.\nApplication:\nCombine DDD (Domain-Driven Design) with AI workflows to design modular, adaptive systems. Use serverless computing (Lambda, API Gateway) for scalable AI-driven processes. Introduce event-driven patterns to handle asynchronous data flows efficiently. üåü Overall Reflection Through this AWS Data Science event, I learned not just about individual AI services but also about how to connect them into a cohesive, data-driven ecosystem.\nEach tool‚ÄîComprehend, Translate, Polly, Rekognition, Personalize, and SageMaker‚Äîrepresents a piece of a larger puzzle: building intelligent, automated, and human-centered applications.\nThese lessons are directly applicable to my projects, helping me move from manual workflows to AI-assisted automation and smarter user experiences.\nEvent Photos "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Tao Bao Thanh\nPhone Number: 0901452366\nEmail: taobaothanh365@gmail.com\nUniversity: Ho Chi Minh FPT University\nMajor: Software Engineering\nClass: OJT202\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Make friends with FCJ member - Pay attention about rule on job training - Practice: +Create AWS Free tier account 08/09/2025 09/09/2025 3 - Learn AWS and AWS services: + AWS EC2 + AWS Lambda + AWS SQS + AWS SNS + CLI,SDKs 09/09/2025 10/09/2025 4 - Learn AWS ECS, AWS EKS, VPC,and AWS CloudFormation - Cost management - Practice: + Create EC2 instance and Lambda function +Using AWS CLI 10/09/2025 11/09/2025 5 - Learn VPC basic and S3 - Practice: +Create Security group +Create Internet Gateway +Create Subnet +Create Route tables 11/09/2025 12/09/2025 6 - Practice: + Launch S3 bucket 12/09/2025 13/09/2025 Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nUsed AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "\rWeek 2 Objectives: Learn about Amazon RDS,AWS S3, AWS EC2 Auto Scaling, AWS DynamoDB and Amazon LightSail. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS DynamoDB, AWS RDS, Amazon Aurora -Practice:\n+Create AWS cloudFront and AWS S3 14/09/2025 15/09/2025 https://render.skillbuilder.aws/?module_id=B1DGAV8S16%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX/ 3 -Practice:\n+Create AWS RDS +Create DynamoDB using Python and AWS CLI 15/09/2025 16/09/2025 https://render.skillbuilder.aws/?module_id=B1DGAV8S16%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 4 - Translate Blog 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2 Auto Scaling , Simplified Computing with Amazon Lightsail - Practice: +Create Database in AWS RDS +Use Amazon LightSail 19/09/2025 20/09/2025 https://000005.awsstudygroup.com/ https://000045.awsstudygroup.com/ 6 - Practice: + Create RDS, VPC, EC2 with AWS EC2 Auto Scaling 20/09/2025 21/09/2025 https://000006.awsstudygroup.com/ Week 2 Achievements: Static Website Hosting with Amazon S3:\nKnow how to create bucket S3, upload file vaÃÄ accessble management. know how to open Static Website Hosting for bucket. Know how to configure public to website can be access through Internet. know how to configure Bucket policy to access to read public only. Database Essentials with Amazon Relational Database Service (RDS):\nSupport a lot for famous engine like MySQL,PostgreSQL,Oracle, vaÃÄ SQL Server. Know how to connect from RDS to EC2. Know endpoint, port,username/password to access DB. Know how to create VPC, subnet, security group to protect RDS instance. Scaling Applications with EC2 Auto Scaling:\nIs service that automatic increase or descrease the EC2 instance base on product on-demand Component of Auto Scaling: Launch Configuration/ Launch Template Auto Scaling Group(ASG) Extend policy: Dynamic Scaling vaÃÄ Schedule Scaling Health checks Based on Amazon CloudWatch to collect metrics like CPU utilization, network traffic, request const. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "\rWeek 3 Objectives: Understand about security Understand about services like AWS KMS, AWS Macie vaÃÄ AWS Certificate Manager. Understand how to protect the infrasturement Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 -Learn how to protect data by using AWS built-in data protection(Amazon S3, Amazon EBS, Amazon DynamoDB) and AWS data protection services(AWS KMS, Amazon Macie, AWS Certificate Manager) 22/09/2025 23/09/2025 https://render.skillbuilder.aws/?module_id=TNAPD78T9R%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 3 -Learn how to protect networks and applications by using AWS protection through infrastructure(Security groups, ELB, AWS regions) and AWS protection through services(AWS shield, AWS WAF) 23/09/2025 24/09/2025 https://render.skillbuilder.aws/?module_id=TNAPD78T9R%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 4 -Learn Detecting and Responding to Security Incidents by using Amazon Inspector, Amazon GuardDuty,Amazon Detective, Amazon Security Hub 24/09/2025 25/09/2025 https://render.skillbuilder.aws/?module_id=TNAPD78T9R%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 5 - Learn how to preventing Unauthorized Acces, using additional access management services like AWS IAM Identity Center, AWS Secrets Manager, AWS Systems Manager 25/09/2025 26/09/2025 https://render.skillbuilder.aws/?module_id=TNAPD78T9R%3A001.000.000\u0026product_id=8D79F3AVR7%3A002.000.000\u0026registration_id=c9de1df5-b12f-5abf-84c4-aac56a36dcae\u0026navigation=digital\u0026parentId=Y4YASRJEVX 6 - Practice: + Identity Federation with AWS Single Sign-On 26/09/2025 29/09/2025 https://000012.awsstudygroup.com/ Week 3 Achievements: Understand the basics of AWS data security, including:\nEncryption and key management with AWS KMS Secure sensitive data with Amazon Macie Manage certificates with AWS Certification Manager Data security mode of S3, EBS, DynamoDB Learn the concepts of layered and application protection:\nSecurity groups, Elastic Load Balancers (ELB), AWS Regions How AWS Shield and AWS WAF work Learn the role of services in the process of detecting and responding to security incidents:\nAmazon Inspector, GuardDuty, Amazon Detective, AWS Security Center Understand the basics of preventing unauthorized access:\nIAM Identity Center (SSO) AWS Secret Manager AWS System Administrator "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Practice create, management, v√† deploy all AWS resource through all different tools (Console, CLI, SDK, Elastic Beanstalk\u0026hellip;). Build a prototype web application by using Lambda, S3, DynamoDB, and API Gateway. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: Create table in AWS DynamoDB by using AWS CLI or Python through accessKey 28/09/2025 29/09/2025 3 -Practice: Create book store by using Lambda, S3 and DynamoDB 30/09/2025 01/10/2025 https://000078.awsstudygroup.com/ 4 - Learn JSON \u0026amp; Document model 01/10/2025 02/10/2025 5 - Pratice: + Use the AWS Console to deploy and verify AWS resources using an AWS CloudFormation template. + Use AWS Tools for Eclipse to deploy a Java Application to an Elastic Beanstalk environment. + Install and configure the AWS Elastic Beanstalk CLI tool. + Use the AWS Elastic Beanstalk CLI to deploy an update to an existing Elastic Beanstalk environment. + Use the AWS SDK to query and modify the AWS environment using code. 03/10/2025 04/10/2025 https://000050.awsstudygroup.com/ 6 - Practice: + Build a web application(Front-end) to interact with database through Lambda and API Gateway 04/10/2025 05/10/2025 https://000079.awsstudygroup.com/ Week 4 Achievements: Understood and worked with AWS DynamoDB:\nCreated and managed DynamoDB tables using AWS CLI and Python SDK (boto3) with Access Keys. Learned how to define Primary Key, Sort Key, and configure Read/Write Capacity Modes. Developed a Book Store application using AWS Lambda, S3, and DynamoDB:\nBuilt Lambda functions to handle CRUD operations for book data. Connected API Gateway with Lambda to create a REST API for frontend interaction. Stored book cover images and static files in an S3 Bucket. Explored JSON and Document Data Models:\nUnderstood how to store semi-structured data in DynamoDB and query it using JSON format.\nDeployed a Java application using AWS Elastic Beanstalk:\nUsed AWS Console and CloudFormation Templates to automate environment setup. Installed and configured Elastic Beanstalk CLI to deploy and update applications. Integrated AWS SDK for Java to query and modify AWS environments programmatically. Built a web application (frontend) to interact with the database via Lambda and API Gateway:\nDeveloped a simple web interface that allows users to add, edit, and delete data in DynamoDB. Understood the workflow Frontend ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB and how to secure endpoints. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Explore and practice authentication and storage using AWS Amplify. Understand and deploy serverless applications with AWS SAM (Serverless Application Model). Learn and implement user authentication using Amazon Cognito. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS Amplify 05/10/2025 06/10/2025 https://aws.amazon.com/vi/amplify/ 3 - Practice: Using Amplify authentication and storage for serverless applications 06/10/2025 09/10/2025 https://000134.awsstudygroup.com/ 4 - Learn AWS SAM : + AWS SAM templates and AWS SAM CLI + AWS SAM Accelerate and AWS SAM CLI Integration 09/10/2025 10/10/2025 https://aws.amazon.com/vi/serverless/sam/ 5 - Practice: + Install SAM CLI + Deploy front-end and Lambda function + Config API Gateway and Test API by Postman 10/10/2025 11/10/2025 https://000080.awsstudygroup.com/ 6 -Learn: Amazon Cognito -Practice: Authentication with Amazon Cognito 11/10/2025 13/10/2025 https://000081.awsstudygroup.com/ Week 5 Achievements: Learned and practiced using AWS Amplify, including:\nUnderstanding Amplify architecture and its integration with serverless applications Implementing Amplify Authentication for user login and registration. Using Amplify Storage to manage file uploads and access via S3. Managing Amplify projects through the CLI and connecting to the front-end. Studied and applied AWS SAM (Serverless Application Model):\nLearned about SAM templates, Lambda functions, and API Gateway integration. Installed and configured the SAM CLI. Practiced deploying a serverless application using SAM Accelerate. Deployed Lambda functions and tested APIs using Postman. Understood the workflow between SAM, CloudFormation, and serverless infrastructure. Learned and practiced Amazon Cognito:\nUnderstood Cognito User Pools and Identity Pools. Implemented user authentication and authorization using Cognito. Integrated Cognito authentication into a front-end application with AWS Amplify. Successfully tested sign-up, sign-in, and token verification workflows. Built the ability to combine Amplify, SAM, and Cognito to form a fully functional serverless application that supports authentication, API interaction, and deployment on AWS.\nStrengthened understanding of serverless architecture, preparing for more advanced cloud-native development tasks.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Understand and practice AWS Backup, including creating Backup Vaults, Backup Plans, and deploying backup configurations using AWS CloudFormation. Learn how AWS WAF (Web Application Firewall) and AWS PrivateLink work, including key components such as ACLs, Rules, Rule Groups, VPC Endpoint Services, and Network Load Balancers. Study AWS KMS (Key Management Service) ‚Äî covering symmetric and asymmetric key management and their role in data encryption. Gain a solid understanding of Containerization with Docker, including how to build and deploy applications using Docker Images and Docker Compose. Strengthen skills in Infrastructure as Code (IaC) and container-based deployment for secure and scalable cloud environments. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS Backup: + Backup vault + Backup plan + Using cloudFormation to create backup plan 13/10/2025 15/10/2025 https://000013.awsstudygroup.com/ 3 - Practice: + Create backup plan +Create on-demand backup + Create backup vaults 13/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn AWS WAF and AWS privateLink : + ACL + Rules + Rules groups + VPC Endpoint Service + VPC Endpoint + Network Load Balancer 15/10/2025 17/10/2025 https://000026.awsstudygroup.com/ https://000111.awsstudygroup.com/ 5 - Learn AWS KMS : + Symetric Key + Asymetric Key -Practice: + Create ACLs + Create rules and rules group 17/10/2025 19/10/2025 https://000033.awsstudygroup.com/ 6 - Learn Containerization with Docker: + What is Docker + Deploy use only Docker Image Deploy with Docker compose and push image 19/10/2025 21/10/2025 https://000015.awsstudygroup.com/ Week 6 Achievements: Successfully learned and practiced AWS Backup, created Backup Vaults, Backup Plans, and On-demand Backups through both AWS Console and CloudFormation templates. Configured and tested AWS WAF, created ACLs, Rules, and Rule Groups to protect web applications from common attacks. Gained hands-on experience with AWS PrivateLink, implemented VPC Endpoint Services and VPC Endpoints connected via Network Load Balancer for secure private communication. Practiced using AWS KMS, created and managed both Symmetric and Asymmetric Keys for data encryption and secure key management. Built and deployed containerized applications using Docker, practiced building Docker Images, running containers, deploying with Docker Compose, and pushing images to Docker Hub. Strengthened understanding of Cloud Security, Encryption, and Containerization, forming a strong foundation for deploying real-world applications on AWS Cloud. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn the AWS services that use to optimized performance The neccessary services for optimizing performance like ECS, EKS,CodePipeline, Storage gateway Learn about Docker, Kubernetes, and relationship between Docker and kubernetes Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn the AWS EKS : + Control Plane(AWS managed) + Workder Nodes(user-managed) + All component in EKS :Cluster, Node group, Pod, Deployment Service 20/10/2025 21/10/2025 https://000126.awsstudygroup.com/1-introduce/ 3 - Practice: : + Create network:VPC, subnets, internet gateway + Create Auth for control plane + Create EKS cluster + Setting the addon: VPC-cni, kube proxy + Create Auth for worker node + Create worker node + Install addon: coredns + Test nginx deployment 21/10/2025 22/10/2025 https://000126.awsstudygroup.com/1-introduce 4 - Learn AWS ECS: + Cluster, task denifition, task, service, container agent + ECS launch types + Networking in ECS + Integrate with another services + ECS Auto Scaling 22/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn AWS Storage Gateway: - Practice:\n+ Create ECS cluster + Config the Docker image + Create Task definition + Register namespace in Cloud Map 23/10/2025 25/10/2025 https://000016.awsstudygroup.com/1-introduction/ 6 - Learn AWS CodePipeline: + Source stage: Take code from GitHub,Codecommit or S3 + Build stage: Call CodeBuild to translate + Deploy stage: Call CodeDeploy to deploy 25/10/2025 27/10/2025 https://000017.awsstudygroup.com/ Week 7 Achievements: Learn AWS EKS:\nUnderstadn the architecture EKS include Control Plane (AWS managed) and Worker Nodes (self-managed). Definition about Cluster, Node Group, Pod, Deployment, Service. Understand how to connect kubectl to EKS Cluster and manage workloads. Practice on EKS:\nCreate VPC, Subnets, Internet Gateway, Route Table. Give Authentication EKS Control Plane (IAM Role). Create EKS Cluster and install addon: vpc-cni, kube-proxy. Create Node Group for Worker Node and install addon coredns. Deploy Nginx Deployment successfully on EKS and access through LoadBalancer. Learn AWS ECS:\nUnderstand how ECS Cluster work, Task Definition, Service, Container Agent. Discriminate ECS Launch Type (EC2 / Fargate). Understand to config the internet (ECS networking modes: bridge, host, awsvpc). Understand how to intergrate ECS with CloudWatch, Load Balancer and Auto Scaling. Practice to deploy ECS:\nCreate ECS Cluster (Fargate). Build and push Docker image into ECR. Create Task Definition and Service run container. Register namespace on Cloud Map that ECS can service discovery. Learn the AWS Storage Gateway basic and hybrid storage model. Learn AWS CodePipeline:\nUnderstand pipeline CI/CD include 3 stage: Source ‚Üí Build ‚Üí Deploy. Understand how to intergrate CodePipeline with GitHub, CodeBuild and CodeDeploy. Understand the automation build and deploy Application Spring Boot or React into EC2/S3. Write file buildspec.yml and appspec.yml template. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Understand the fundamentals and use cases of AWS Step Functions, including its 7 core state types and how to orchestrate complex workflows.\nGain hands-on experience creating and testing workflows in AWS Cloud9, focusing on task orchestration and error handling.\nLearn the key features and deployment steps of Amazon FSx, including its different variants (Windows File Server, Lustre, NetApp ONTAP, OpenZFS).\nPractice configuring an FSx file system integrated with AWS Managed Microsoft AD, ensuring proper setup of networking, authentication, and file sharing.\nExplore AWS X-Ray to understand how to trace and visualize requests across distributed applications for performance optimization and debugging.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS Step Functions: + 7 states: Task state, choice state, a fail or success state, pass state, wait state, parallel state, map state + Uses cases to use AWS Step Functions + Benefits of AWS Step Functions 26/10/2025 27/10/2025 https://000047.awsstudygroup.com/1-intro/ 3 - Practice:\n+ Create Cloud9 enviroment + Create sample services + Initialize workflow + Error handling 27/10/2025 28/10/2025 https://000047.awsstudygroup.com/1-intro/ 4 - Learn Amazon FSx: + FSx for Windows File Server + FSx for Lustre + FSx for NetApp ONTAP + FSx for OpenZFS 28/10/2025 29/10/2025 https://000025.awsstudygroup.com/ 5 - Practice: + Configure File system details + Choose existing VPC + Choose AWS Managed Microsoft AD(Provide DNS name of domain, Service account username and password) + Define Windows file share name(Choose AWS Managed Microsoft AD) Review and create 29/10/2025 30/10/2025 https://000025.awsstudygroup.com/ 6 - Learn AWS X-Ray: + Trace + Segment + Subsegment + Annotation / Metadata + Service Map 30/10/2025 31/10/2025 https://000140.awsstudygroup.com/ Week 8 Achievements: Successfully learned and explained the 7 states of AWS Step Functions, including how each contributes to workflow automation and state transitions.\nPracticed building a sample workflow using AWS Step Functions in Cloud9, implementing error handling and validating execution flow.\nGained a solid understanding of the different Amazon FSx options and their ideal use cases for Windows workloads, high-performance computing, and enterprise storage.\nCompleted hands-on setup of an FSx for Windows File Server, including file system configuration, VPC and AD integration, and file share creation.\nLearned how AWS X-Ray collects traces, segments, and subsegments, and visualized a Service Map to identify performance bottlenecks and errors across AWS services.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Understand and apply AWS AppSync for building GraphQL APIs with various data sources. Learn and configure AWS EBS Data Lifecycle Manager to automate snapshots and backups. Explore AWS GuardDuty for intelligent threat detection using ML and behavior analytics. Study AWS Macie to identify and protect sensitive data in S3 buckets using machine learning. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS AppSync: + GraphQL APIs: Query, Mutation, Subscription + Data Sources: DynamoDB, RDS/Aurora, AWS Lambda, HTTP Endpoints, OpenSearch + Authentication and Authorization: AWS IAM, API Keys, Cognito User Pools, OpenID Connect + Realtime Subscriptions, Caching, Offline support - Practice: + Create GraphQL API + Define schema and attach to a data source + Configure request/response mapping templates 02/11/2025 03/11/2025 https://000086.awsstudygroup.com/1-introduction/ 3 - Learn AWS EBS Data Lifecycle Manager: + Automate backup and recovery + Reduce storage costs + Ensure compliance and data protection + Understand Lifecycle Policies: EBS Snapshot Policy, EBS-backed AMI Policy, Cross-region/ Cross-account Copy Policy 03/11/2025 05/11/2025 https://000088.awsstudygroup.com/ 4 - Practice: + Launch EC2 instance and configure lifecycle policies + Define which resources to back up, schedule frequency, and retention duration - Learn AWS GuardDuty: + Understand ML-based threat detection and behavioral analysis 03/11/2025 06/11/2025 https://000098.awsstudygroup.com/ 5 - Learn AWS Macie: + Key Features: Data Discovery, Classification, Bucket-level Visibility + How Macie works internally + Use Cases and Pricing Model 08/14/2025 08/15/2025 https://000090.awsstudygroup.com/ 6 - Practice on AWS Macie: + Create and configure an S3 bucket + Enable Macie service + Create Macie jobs to scan and classify data 08/15/2025 08/15/2025 https://000090.awsstudygroup.com/ Week 9 Achievements: Objectives and Achievements AWS AppSync: Built a simple GraphQL API integrated with DynamoDB and Lambda, understanding schema design, resolvers, and mapping templates. AWS EBS Lifecycle Manager: Configured automated backup policies to improve reliability and cost-efficiency in EC2 environments. AWS GuardDuty: Gained knowledge of continuous threat detection using ML and anomaly-based analysis. AWS Macie: Enabled Macie for S3, ran data classification jobs, and reviewed findings on sensitive data exposure and compliance monitoring. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "\rWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn Amazon RDS,AWS DynamoDB, LightSail and EC2 Auto Scaling Week 3: Learn security\nWeek 4: Learn AWS DynamoDB and Relational Database Services\nWeek 5: authentication and storage\nWeek 6: Data encrytion\nWeek 7: optimized performance\nWeek 8: setup of networking, authentication, and file sharing\nWeek 9: Practice with security and operations\nWeek 10: Learn monitoring performance\nWeek 11: Learn Redis and more AWS Security Week 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "URBAN RAIL TRANSIT SERVICE SYSTEM ON AWS Executive Summary Rapid urban development has increased the need for intelligent and sustainable public transportation. A modern Urban Rail Transit System requires a reliable, scalable, and highly available digital platform operating 24/7.\nThis proposal presents an AWS microservices architecture using Amazon ECS Fargate, enabling:\nTicket booking, scheduling, payments, and traffic monitoring Real-time passenger data ingestion through Amazon Kinesis BI dashboards and predictive analytics with QuickSight and SageMaker Full CI/CD automation and comprehensive monitoring Key Highlights Microservices container architecture powered by Amazon ECS Fargate End-to-end security: Route53 ‚Üí CloudFront ‚Üí WAF ‚Üí ALB ‚Üí Private Subnets Fully automated CI/CD using CodePipeline, CodeBuild, CodeDeploy, ECR Real-time processing using Kinesis Data Streams Analytics \u0026amp; forecasting with QuickSight + SageMaker Scalability for hundreds of thousands of daily ticket requests 1. Project Objectives Primary Objective Build a digital platform for the Urban Rail Transit System with scalability, security, and long-term operational stability.\nSpecific Objectives Implement ticketing, scheduling, payment, and notification services as microservices Enable automated operations and system-wide monitoring Establish a full CI/CD pipeline with zero downtime Support real-time passenger data ingestion and analytics Provide multi-AZ architecture with ‚â• 99.95% system availability 2. Project Scope Component Description Region AWS Singapore (ap-southeast-1) Users Passengers, operators, administrators Architecture Style Microservices on ECS Fargate Phase 1 Ticketing, scheduling, notifications Phase 2 Analytics, BI dashboards, AI forecasting 3. Proposed AWS Architecture 3.1 Architecture Overview A multi-tier microservices architecture will be deployed, including:\nEdge Layer: Route53, CloudFront, AWS WAF Application Layer: ALB ‚Üí ECS Fargate ‚Üí ECR Data Layer: RDS SQL Server, ElastiCache Redis Event Layer: EventBridge, SNS, SQS Analytics Layer: Kinesis ‚Üí S3 ‚Üí QuickSight ‚Üí SageMaker Monitoring Layer: CloudWatch, CloudTrail CI/CD Layer: CodePipeline, CodeBuild, CodeDeploy 3.2 Networking \u0026amp; Access Layer Route 53: Global DNS routing CloudFront: CDN caching and low-latency access AWS WAF: Protection from DDoS, SQL injection, XSS Application Load Balancer: Routes traffic to microservices Traffic Flow:\nUser ‚Üí Route53 ‚Üí CloudFront ‚Üí WAF ‚Üí ALB ‚Üí Private Subnet ‚Üí ECS Fargate\n3.3 Application Layer ‚Äî Microservices on ECS Fargate Why Fargate? No server management Autoscaling based on CPU/Memory High security in private subnets Microservices Booking Service Schedule Service Payment Service Notification Service User Service Staff \u0026amp; Operation Service Docker Images stored in Amazon ECR\n3.4 Data Layer Amazon RDS (SQL Server) Stores tickets, schedules, user accounts, payments Multi-AZ high availability Automated backup and failover ElastiCache Redis Cache schedules ‚Üí reduce RDS load Increase API performance up to 10√ó Amazon S3 Stores reports, invoices, files Destination for streaming data from Kinesis 3.5 Event \u0026amp; Messaging Layer Amazon EventBridge Automates workflows, including:\nPaymentSuccess ‚Üí CreateInvoice ‚Üí NotifyUser ScheduleUpdate ‚Üí BroadcastToMobile Amazon SQS Queue system for notifications and ticket processing Protects services during traffic spikes Amazon SNS Sends SMS, email, and push notifications 3.6 Real-Time Analytics Kinesis Data Streams Ingests passenger traffic in real time Application logs ‚Üí Kinesis ‚Üí S3 QuickSight Dashboards for daily sales, station load, peak hours SageMaker Predict passenger demand Optimize train frequencies during peak hours 3.7 Monitoring \u0026amp; Observability CloudWatch Metrics: CPU, Memory, ALB latency CloudWatch Logs: Fargate application logs SNS Alerts: Error notifications CloudTrail: Governance \u0026amp; admin activity tracking 3.8 CI/CD Pipeline Developer Commit\n‚Üí CodePipeline\n‚Üí CodeBuild\n‚Üí Build Docker Image\n‚Üí ECR\n‚Üí CodeDeploy\n‚Üí ECS Fargate\nFeatures: Rolling deployments with zero downtime ALB health checks Automatic rollback on failure 4. Deployment Plan Phase Duration Deliverables 1 1 week Route53, CloudFront, WAF, VPC, ALB 2 3 weeks ECS, ECR, RDS, ElastiCache 3 1 week EventBridge, SQS, SNS 4 3 weeks Kinesis, S3, QuickSight 5 2 weeks CI/CD Pipeline 6 2 weeks Security hardening, cost tuning 5. Estimated Monthly Operational Cost Service Cost/Month CloudFront + Route53 + WAF $24 ECS Fargate $40 RDS $19 ElastiCache Redis $73 S3 + Kinesis $1 Monitoring $37 CI/CD $18 Total Estimated Cost $212 6. Expected Outcomes Stable 24/7 rail transit operations 1,000+ concurrent users supported Secure and automated payment processing Accurate passenger demand forecasting Reduced operation costs through autoscaling and CI/CD Appendix A ‚Äî AWS Services Summary Category AWS Services Edge Route53, CloudFront, WAF Networking VPC, ALB Compute ECS Fargate, ECR Database RDS SQL Server, ElastiCache Event EventBridge, SNS, SQS Analytics Kinesis, S3, QuickSight, SageMaker Monitoring CloudWatch, CloudTrail CI/CD CodePipeline, CodeBuild, CodeDeploy "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Practice with AWS core services (VPC, EC2, EBS, Postgres) Learn AWS SageMaker for AI/ML workflow Practice AI/ML data analysis with SageMaker Learn AWS Bedrock AgentCore components Learn AWS Glue and Amazon Athena for ETL and analytics Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: + Creating VPCs + Initializing EC2 instances + Creating an EBS volume + Assigning EBS volume Installing Postgres Mount EBS EC2 production + Postgres backup Mount EBS EC2 test + Data recovery 08/11/2025 10/11/2025 https://100000.awsstudygroup.com/ 3 - Learn AWS SageMaker AI:\n+ What is SageMaker + The component + End-to-End Workflow 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 4 - Practice: Use Amazon SageMaker AI to analyze data, import Excel files, choose data types and export results including charts 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 5 - Learn AWS Bedrock AgentCore:\n+ Identity + Memory + Code Interpreter + Browser + Gateway + Observability + Common use cases: Equip agents with built-in tools and capabilities, Deploy securely at scale, Test and monitor agents 12/11/2025 14/11/2025 https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html 6 - Learn AWS Glue and Amazon Athena 14/11/2025 16/11/2025 https://000040.awsstudygroup.com/ Week 10 Achievements: AWS Core Services Practice:\nCreated VPCs with proper subnets, route tables, and Internet Gateways. Launched EC2 instances for production and test environments. Created and attached EBS volumes to EC2 instances. Installed and configured PostgreSQL on EC2 instances. Mounted EBS volumes on both production and test instances. Performed PostgreSQL backup and data recovery successfully. AWS SageMaker AI Learning \u0026amp; Practice:\nUnderstood SageMaker components: Notebook, Training, Endpoint, Model, Pipelines. Completed end-to-end AI/ML workflow: imported Excel datasets, analyzed data, visualized charts. Learned to choose proper data types, clean data, and export results for further use. AWS Bedrock AgentCore Learning:\nLearned core components: Identity, Memory, Code Interpreter, Browser, Gateway, Observability. Explored common use cases: Equipping agents with built-in tools and capabilities. Deploying agents securely at scale. Monitoring and testing agents in real scenarios. AWS Glue \u0026amp; Amazon Athena Learning:\nPracticed building ETL pipelines using AWS Glue. Learned to catalog datasets and transform data for analytics. Queried S3 data using Amazon Athena and verified results. Connected Athena queries to QuickSight dashboards for reporting and visualization. AWS CLI \u0026amp; Management Console Proficiency:\nConfigured AWS CLI with Access Key, Secret Key, and Default Region. Used CLI to retrieve regions, list services, and check configurations. Managed AWS resources via CLI and Web Console interchangeably. General Achievements:\nGained hands-on experience across multiple AWS services. Developed confidence in connecting different services into functional workflows. Able to combine compute, storage, networking, database, AI/ML, ETL, and analytics services in practical exercises. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Understand and practice AWS services: ElastiCache (Redis), Certificate Manager, Inspector, Detective, Systems Manager. Learn the structure, operation, and benefits of each service. Practice connecting, granting permissions, deploying, and performing security checks on AWS services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon ElastiCache - Redis : + What is Redis + The components: Clusters, ElastiCache nodes, Redis shard + How it works + Benefits of using ElastiCache 17/11/2025 18/11/2025 https://000061.awsstudygroup.com/1-introduce/ 3 - Learn AWS Certificate Manager - Practice:\n+ Create cluster + Grant access to the cluster + Connect to cluster node 18/11/2025 19/11/2025 https://000061.awsstudygroup.com/ https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html 4 - Learn AWS Inspector:\n+ Use to find the security gap + Security misconfigurations + Remediation steps 19/11/2025 20/11/2025 https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html 5 - Learn AWS Detective 21/11/2025 22/11/2025 https://docs.aws.amazon.com/detective/latest/userguide/what-is-detective.html 6 - Practice with AWS Systems Manager 22/11/2025 24/11/2025 https://000031.awsstudygroup.com/1-introduce/ Week 11 Achievements: Completed theory and hands-on practice with ElastiCache (Redis): learned about Redis, clusters, nodes, shards, how it works, and its benefits. Practiced creating Redis clusters, granting access, and connecting to nodes. Learned and practiced AWS Certificate Manager: creating and deploying certificates on clusters. Learned and practiced AWS Inspector: identifying security gaps, misconfigurations, and remediation steps. Learned AWS Detective: detecting and investigating security events. Practiced AWS Systems Manager: centralized management and operation of AWS resources. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 -AWS services scale to new heights for Prime Day 2025: key metrics and milestones Prime Day 2025 set new records powered by AWS. More than 7,000 ASRS robots at Amazon fulfillment centers executed over 524 million commands, peaking at 8 million per hour‚Äîa 160% increase over 2024. With AWS Outposts ensuring low-latency operations and Amazon‚Äôs retail expertise, customers quickly found deals, accessed product information, and enjoyed fast same-day or next-day delivery.\nBlog 2 - AWS named as a Leader in 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms and Container Management In the era of booming cloud computing and artificial intelligence, businesses are constantly seeking technology platforms that enable faster innovation, greater agility, and better cost optimization. With its comprehensive ecosystem of services, AWS not only supports millions of customers worldwide on their digital transformation journey but also continues to affirm its pioneering position by being recognized by Gartner as a Leader in several key categories of the 2025 Magic Quadrant.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]