[
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/",
	"title": "Báo cáo thực tập",
	"tags": [],
	"description": "",
	"content": "Báo cáo thực tập Thông tin sinh viên: Họ và tên: Tào Bảo Thành\nSố điện thoại: 0901452366\nEmail: taobaothanh365@gmail.com\nTrường: Đại học FPT Hồ Chí Minh\nNgành: Kỹ Thuật Phần Mềm\nLớp: OJT202\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 12/08/2025 đến ngày 12/11/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Các dịch vụ AWS vươn lên tầm cao mới trong Prime Day 2025: các số liệu và cột mốc chính Amazon Prime Day 2025 là sự kiện mua sắm Prime Day lớn nhất từ trước đến nay, thiết lập kỷ lục cả về khối lượng bán hàng và tổng số sản phẩm được bán ra trong suốt 4 ngày diễn ra sự kiện. Các thành viên Prime đã tiết kiệm hàng tỷ đô la khi mua sắm hàng triệu ưu đãi trên Amazon trong sự kiện này.\nNăm nay đánh dấu một sự chuyển đổi đáng kể trong trải nghiệm Prime Day nhờ những tiến bộ trong các dịch vụ AI tạo sinh của Amazon và AWS. Khách hàng đã sử dụng Alexa+ — trợ lý cá nhân thế hệ tiếp theo của Amazon hiện có sẵn trong giai đoạn truy cập sớm cho hàng triệu khách hàng — cùng với trợ lý mua sắm được hỗ trợ AI, Rufus, và Hướng dẫn mua sắm AI. Những tính năng này, được xây dựng dựa trên hơn 15 năm đổi mới đám mây và chuyên môn về học máy từ AWS, kết hợp với kinh nghiệm bán lẻ và người tiêu dùng sâu rộng từ Amazon, đã giúp khách hàng nhanh chóng khám phá các ưu đãi và có được thông tin sản phẩm, bổ sung cho dịch vụ giao hàng nhanh, miễn phí mà thành viên Prime tận hưởng quanh năm.\nNhư một phần trong truyền thống hằng năm của chúng tôi nhằm chia sẻ về cách AWS đã hỗ trợ Prime Day với doanh số kỷ lục, tôi muốn giới thiệu các dịch vụ và những số liệu ấn tượng từ AWS đã tạo nên trải nghiệm mua sắm tuyệt vời của bạn.\nPrime Day 2025 – tất cả các con số Trong những tuần trước các sự kiện mua sắm lớn như Prime Day, các trung tâm hoàn thiện đơn hàng và trạm giao hàng của Amazon hoạt động để chuẩn bị sẵn sàng và đảm bảo vận hành hiệu quả, an toàn. Ví dụ, hệ thống lưu trữ và truy xuất tự động (ASRS) của Amazon vận hành một đội ngũ robot di động công nghiệp toàn cầu để di chuyển hàng hóa trong các trung tâm hoàn thiện đơn hàng.\nAWS Outposts, một dịch vụ được quản lý toàn phần mở rộng trải nghiệm AWS đến môi trường tại chỗ, cung cấp sức mạnh cho các ứng dụng phần mềm quản lý điều khiển và vận hành ASRS, và hỗ trợ giao hàng trong ngày và ngày kế tiếp thông qua xử lý độ trễ thấp cho các lệnh điều khiển robot quan trọng.\nTrong Prime Day 2025, AWS Outposts tại một trong những trung tâm hoàn thiện đơn hàng lớn nhất đã gửi hơn 524 triệu lệnh đến hơn 7.000 robot, đạt đỉnh 8 triệu lệnh mỗi giờ — tăng 160% so với Prime Day 2024.\nDưới đây là một số số liệu thú vị và ấn tượng khác:\nAmazon Elastic Compute Cloud (Amazon EC2) – Trong Prime Day 2025, AWS Graviton, một bộ vi xử lý được thiết kế để mang lại hiệu suất/chi phí tốt nhất cho các khối lượng công việc trên Amazon EC2, đã cung cấp hơn 40% năng lực tính toán Amazon EC2 được Amazon.com sử dụng. Amazon cũng đã triển khai hơn 87.000 chip AWS Inferentia và AWS Trainium — chip silicon tùy chỉnh dành cho huấn luyện và suy luận AI/học sâu — để hỗ trợ Amazon Rufus trong Prime Day.\nAmazon SageMaker AI — Amazon SageMaker AI, một dịch vụ được quản lý toàn phần tập hợp nhiều công cụ cho học máy hiệu suất cao và chi phí thấp, đã xử lý hơn 626 tỷ yêu cầu suy luận trong Prime Day 2025.\nAmazon Elastic Container Service (Amazon ECS) và AWS Fargate – Amazon ECS, dịch vụ điều phối container được quản lý toàn phần, kết hợp với AWS Fargate, một công cụ tính toán serverless cho container, đã khởi chạy trung bình 18,4 triệu tác vụ mỗi ngày trên AWS Fargate trong Prime Day 2025, tăng 77% so với mức trung bình của Prime Day năm trước.\nAWS Fault Injection Service (AWS FIS) – Chúng tôi đã chạy hơn 6.800 thí nghiệm AWS FIS — nhiều gấp tám lần so với năm 2024 — để kiểm tra khả năng chịu lỗi và đảm bảo Amazon.com luôn khả dụng trong Prime Day. Sự gia tăng đáng kể này có được nhờ hai cải tiến: hỗ trợ mới cho các thí nghiệm lỗi mạng trên Amazon ECS với AWS Fargate, và việc tích hợp thử nghiệm FIS vào pipeline CI/CD.\nAWS Lambda – AWS Lambda đã xử lý hơn 1,7 nghìn tỷ lượt gọi mỗi ngày trong Prime Day 2025.\nAmazon API Gateway – Trong Prime Day 2025, Amazon API Gateway đã xử lý hơn 1 nghìn tỷ yêu cầu dịch vụ nội bộ — tăng trung bình 30% mỗi ngày so với Prime Day 2024.\nAmazon CloudFront – Amazon CloudFront đã phân phát hơn 3 nghìn tỷ yêu cầu HTTP trong tuần Prime Day toàn cầu 2025, tăng 43% so với Prime Day 2024.\nAmazon Elastic Block Store (Amazon EBS) – Trong Prime Day 2025, Amazon EBS đã đạt đỉnh 20,3 nghìn tỷ thao tác I/O, di chuyển đến 1 exabyte dữ liệu mỗi ngày.\nAmazon Aurora – Trong Prime Day, Amazon Aurora đã xử lý 500 tỷ giao dịch, lưu trữ 4.071 terabyte dữ liệu, và truyền 999 terabyte dữ liệu.\nAmazon DynamoDB – Amazon DynamoDB đã xử lý hàng chục nghìn tỷ cuộc gọi API trong Prime Day, duy trì tính khả dụng cao với thời gian phản hồi đơn vị mili-giây, và đạt đỉnh 151 triệu yêu cầu mỗi giây.\nAmazon ElastiCache – Trong Prime Day, Amazon ElastiCache đã đạt đỉnh hơn 1,5 triệu tỷ yêu cầu mỗi ngày và hơn 1,4 nghìn tỷ yêu cầu trong một phút.\nAmazon Kinesis Data Streams – Đã xử lý đỉnh 807 triệu bản ghi mỗi giây trong Prime Day 2025.\nAmazon Simple Queue Service (Amazon SQS) – Trong Prime Day 2025, Amazon SQS đã thiết lập kỷ lục lưu lượng mới với 166 triệu tin nhắn mỗi giây.\nAmazon GuardDuty – Trong Prime Day 2025, Amazon GuardDuty đã giám sát trung bình 8,9 nghìn tỷ sự kiện log mỗi giờ, tăng 48,9% so với Prime Day năm ngoái.\nAWS CloudTrail – AWS CloudTrail đã xử lý hơn 2,5 nghìn tỷ sự kiện trong Prime Day 2025, so với 976 tỷ sự kiện trong năm 2024.\nSẵn sàng để mở rộng quy mô\nNếu bạn đang chuẩn bị cho các sự kiện quan trọng đối với doanh nghiệp như ra mắt sản phẩm, di chuyển hệ thống, và các cuộc di cư khác, tôi khuyên bạn nên tận dụng AWS Countdown (trước đây gọi là AWS Infrastructure Event Management, hay IEM). Đây là một chương trình hỗ trợ toàn diện giúp đánh giá mức độ sẵn sàng vận hành, xác định và giảm thiểu rủi ro, và lập kế hoạch năng lực, sử dụng các playbook đã được chứng minh do các chuyên gia AWS phát triển.Chúng tôi đã mở rộng để bao gồm: generative AI implementation support nhằm giúp bạn tự tin ra mắt và mở rộng các sáng kiến AI; migration and modernization support, bao gồm mainframe modernization; và tối ưu hóa hạ tầng cho các lĩnh vực chuyên biệt bao gồm election systems, retail operations, healthcare services, và sports and gaming events.\nTôi mong được chứng kiến những kỷ lục nào khác sẽ bị phá vỡ vào năm tới!\n— Channy\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\rAWS được công nhận là Nhà lãnh đạo trong báo cáo Gartner Magic Quadrant 2025 về Nền tảng Ứng dụng Cloud-Native và Quản lý Container Một tháng trước, mình đã chia sẻ rằng Amazon Web Services (AWS) được công nhận làLeader in 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services (SCPS),đánh dấu năm thứ mười lăm liên tiếp AWS được Gartner xếp hạng là Nhà lãnh đạo.\nNăm 2024, AWS được công nhận là Nhà lãnh đạo trong nhiều báo cáo Gartner Magic Quadrant, bao gồm: AI Code Assistants, Cloud-Native Application Platforms, Cloud Database Management Systems, Container Management, Data Integration Tools, Desktop as a Service (DaaS), and Data Science and Machine Learning Platforms as cũng như SCPS.Sang năm 2025, AWS tiếp tục được công nhận là Nhà lãnh đạo trong Gartner Magic Quadrant cho Contact Center as a Service (CCaaS), Desktop as a Service và Data Science and Machine Learning (DSML) nền tảng. Chúng tôi tin tưởng rằng điều này cho thấy AWS đang cung cấp danh mục dịch vụ rộng và sâu nhất cho khách hàng.\nHôm nay, mình vui mừng chia sẻ các báo cáo Magic Quadrant mới nhất, trong đó AWS được vinh danh là Nhà lãnh đạo trong nhiều thị trường công nghệ đám mây: Nền tảng Ứng dụng Cloud-Native (còn gọi là Nền tảng Ứng dụng Đám mây) và Quản lý Container.\n2025 Gartner Magic Quadrant for Cloud-Native Application Platforms AWS đã được công nhận là Nhà lãnh đạo trong Gartner Magic Quadrant cho Nền tảng Ứng dụng Cloud-Native trong 2 năm liên tiếp. AWS được xếp hạng cao nhất về “Khả năng Thực thi”.\nGartner định nghĩa nền tảng ứng dụng cloud-native là những dịch vụ cung cấp môi trường runtime được quản lý cho ứng dụng, cùng các khả năng tích hợp để quản lý vòng đời của ứng dụng hoặc thành phần ứng dụng trong môi trường đám mây.\nDanh mục ứng dụng cloud-native toàn diện của chúng tôi—AWS Lambda, AWS App Runner, AWS Amplify, and AWS Elastic Beanstalk—mang đến sự linh hoạt để xây dựng các ứng dụng hiện đại với khả năng AI mạnh mẽ, nhờ đổi mới liên tục và tích hợp sâu rộng trong toàn bộ danh mục dịch vụ AWS.\nKhách hàng có thể dễ dàng lựa chọn dịch vụ thông qua tài liệu chi tiết, kiến trúc tham chiếu, và hướng dẫn có sẵn trong AWS Solutions Library, cùng với khuyến nghị ngữ cảnh được hỗ trợ bởi AI từ Amazon Q dựa trên yêu cầu cụ thể. Mặc dù AWS Lambda được tối ưu hóa cho AWS để mang lại trải nghiệm serverless (không máy chủ) tốt nhất, nhưng nó vẫn tuân theo các tiêu chuẩn của ngành về điện toán serverless và hỗ trợ các ngôn ngữ lập trình cũng như framework phổ biến. Bạn có thể tìm thấy tất cả các khả năng cần thiết trong AWS, bao gồm các tính năng nâng cao cho AI/ML, điện toán biên (edge computing) và tích hợp doanh nghiệp.\nBạn có thể xây dựng, triển khai, và mở rộng các ứng dụng generative AI bằng cách tích hợp các dịch vụ compute này với Amazon Bedrock để thực hiện suy luận serverless, vàAmazon SageMaker cho cho việc huấn luyện và quản lý artificial intelligence and machine learning (AI/ML).\nTruy cập 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms để xem thêm thông tin.\nGartner Magic Quadrant 2025 cho Quản lý Container Trong báo cáo Gartner Magic Quadrant 2025 cho Quản lý Container, AWS tiếp tục được công nhận là Nhà lãnh đạo trong 3 năm liên tiếp và được xếp hạng xa nhất về “Độ hoàn thiện Tầm nhìn”.\nGartner định nghĩa quản lý container là các dịch vụ hỗ trợ triển khai và vận hành workloads được container hóa. Quá trình này bao gồm việc điều phối và giám sát toàn bộ vòng đời container—từ triển khai, mở rộng, đến vận hành—để đảm bảo hiệu năng và tính nhất quán trên nhiều môi trường khác nhau.\nAWS container services cung cấp quản lý container được AWS vận hành toàn phần, kết hợp giữa công nghệ gốc của AWS và mã nguồn mở, mang lại nhiều lựa chọn triển khai từ Kubernetes cho đến bộ điều phối native.\nBạn có thể sử dụng Amazon Elastic Container Service (Amazon ECS) và Amazon Elastic Kubernetes Service (Amazon EKS). cả hai đều có thể chạy với AWS Fargate để triển khai container serverless. Ngoài ra, EKS Auto Mode đơn giản hóa việc quản lý Kubernetes bằng cách tự động cung cấp hạ tầng, chọn instance tối ưu, và mở rộng tài nguyên động cho ứng dụng container.\nĐể kết nối hạ tầng on-premises hoặc edge với dịch vụ container của AWS, bạn có thể dùng EKS Hybrid Nodes and ECS Anywhere, hoặc dùng EKS Anywhere cho trải nghiệm Kubernetes tách biệt hoàn toàn nhưng vẫn được AWS hỗ trợ. Với các tùy chọn compute và triển khai linh hoạt, bạn có thể giảm chi phí vận hành, tập trung đổi mới, và mang lại giá trị kinh doanh nhanh hơn.\nTruy cập vào 2025 Gartner Magic Quadrant for Container Management để xem thêm thông tin.\n— Channy\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Chuẩn bị tài nguyên",
	"tags": [],
	"description": "",
	"content": "Để chuẩn bị cho phần này của workshop, bạn sẽ cần phải:\nTriển khai CloudFormation stack Sửa đổi bảng định tuyến VPC. Các thành phần này hoạt động cùng nhau để mô phỏng DNS forwarding và name resolution.\nTriển khai CloudFormation stack Mẫu CloudFormation sẽ tạo các dịch vụ bổ sung để hỗ trợ mô phỏng môi trường truyền thống:\nMột Route 53 Private Hosted Zone lưu trữ các bản ghi Bí danh (Alias records) cho điểm cuối PrivateLink S3 Một Route 53 Inbound Resolver endpoint cho phép \u0026ldquo;VPC Cloud\u0026rdquo; giải quyết các yêu cầu resolve DNS gửi đến Private Hosted Zone Một Route 53 Outbound Resolver endpoint cho phép \u0026ldquo;VPC On-prem\u0026rdquo; chuyển tiếp các yêu cầu DNS cho S3 sang \u0026ldquo;VPC Cloud\u0026rdquo; Click link sau để mở AWS CloudFormation console. Mẫu yêu cầu sẽ được tải sẵn vào menu. Chấp nhận tất cả mặc định và nhấp vào Tạo stack. Có thể mất vài phút để triển khai stack hoàn tất. Bạn có thể tiếp tục với bước tiếp theo mà không cần đợi quá trình triển khai kết thúc.\nCập nhật bảng định tuyến private on-premise Workshop này sử dụng StrongSwan VPN chạy trên EC2 instance để mô phỏng khả năng kết nối giữa trung tâm dữ liệu truyền thống và môi trường cloud AWS. Hầu hết các thành phần bắt buộc đều được cung cấp trước khi bạn bắt đầu. Để hoàn tất cấu hình VPN, bạn sẽ sửa đổi bảng định tuyến \u0026ldquo;VPC on-prem\u0026rdquo; để hướng lưu lượng đến cloud đi qua StrongSwan VPN instance.\nMở Amazon EC2 console\nChọn instance tên infra-vpngw-test. Từ Details tab, copy Instance ID và paste vào text editor của bạn để sử dụng ở những bước tiếp theo\nĐi đến VPC menu bằng cách gõ \u0026ldquo;VPC\u0026rdquo; vào Search box\nClick vào Route Tables, chọn RT Private On-prem route table, chọn Routes tab, và click Edit Routes.\nClick Add route. Destination: CIDR block của Cloud VPC Target: ID của infra-vpngw-test instance (bạn đã lưu lại ở bước trên) Click Save changes "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Báo cáo Tóm tắt: “Generative AI with Amazon Bedrock” Mục tiêu của sự kiện Tìm hiểu Amazon Bedrock là gì và các mô hình AI đang được sử dụng Học sâu hơn về Prompt Engineering: Zero-Shot, Few-Shot, Chain of Thought (CoT) Tìm hiểu Retrieval Augmented Generation và các trường hợp sử dụng RAG Khám phá thêm các dịch vụ AI pretrained Tìm hiểu Amazon Bedrock AgentCore và xem demo trực tiếp Diễn giả Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Các điểm nổi bật Foundation Models Prompt Engineering Retrieval-Augmented Generation (RAG) Bedrock AgentCore Foundation Models Mô hình ML truyền thống Foundation Models Mục đích Hẹp, chuyên biệt từng tác vụ (classification, regression, forecasting, clustering,\u0026hellip;) Đa dụng, đa miền, đa tác vụ Dữ liệu huấn luyện Thường là dữ liệu structured/tabular hoặc domain-specific Tập dữ liệu khổng lồ (text, image, code,\u0026hellip;) Yêu cầu huấn luyện Phải huấn luyện thủ công với dữ liệu gán nhãn Không cần — đã được pretrained Tùy chỉnh Cần tinh chỉnh hyperparameter, feature, dataset Tùy chọn — fine-tuning, RAG, prompt engineering Ví dụ Random Forest, XGBoost, SVM, Linear Regression Claude, Llama, Cohere Command, Titan Kỹ năng cần thiết Chuyên môn ML cao (feature engineering, training pipeline) Rất thấp — chỉ cần viết prompt Dịch vụ AWS SageMaker, Amazon ML, custom training Amazon Bedrock, Bedrock Studio Use cases Dự đoán, Forecasting doanh số, Phân loại ảnh, Fraud detection, anomaly detection Chatbot, Tóm tắt văn bản, Sinh mã, Sinh ảnh, Ứng dụng hội thoại, Tìm kiếm tài liệu với RAG, Sentiment analysis, Dịch đa ngôn ngữ Supported foundation models in Amazon Bedrock\nPrompt Engineering - Tạo và tinh chỉnh hướng dẫn (prompt) Prompt là gì? Là đầu vào bạn cung cấp cho mô hình AI (như ChatGPT, Claude, Gemini, hoặc các mô hình trên Amazon Bedrock) để nhận được đầu ra mong muốn. Ví dụ Sự khác nhau giữa Zero-Shot, Few-Shot và Chain of Thought trong Prompting Techniques Kỹ thuật Mô tả Khi nào dùng Ưu điểm Hạn chế Ví dụ Zero-Shot Model chỉ nhận instruction, không có ví dụ mẫu Nhiệm vụ đơn giản; model đã quen miền kiến thức Prompt ngắn, nhanh, không cần chuẩn bị nhiều Chính xác thấp với tác vụ phức tạp \u0026ldquo;Hãy phân loại đoạn văn thành neutral, negative hoặc positive.\u0026rdquo; Few-Shot Cung cấp một vài ví dụ để dạy model pattern Yêu cầu output theo format riêng; tác vụ theo mẫu Chính xác cao hơn; model học phong cách bạn muốn Prompt dài hơn; tốn token Ví dụ có mô tả về \u0026ldquo;whatpu\u0026rdquo; Chain-of-Thought Yêu cầu model giải thích từng bước Bài toán logic, toán, code, multi-step Lý luận tốt hơn, output chính xác hơn Chậm hơn; trả lời dài \u0026ldquo;Hãy cho thấy từng bước suy luận.\u0026rdquo; Retrieval Augmented Generation (RAG) RAG là gì? Retrieval: Truy xuất dữ liệu liên quan từ nguồn ngoài (database, document store, API,…) Augmentation: Thêm ngữ cảnh vào prompt gửi cho mô hình Generation: Mô hình trả lời dựa trên ngữ cảnh đã bổ sung Use cases RAG Cải thiện chất lượng nội dung: giảm hallucination, dùng kiến thức cập nhật Chatbot theo ngữ cảnh: tích hợp dữ liệu doanh nghiệp Tìm kiếm cá nhân hóa Tóm tắt dữ liệu real-time Embeddings là gì? Biểu diễn văn bản dưới dạng vector số Giúp mô hình hiểu ngữ nghĩa và mối quan hệ giữa các từ So sánh độ tương đồng giữa các đoạn văn Hỗ trợ đa ngôn ngữ Amazon Titan Embedding RAG in Action Data Ingestion Workflow RetrieveAndGenerate API Các dịch vụ AI Pretrained khác (hình ảnh Amazon Polly)\n(hình ảnh Amazon Comprehend)\n(hình ảnh Amazon Kendra)\n(hình ảnh Amazon Lookout Family)\n(hình ảnh Amazon Personalize)\nAmazon Bedrock AgentCore Sự phát triển của Agentic Generative AI assistants: theo quy tắc có sẵn, tự động hoá tác vụ lặp lại Generative AI agents: đạt mục tiêu, xử lý tác vụ rộng hơn, tự động hoá quy trình hoàn chỉnh Agentic AI systems: đa agent, tự điều phối, mô phỏng tư duy con người Framework xây dựng agent Strands agents SDK Langgraph, Langchain OpenAI Agents SDK Crew.AI Google ADK LlamaIndex “Vực sâu” từ prototype → production AgentCore — giải pháp giúp agent vận hành ở quy mô lớn Trải nghiệm Sự kiện (Event Experience) Tham dự workshop “Generative AI with Amazon Bedrock” mang lại nhiều giá trị giúp tôi hiểu sâu hơn cách AI hiện đại và các công nghệ cloud được ứng dụng trong thực tế.\nHọc hỏi từ chuyên gia Diễn giả chia sẻ các tình huống thực tế về ứng dụng AI trong doanh nghiệp Hiểu vì sao foundation models, RAG và agentic workflow quan trọng đối với scalability Trải nghiệm kỹ thuật thực hành Học trực tiếp về Zero-Shot / Few-Shot / Chain-of-Thought Demo RAG giúp thấy rõ cách giảm hallucination Quan sát Bedrock AgentCore vận hành tool, lập kế hoạch, xử lý quy trình Hiểu thêm về hiện đại hóa hệ thống Nắm rõ sự khác nhau giữa ML truyền thống và foundation models Hiểu các mô hình hiện đại hóa như Event-driven Architecture, DDD, AI-augmented workflows Khám phá công cụ AI hiện đại Titan Embedding, Textract, Comprehend, Kendra, Polly… Amazon Q Developer hỗ trợ coding, debugging, refactoring Mở rộng kết nối Giao lưu với cloud engineers, AI specialists, developers Hiểu thêm các use cases thực tế trong doanh nghiệp Những điều rút ra RAG là chìa khóa cho AI doanh nghiệp Prompt engineering rất quan trọng Agentic AI là tương lai của workflow automation Hiện đại hoá hệ thống cần roadmap rõ ràng + đo lường ROI AI trong developer workflow tăng năng suất đáng kể "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.1-workshop-overview/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Tổng quan workshop Dưới đây là cái nhìn tổng quan về cách các dịch vụ AWS được sử dụng để thiết lập hệ thống Metropolitano, đảm bảo các tiêu chí \u0026ldquo;high availability – secure – scalable\u0026rdquo; (khả dụng cao – bảo mật – mở rộng tốt):\nTổng quan kiến trúc hệ thống Metropolitano trên AWS Hệ thống được thiết kế để quản lý, giám sát và điều phối hoạt động đường sắt đô thị, sử dụng nhiều dịch vụ AWS cho từng lớp chức năng:\nLớp Dịch vụ AWS Vai trò trong hệ thống Metropolitano Mạng \u0026amp; Bảo mật Route 53 Quản lý DNS, định tuyến lưu lượng người dùng (operator, quản lý) tới các ứng dụng một cách hiệu quả và ổn định. CloudFront Mạng phân phối nội dung (CDN), giúp phân phối nội dung tĩnh (giao diện người dùng, báo cáo) với độ trễ thấp và tăng cường bảo mật. WAF (Web Application Firewall) Bảo vệ ứng dụng web khỏi các cuộc tấn công phổ biến, lọc lưu lượng độc hại trước khi vào máy chủ ứng dụng. Secrets Manager Quản lý an toàn thông tin nhạy cảm như mật khẩu database (RDS), API Key và các credentials khác. Giúp tăng cường bảo mật và giảm rủi ro rò rỉ thông tin. Lưu trữ \u0026amp; Ứng dụng S3 (Simple Storage Service) Lưu trữ dữ liệu phi cấu trúc hoặc ít truy cập, như log, backup, tài liệu dự án. EC2 (Elastic Compute Cloud) Cung cấp tài nguyên tính toán để chạy backend, các dịch vụ vận hành và giao diện người dùng. Có thể chạy trong Auto Scaling Groups để đảm bảo High Availability. RDS (MSSQL) Cơ sở dữ liệu quan hệ được quản lý hoàn toàn, dùng để lưu thông tin quan trọng (route, lịch trình, trạng thái thiết bị). Thu thập \u0026amp; Xử lý Kinesis Thu thập và xử lý dữ liệu realtime, giúp xây dựng hệ thống phân tích theo thời gian thực. EventBridge Xây dựng kiến trúc hướng sự kiện, tự động hóa workflow khi có sự kiện xảy ra (ví dụ: cảnh báo vượt ngưỡng). SQS (Simple Queue Service) Hàng đợi tin nhắn, tách biệt các thành phần hệ thống, xử lý tác vụ async (như gửi thông báo hàng loạt). SNS (Simple Notification Service) Gửi thông báo quan trọng qua Email/SMS, tích hợp với các hệ thống khác. CloudWatch Giám sát tài nguyên và ứng dụng, thu thập log, tạo cảnh báo sớm để đảm bảo hệ thống ổn định. Phân tích \u0026amp; Trực quan hóa QuickSight Dịch vụ BI trực quan hóa dữ liệu từ RDS và Kinesis, hỗ trợ quản lý theo dõi hệ thống realtime và ra quyết định. Tự động hóa phát triển (DevOps) CodePipeline CI/CD tự động build – test – deploy mã nguồn. CodeBuild Build source, chạy test tự động và tạo artifact sẵn sàng deploy. CodeDeploy Triển khai ứng dụng lên EC2 hoặc môi trường khác mà không gián đoạn hoạt động. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/",
	"title": "Nhật ký công việc",
	"tags": [],
	"description": "",
	"content": "Tuần 1: Làm quen với AWS và các dịch vụ cơ bản trong AWS\nTuần 2: Làm công việc với Amazon RDS,AWS DynamoDB, LightSail và EC2 Auto Scaling\nTuần 3: Học về AWS Security\nTuần 4: Học về AWS DynamoDB và Relational Database Services\nTuần 5: Phân quyền và lưu trữ\nTuần 6: Mã hóa dữ liệu\nTuần 7: Tối ưu hiệu năng\nTuần 8: setup networking, Phân quyền, và chia sẻ file\nTuần 9: Thực hành với bảo mật và vận hành\nTuần 10: Học về quan sát hiệu năng\nTuần 11: Học về Redis và những dịch vụ bảo mật\nTuần 12: Học về QuickSight, Athena và data lake\nTuần 12: Học về cách chuyển qua dịch vụ AWS\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/4-eventparticipated/4.1-event1/",
	"title": "Sự kiện 1",
	"tags": [],
	"description": "",
	"content": "Báo cáo tổng kết: “Data Science on AWS” Mục tiêu sự kiện Khám phá cách AWS giải quyết các bài toán dữ liệu bằng dịch vụ của mình Giới thiệu tổng quan về các Managed AI Services và các trường hợp sử dụng thực tế Chuẩn bị dữ liệu với Amazon SageMaker Ứng dụng XGBoost trong SageMaker Studio Notebooks Khai thác AutoML không cần code với SageMaker Canvas Diễn giả Văn Hoàng Kha – Cloud Solutions Architect, AWS User Group Leader Bạch Doãn Vương – Cloud DevOps Engineer, AWS Community Builder Đoàn Nguyễn Thanh Hòa – Giảng viên CF, Đại học FPT TP.HCM Nội dung nổi bật (Key Highlights) Amazon Comprehend và Amazon Translate Phân tích và dịch văn bản bằng công nghệ học sâu (Deep Learning)\nXử lý nhiều loại tài liệu như email, chat, mạng xã hội, cuộc gọi\u0026hellip; và tự động trích xuất thông tin hữu ích. Các trường hợp sử dụng phổ biến của Amazon Comprehend: Xử lý tài liệu thông minh Tự động hóa quy trình email Phân loại và định tuyến ticket hỗ trợ khách hàng Gắn thẻ tài liệu và nội dung media Phân tích cảm xúc khách hàng Phân tích cuộc gọi tổng đài Phát hiện và ẩn thông tin nhạy cảm (PII) Amazon Translate Dịch máy thần kinh (Neural Machine Translation)\nTính năng chính:\nHỗ trợ ngôn ngữ rộng: 4970 cặp ngôn ngữ dịch X↔Y Độ trễ thấp: \u0026lt;150ms cho mỗi câu Bảo mật dữ liệu: mã hóa và quản lý truy cập đầy đủ Phủ sóng khu vực rộng: 17 vùng AWS Tùy chỉnh dịch: dùng Custom Terminologies và Active Custom Translation Dịch hàng loạt: hỗ trợ định dạng DOCX, PPTX, XLSX, XML, HTML, TXT Mô hình huấn luyện đa lĩnh vực: 11 domain khác nhau Tính phí theo sử dụng: dễ tích hợp qua API Trường hợp sử dụng:\nBản địa hóa nội dung: tài liệu doanh nghiệp, phụ đề video, lưu trữ Giao tiếp: tương tác khách hàng, chat trong game, bài viết mạng xã hội Phân tích văn bản: Voice of Customer, phân tích media, eDiscovery Amazon Polly Dịch vụ chuyển văn bản thành giọng nói (Text-to-Speech)\nAmazon Polly sử dụng công nghệ học sâu để tổng hợp giọng nói tự nhiên như con người.\nTính năng:\nText-to-Speech (TTS) Ngôn ngữ đánh dấu SSML Tùy chỉnh từ vựng (Lexicons) Dấu giọng nói (Speech Marks) Tạo giọng thương hiệu (Brand Voice) Ứng dụng thực tế:\nĐọc tin tức, tài liệu đào tạo Tổng đài thoại/IVR Podcast, học ngoại ngữ Dẫn đường, nhắc việc, công cụ hỗ trợ người khuyết tật Amazon Transcribe Dịch vụ nhận dạng giọng nói tự động (ASR)\nChuyển nội dung âm thanh/video thành văn bản Hỗ trợ cả ghi âm sẵn và phát trực tiếp theo thời gian thực Amazon Lex Dịch vụ xây dựng chatbot và giao diện hội thoại thông minh\nTính năng:\nDễ sử dụng Hiểu ngôn ngữ tự nhiên chính xác Tích hợp sẵn với hệ sinh thái AWS Tiết kiệm chi phí Amazon Rekognition Phân tích hình ảnh và video để phát hiện đối tượng, khuôn mặt, cảnh vật và nội dung không phù hợp.\nAmazon Personalize Cá nhân hóa trải nghiệm người dùng\nTriển khai hệ thống gợi ý nhanh chóng Phản ứng theo hành vi người dùng theo thời gian thực Dễ dàng tích hợp với hệ thống hiện có Giảm thời gian ra thị trường nhờ dịch vụ ML được quản lý Feature Engineering Chuẩn bị dữ liệu với Amazon SageMaker Canvas Ứng dụng vào công việc — Bài học rút ra từ Key Highlights 1. Hiểu dữ liệu \u0026amp; Tự động hóa (Amazon Comprehend, Translate) Bài học:\nKhai thác dữ liệu phi cấu trúc là nền tảng cho việc ra quyết định thông minh.\nỨng dụng:\nSử dụng Amazon Comprehend để phân tích cảm xúc khách hàng, phân loại tài liệu hoặc email. Tự động định tuyến ticket hỗ trợ dựa trên nội dung và cảm xúc. Dùng Amazon Translate để dịch nhanh tài liệu trong các dự án đa ngôn ngữ. 2. Tăng tương tác \u0026amp; Giao diện thoại (Amazon Polly, Lex, Transcribe) Bài học:\nAI giọng nói giúp nâng cao trải nghiệm người dùng và khả năng tiếp cận dịch vụ.\nỨng dụng:\nKết hợp Lex + Transcribe + Polly để xây dựng chatbot hỗ trợ khách hàng bằng giọng nói 24/7. Sử dụng Polly tạo giọng đọc tự nhiên cho nội dung đào tạo hoặc podcast. Dùng Transcribe để ghi âm, phân tích và tóm tắt nội dung cuộc họp hoặc cuộc gọi. 3. Phân tích hình ảnh \u0026amp; video (Amazon Rekognition) Bài học:\nThị giác máy tính giúp tự động hóa việc phân tích và kiểm duyệt nội dung đa phương tiện.\nỨng dụng:\nÁp dụng Rekognition để tự động gắn thẻ, phân loại hình ảnh/video. Phát hiện khuôn mặt hoặc hành vi bất thường trong hệ thống an ninh hoặc bán lẻ. 4. Cá nhân hóa trải nghiệm người dùng (Amazon Personalize) Bài học:\nCá nhân hóa là yếu tố cốt lõi để giữ chân người dùng và nâng cao trải nghiệm.\nỨng dụng:\nTích hợp Personalize để gợi ý sản phẩm/dịch vụ theo hành vi người dùng. Xây dựng hệ thống gợi ý phản hồi theo thời gian thực, thích ứng liên tục. 5. Đơn giản hóa Machine Learning (Amazon SageMaker, Canvas) Bài học:\nMachine Learning không còn là đặc quyền của chuyên gia — AWS giúp AI trở nên dễ tiếp cận.\nỨng dụng:\nDùng SageMaker Canvas để huấn luyện và dự đoán mà không cần viết code. Ứng dụng SageMaker Studio để thử nghiệm các mô hình như XGBoost. Hiểu rõ vai trò của Feature Engineering để cải thiện độ chính xác của mô hình. 6. Tư duy hiện đại hóa dựa trên dữ liệu Bài học:\nCác dịch vụ AI của AWS thúc đẩy tư duy kiến trúc hướng sự kiện (event-driven) và tập trung vào dữ liệu.\nỨng dụng:\nKết hợp Domain-Driven Design (DDD) với quy trình AI để xây dựng hệ thống linh hoạt, dễ mở rộng. Sử dụng serverless (Lambda, API Gateway) để triển khai nhanh các pipeline AI. Thiết kế hệ thống xử lý dữ liệu bất đồng bộ theo mô hình event streaming để tăng hiệu năng. 🌟 Tổng kết Qua hội thảo “Data Science on AWS”, tôi không chỉ học về từng dịch vụ AI riêng lẻ mà còn hiểu được cách liên kết chúng thành một hệ sinh thái dữ liệu thông minh.\nMỗi công cụ — từ Comprehend, Translate, Polly, Rekognition, Personalize đến SageMaker — đều góp phần tạo nên hệ thống tự động, linh hoạt và hướng người dùng.\nNhững kiến thức và bài học này có thể áp dụng trực tiếp vào dự án thực tế, giúp chuyển đổi quy trình thủ công sang mô hình vận hành thông minh dựa trên AI và dữ liệu.\nHình ảnh sự kiện (Thêm ảnh sự kiện của bạn tại đây)\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.3-s3-secrets-manager/5.3.1-create-secrets-manager/",
	"title": "Tạo Secrets Manager",
	"tags": [],
	"description": "",
	"content": " Mở Amazon Secrets Manager console Trong thanh điều hướng, chọn Secrets, sau đó nhấn Store a new secret: Trong giao diện Create endpoint: Chọn 'Other type of secret' Trong phần Key/value pairs, chúng ta sẽ có 5 tên secret Các bước tiếp theo giữ mặc định. Sau khi tạo xong cả 5 secrets manager: "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.1-week1/",
	"title": "Worklog Tuần 1",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 1 Làm quen với các thành viên trong First Cloud Journey. Hiểu các dịch vụ cơ bản của AWS và cách sử dụng AWS Console \u0026amp; CLI. Các nhiệm vụ thực hiện trong tuần Ngày Nhiệm vụ Bắt đầu Hoàn thành Tài liệu tham khảo 2 - Làm quen với thành viên FCJ.\n- Tìm hiểu nội quy trong quá trình đào tạo.\nThực hành:\n+ Tạo tài khoản AWS Free Tier 08/09/2025 09/09/2025 — 3 Học về AWS và các dịch vụ AWS:\n+ EC2\n+ Lambda\n+ SQS\n+ SNS\n+ CLI \u0026amp; SDKs 09/09/2025 10/09/2025 — 4 Học về ECS, EKS, VPC, CloudFormation.\nHọc Cost Management.\nThực hành:\n+ Tạo EC2 instance\n+ Tạo Lambda function\n+ Sử dụng AWS CLI 10/09/2025 11/09/2025 — 5 Học VPC cơ bản và S3.\nThực hành:\n+ Tạo Security Group\n+ Tạo Internet Gateway\n+ Tạo Subnet\n+ Tạo Route Table 11/09/2025 12/09/2025 — 6 Thực hành:\n+ Tạo S3 Bucket 12/09/2025 13/09/2025 — Kết quả đạt được trong tuần 1. Kiến thức nền tảng AWS Hiểu được các nhóm dịch vụ chính của AWS: Compute, Storage, Networking, Database. Tạo và cấu hình thành công tài khoản AWS Free Tier. Làm quen với AWS Management Console và biết cách truy cập, tìm kiếm và sử dụng các dịch vụ. 2. Sử dụng AWS CLI Thực hiện được các thao tác cơ bản:\nKiểm tra thông tin tài khoản \u0026amp; cấu hình. Xem danh sách region. Lấy thông tin dịch vụ EC2. Tạo và quản lý Key Pair. Kiểm tra trạng thái dịch vụ đang chạy. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.2-week2/",
	"title": "Worklog Tuần 2",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 2 Tìm hiểu về Amazon RDS, AWS S3, AWS EC2 Auto Scaling, DynamoDB và Amazon Lightsail. Các nhiệm vụ thực hiện trong tuần Ngày Nhiệm vụ Bắt đầu Hoàn thành Tài liệu tham khảo 2 - Học AWS DynamoDB, AWS RDS, Amazon Aurora.\nThực hành:\n+ Tạo CloudFront và S3 14/09/2025 15/09/2025 https://render.skillbuilder.aws 3 Thực hành:\n+ Tạo AWS RDS\n+ Tạo DynamoDB bằng Python và AWS CLI 15/09/2025 16/09/2025 https://render.skillbuilder.aws 4 Dịch Blog 13/09/2025 13/09/2025 https://cloudjourney.awsstudygroup.com/ 5 Học EC2 Auto Scaling, Amazon Lightsail.\nThực hành:\n+ Tạo Database trên RDS\n+ Sử dụng Amazon Lightsail 19/09/2025 20/09/2025 https://000005.awsstudygroup.com/ https://000045.awsstudygroup.com/ 6 Thực hành:\n+ Tạo RDS, VPC, EC2 với EC2 Auto Scaling 20/09/2025 21/09/2025 https://000006.awsstudygroup.com/ Kết quả đạt được trong tuần 1. Static Website Hosting với Amazon S3 Biết cách tạo S3 Bucket, upload file và quản lý truy cập. Cấu hình Static Website Hosting để bucket hoạt động như một website. Thiết lập quyền public giúp website được truy cập từ Internet. Cấu hình Bucket Policy để chỉ cho phép đọc công khai (public read-only). 2. Kiến thức về Amazon RDS – Dịch vụ cơ sở dữ liệu quan hệ Hỗ trợ các hệ quản trị phổ biến: MySQL, PostgreSQL, Oracle, SQL Server. Biết cách kết nối RDS từ EC2. Nắm rõ endpoint, port, username/password để đăng nhập DB. Tự tạo và cấu hình VPC, subnet, security group để bảo vệ RDS Instance. 3. Mở rộng ứng dụng với EC2 Auto Scaling Hiểu EC2 Auto Scaling là dịch vụ tự động tăng/giảm số lượng EC2 dựa trên nhu cầu thực tế. Thành phần chính của Auto Scaling: Launch Configuration / Launch Template Auto Scaling Group (ASG) Scaling Policies: Dynamic Scaling và Scheduled Scaling Health Checks Biết cách sử dụng Amazon CloudWatch để thu thập các metric như CPU Utilization, Network Traffic, Request Count… giúp ASG đưa ra quyết định scale hợp lý. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.3-week3/",
	"title": "Worklog Tuần 3",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 3 Hiểu các khái niệm bảo mật cơ bản của AWS. Tìm hiểu các dịch vụ bảo mật quan trọng như AWS KMS, Amazon Macie, AWS Certificate Manager. Hiểu cách bảo vệ hạ tầng AWS. Công việc đã thực hiện trong tuần Ngày Công việc Bắt đầu Hoàn thành Tài liệu tham khảo 2 Tìm hiểu cách bảo vệ dữ liệu bằng các cơ chế sẵn có của AWS (S3, EBS, DynamoDB) và các dịch vụ bảo mật (KMS, Macie, Certificate Manager). 22/09/2025 23/09/2025 Link SkillBuilder 3 Tìm hiểu cách bảo vệ mạng và ứng dụng thông qua bảo mật hạ tầng (Security Groups, ELB, Regions) và dịch vụ bảo vệ (AWS Shield, AWS WAF). 23/09/2025 24/09/2025 Link SkillBuilder 4 Tìm hiểu quy trình phát hiện và phản hồi sự cố bảo mật với Amazon Inspector, GuardDuty, Amazon Detective, AWS Security Hub. 24/09/2025 25/09/2025 Link SkillBuilder 5 Tìm hiểu cách ngăn truy cập trái phép bằng các dịch vụ như IAM Identity Center, Secrets Manager, Systems Manager. 25/09/2025 26/09/2025 Link SkillBuilder 6 Thực hành: + Cấu hình liên kết danh tính (Identity Federation) với AWS Single Sign-On. 26/09/2025 29/09/2025 https://000012.awsstudygroup.com/ Kết quả đạt được 1. Bảo vệ dữ liệu và quản trị bảo mật Hiểu: Mã hóa \u0026amp; quản lý khóa bằng AWS KMS Quét dữ liệu nhạy cảm với Amazon Macie Quản lý chứng chỉ bằng AWS Certificate Manager Các chế độ bảo mật có sẵn của S3, EBS, DynamoDB 2. Bảo mật mạng và ứng dụng Nắm được các khái niệm bảo mật nhiều lớp: Security Groups Elastic Load Balancer Chiến lược bảo vệ đa vùng (Multi-Region) Hiểu cách AWS Shield và AWS WAF giảm thiểu tấn công DDoS và tấn công web. 3. Phát hiện \u0026amp; ứng phó sự cố bảo mật Hiểu vai trò của: Amazon Inspector – kiểm tra lỗ hổng Amazon GuardDuty – phát hiện mối đe dọa Amazon Detective – phân tích điều tra AWS Security Hub – quản lý tuân thủ \u0026amp; tổng hợp cảnh báo 4. Ngăn truy cập trái phép Hiểu cơ bản về: IAM Identity Center (SSO) Secrets Manager Systems Manager "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.4-week4/",
	"title": "Worklog tuần 4",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 4 Luyện tập tạo, quản lý và triển khai tài nguyên AWS thông qua nhiều công cụ (Console, CLI, SDK, Elastic Beanstalk\u0026hellip;). Xây dựng một ứng dụng web mẫu sử dụng Lambda, S3, DynamoDB và API Gateway. Công việc đã thực hiện trong tuần Ngày Công việc Bắt đầu Hoàn thành Tài liệu tham khảo 2 Thực hành: Tạo bảng trong AWS DynamoDB bằng AWS CLI hoặc Python SDK thông qua Access Key. 28/09/2025 29/09/2025 3 Thực hành: Xây dựng ứng dụng Book Store sử dụng Lambda, S3 và DynamoDB. 30/09/2025 01/10/2025 https://000078.awsstudygroup.com/ 4 Học mô hình dữ liệu JSON \u0026amp; Document. 01/10/2025 02/10/2025 5 Thực hành: + Triển khai và kiểm tra tài nguyên AWS bằng CloudFormation Template.\n+ Sử dụng AWS Tools for Eclipse để deploy ứng dụng Java lên Elastic Beanstalk.\n+ Cài đặt và cấu hình Elastic Beanstalk CLI.\n+ Dùng EB CLI để deploy bản cập nhật cho môi trường có sẵn.\n+ Dùng AWS SDK để truy vấn và chỉnh sửa môi trường AWS bằng code. 03/10/2025 04/10/2025 https://000050.awsstudygroup.com/ 6 Thực hành: Xây dựng web frontend kết nối đến database qua Lambda và API Gateway. 04/10/2025 05/10/2025 https://000079.awsstudygroup.com/ Kết quả đạt được 1. Làm việc với AWS DynamoDB Tạo và quản lý các bảng DynamoDB bằng CLI và Python SDK. Hiểu cách định nghĩa Partition Key, Sort Key và cấu hình Read/Write Capacity. 2. Xây dựng ứng dụng Book Store (Lambda + S3 + DynamoDB) Tạo Lambda function xử lý CRUD cho dữ liệu sách. Kết nối API Gateway với Lambda để xây dựng REST API. Lưu ảnh bìa sách và file tĩnh trong Amazon S3. 3. Hiểu mô hình JSON \u0026amp; Document Nắm cách DynamoDB lưu trữ dữ liệu bán cấu trúc. Biết cách truy vấn và cập nhật dữ liệu ở dạng JSON. 4. Triển khai ứng dụng Java với Elastic Beanstalk Triển khai hạ tầng qua Console và CloudFormation. Cài đặt EB CLI và deploy bản cập nhật. Sử dụng AWS SDK để thao tác với tài nguyên AWS bằng mã nguồn Java. 5. Xây dựng frontend kết nối backend serverless Tạo giao diện cho phép thêm, sửa, xóa dữ liệu trong DynamoDB. Hiểu quy trình: Fr "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.5-week5/",
	"title": "Worklog tuần 5",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 5 Khám phá và thực hành xác thực người dùng và lưu trữ dữ liệu sử dụng AWS Amplify. Hiểu và triển khai ứng dụng serverless sử dụng AWS SAM (Serverless Application Model). Học và triển khai xác thực người dùng với Amazon Cognito. Nhiệm vụ đã hoàn thành trong tuần Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học AWS Amplify. 05/10/2025 06/10/2025 https://aws.amazon.com/vi/amplify/ 3 Thực hành: Sử dụng Amplify Authentication và Amplify Storage cho ứng dụng serverless. 06/10/2025 09/10/2025 https://000134.awsstudygroup.com/ 4 Học AWS SAM: + Cấu trúc template SAM và SAM CLI + SAM Accelerate và tích hợp SAM CLI 09/10/2025 10/10/2025 https://aws.amazon.com/vi/serverless/sam/ 5 Thực hành: + Cài đặt SAM CLI + Triển khai frontend và Lambda function + Cấu hình API Gateway và kiểm thử API bằng Postman 10/10/2025 11/10/2025 https://000080.awsstudygroup.com/ 6 Học Amazon Cognito và Thực hành: Triển khai xác thực người dùng với Cognito. 11/10/2025 13/10/2025 https://000081.awsstudygroup.com/ Thành tựu Tuần 5 1. Làm việc với AWS Amplify Hiểu kiến trúc Amplify và cách tích hợp với hệ thống serverless. Triển khai Amplify Authentication cho luồng đăng ký và đăng nhập người dùng. Sử dụng Amplify Storage để quản lý upload file và truy cập dữ liệu trên Amazon S3. Quản lý môi trường Amplify bằng CLI và tích hợp với ứng dụng frontend. 2. Học và áp dụng AWS SAM (Serverless Application Model) Hiểu cấu trúc template SAM; nắm cách SAM sử dụng CloudFormation. Cài đặt và cấu hình SAM CLI cho phát triển và triển khai cục bộ. Thực hành triển khai ứng dụng serverless sử dụng SAM Accelerate. Triển khai Lambda function và kiểm thử endpoint API qua API Gateway bằng Postman. Hiểu quy trình làm việc: SAM → Lambda → API Gateway → CloudFormation. 3. Học và thực hành Amazon Cognito Hiểu Cognito User Pools (xác thực) và Identity Pools (phân quyền + truy cập dịch vụ AWS). Triển khai luồng xác thực người dùng bằng Cognito. Tích hợp Cognito với AWS Amplify để xác thực liền mạch trên frontend. Thử nghiệm thành công các luồng: đăng ký, đăng nhập, xác nhận và xác thực token. 4. Kết hợp AWS Amplify, SAM và Cognito Xây dựng ứng dụng serverless hoàn chỉnh, hỗ trợ xác thực người dùng, tương tác API và triển khai trên cloud. Nâng cao hiểu biết về kiến trúc serverless, chuẩn bị cho các dự án phát triển cloud-native nâng cao trong các tuần tới. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.6-week6/",
	"title": "Worklog tuần 6",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 6 Hiểu và thực hành AWS Backup, bao gồm tạo Backup Vault, Backup Plan, và triển khai cấu hình sao lưu bằng AWS CloudFormation. Tìm hiểu hoạt động của AWS WAF và AWS PrivateLink, gồm các thành phần: ACL, Rules, Rule Groups, VPC Endpoint Services, và Network Load Balancer. Nghiên cứu AWS KMS (Dịch vụ quản lý khóa) — quản lý khóa đối xứng và khóa bất đối xứng, và vai trò của chúng trong mã hóa dữ liệu. Nắm vững Containerization với Docker, cách xây dựng và triển khai ứng dụng sử dụng Docker Images và Docker Compose. Nâng cao kỹ năng Infrastructure as Code (IaC) và triển khai dựa trên container để xây dựng môi trường cloud an toàn và mở rộng. Nhiệm vụ trong tuần Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học AWS Backup: Backup vault, Backup plan, Sử dụng CloudFormation để tạo backup plan 13/10/2025 15/10/2025 https://000013.awsstudygroup.com/ 3 Thực hành: Tạo backup plan, on-demand backup, backup vaults 13/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 4 Học AWS WAF và AWS PrivateLink: ACL, Rules, Rule Groups, VPC Endpoint Service, VPC Endpoint, Network Load Balancer 15/10/2025 17/10/2025 https://000026.awsstudygroup.com/ https://000111.awsstudygroup.com/ 5 Học AWS KMS: Khóa đối xứng, Khóa bất đối xứng; Thực hành: Tạo ACLs, rules và rule groups 17/10/2025 19/10/2025 https://000033.awsstudygroup.com/ 6 Học Containerization với Docker: Docker là gì, Triển khai dùng Docker Image, Triển khai bằng Docker Compose và push image 19/10/2025 21/10/2025 https://000015.awsstudygroup.com/ Thành tựu Tuần 6 Học và thực hành thành công AWS Backup, tạo Backup Vault, Backup Plans, và On-demand Backups qua AWS Console và CloudFormation. Cấu hình và kiểm thử AWS WAF, tạo ACLs, Rules, và Rule Groups để bảo vệ ứng dụng web. Thực hành AWS PrivateLink, triển khai VPC Endpoint Services và VPC Endpoints kết nối qua Network Load Balancer. Thực hành sử dụng AWS KMS, tạo và quản lý cả Khóa đối xứng và Khóa bất đối xứng cho mã hóa dữ liệu. Xây dựng và triển khai ứng dụng container hóa với Docker, bao gồm Docker Images, containers, Docker Compose, và push images lên Docker Hub. Nâng cao hiểu biết về Bảo mật Cloud, Mã hóa, và Containerization, chuẩn bị triển khai ứng dụng thực tế trên AWS Cloud. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.7-week7/",
	"title": "Worklog tuần 7",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 7: Tìm hiểu các dịch vụ AWS tối ưu hiệu năng. Nắm các dịch vụ cần thiết cho tối ưu hiệu năng như ECS, EKS, CodePipeline, Storage Gateway. Tìm hiểu về Docker, Kubernetes và mối quan hệ giữa Docker và Kubernetes. Công việc thực hiện trong tuần: Ngày Công việc Ngày bắt đầu Ngày kết thúc Tài liệu tham khảo 2 - Học AWS EKS: + Control Plane (AWS quản lý) + Worker Nodes (người dùng quản lý) + Các thành phần trong EKS: Cluster, Node Group, Pod, Deployment, Service 20/10/2025 21/10/2025 https://000126.awsstudygroup.com/1-introduce/ 3 - Thực hành: + Tạo mạng: VPC, Subnets, Internet Gateway + Cấu hình Auth cho Control Plane + Tạo EKS Cluster + Cài addon: VPC-CNI, kube-proxy + Tạo Auth cho Worker Node + Tạo Worker Node + Cài addon: CoreDNS + Triển khai Nginx Deployment 21/10/2025 22/10/2025 https://000126.awsstudygroup.com/1-introduce/ 4 - Học AWS ECS: + Cluster, Task Definition, Task, Service, Container Agent + ECS Launch Types + Networking trong ECS + Tích hợp với các dịch vụ khác + ECS Auto Scaling 22/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Học AWS Storage Gateway: - Thực hành: + Tạo ECS Cluster + Cấu hình Docker Image + Tạo Task Definition + Đăng ký Namespace trên Cloud Map 23/10/2025 25/10/2025 https://000016.awsstudygroup.com/1-introduction/ 6 - Học AWS CodePipeline: + Source Stage: Lấy code từ GitHub, CodeCommit hoặc S3 + Build Stage: Gọi CodeBuild để build code + Deploy Stage: Gọi CodeDeploy để deploy ứng dụng 25/10/2025 27/10/2025 https://000017.awsstudygroup.com/ Thành tựu Tuần 7: 1. Học AWS EKS: Hiểu kiến trúc EKS bao gồm Control Plane (AWS quản lý) và Worker Nodes (người dùng quản lý). Hiểu định nghĩa Cluster, Node Group, Pod, Deployment, Service. Hiểu cách kết nối kubectl với EKS Cluster và quản lý workloads. 2. Thực hành trên EKS: Tạo VPC, Subnets, Internet Gateway, Route Table. Cấu hình Authentication cho EKS Control Plane (IAM Role). Tạo EKS Cluster và cài addon: VPC-CNI, kube-proxy. Tạo Node Group cho Worker Node và cài addon CoreDNS. Triển khai thành công Nginx Deployment trên EKS và truy cập qua LoadBalancer. 3. Học AWS ECS: Hiểu cách ECS Cluster hoạt động, Task Definition, Service, Container Agent. Phân biệt ECS Launch Type (EC2 / Fargate). Hiểu cách cấu hình mạng trong ECS (bridge, host, awsvpc). Hiểu cách tích hợp ECS với CloudWatch, Load Balancer và Auto Scaling. 4. Thực hành triển khai ECS: Tạo ECS Cluster (Fargate). Build và push Docker Image lên ECR. Tạo Task Definition và Service chạy container. Đăng ký Namespace trên Cloud Map để ECS có thể thực hiện Service Discovery. Hiểu cơ bản về AWS Storage Gateway và mô hình lưu trữ hybrid. 5. Học AWS CodePipeline: Hiểu pipeline CI/CD gồm 3 stage: Source → Build → Deploy. Hiểu cách tích hợp CodePipeline với GitHub, CodeBuild và CodeDeploy. Hiểu tự động build và deploy ứng dụng Spring Boot hoặc React lên EC2/S3. Viết file buildspec.yml và appspec.yml theo template chuẩn. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.8-week8/",
	"title": "Worklog Tuần 8",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 8 Hiểu cơ bản và các trường hợp sử dụng của AWS Step Functions, bao gồm 7 loại trạng thái chính và cách điều phối các workflow phức tạp. Thực hành tạo và kiểm thử workflow trong AWS Cloud9, tập trung vào điều phối tác vụ và xử lý lỗi. Học các tính năng chính và các bước triển khai của Amazon FSx, bao gồm các biến thể khác nhau (Windows File Server, Lustre, NetApp ONTAP, OpenZFS). Thực hành cấu hình FSx tích hợp với AWS Managed Microsoft AD, đảm bảo cấu hình mạng, xác thực và chia sẻ file đúng. Khám phá AWS X-Ray để theo dõi và trực quan hóa các yêu cầu trong ứng dụng phân tán nhằm tối ưu hiệu năng và debug. Nhiệm vụ tuần này Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học AWS Step Functions: + 7 trạng thái: Task, Choice, Fail/Success, Pass, Wait, Parallel, Map + Các trường hợp sử dụng Step Functions + Lợi ích của Step Functions 26/10/2025 27/10/2025 https://000047.awsstudygroup.com/1-intro/ 3 Thực hành: + Tạo môi trường Cloud9 + Tạo dịch vụ mẫu + Khởi tạo workflow + Xử lý lỗi 27/10/2025 28/10/2025 https://000047.awsstudygroup.com/1-intro/ 4 Học Amazon FSx: + FSx cho Windows File Server + FSx cho Lustre + FSx cho NetApp ONTAP + FSx cho OpenZFS 28/10/2025 29/10/2025 https://000025.awsstudygroup.com/ 5 Thực hành: + Cấu hình chi tiết hệ thống file + Chọn VPC hiện có + Chọn AWS Managed Microsoft AD (cung cấp DNS, username/password của Service Account) + Đặt tên chia sẻ file Windows + Kiểm tra và tạo 29/10/2025 30/10/2025 https://000025.awsstudygroup.com/ 6 Học AWS X-Ray: + Trace + Segment + Subsegment + Annotation / Metadata + Service Map 30/10/2025 31/10/2025 https://000140.awsstudygroup.com/ Thành tựu Tuần 8 Hiểu và giải thích 7 trạng thái của AWS Step Functions, bao gồm vai trò của từng trạng thái trong tự động hóa workflow và chuyển trạng thái. Thực hành tạo workflow mẫu trong Cloud9 với Step Functions, bao gồm xử lý lỗi và xác nhận luồng thực thi. Nắm vững các tùy chọn của Amazon FSx và các trường hợp sử dụng phù hợp cho Windows workloads, HPC và lưu trữ doanh nghiệp. Hoàn thành cấu hình FSx cho Windows File Server, bao gồm cấu hình hệ thống file, tích hợp VPC/AD và tạo chia sẻ file. Học cách AWS X-Ray thu thập trace, segment, subsegment và trực quan hóa Service Map để nhận diện bottleneck và lỗi trong hệ thống. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.9-week9/",
	"title": "Worklog tuần 9",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 9 Hiểu và áp dụng AWS AppSync để xây dựng GraphQL API với nhiều nguồn dữ liệu khác nhau. Học và cấu hình AWS EBS Data Lifecycle Manager để tự động hóa snapshot và backup. Khám phá AWS GuardDuty cho việc phát hiện mối đe dọa thông minh dựa trên ML và phân tích hành vi. Học AWS Macie để nhận diện và bảo vệ dữ liệu nhạy cảm trong S3 bằng machine learning. Nhiệm vụ tuần này Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học AWS AppSync: + GraphQL APIs: Query, Mutation, Subscription + Nguồn dữ liệu: DynamoDB, RDS/Aurora, Lambda, HTTP Endpoints, OpenSearch + Xác thực và phân quyền: IAM, API Keys, Cognito User Pools, OpenID Connect + Hỗ trợ realtime subscription, caching, offline Thực hành: + Tạo GraphQL API + Định nghĩa schema và gắn với nguồn dữ liệu + Cấu hình request/response mapping template 02/11/2025 03/11/2025 https://000086.awsstudygroup.com/1-introduction/ 3 Học AWS EBS Data Lifecycle Manager: + Tự động backup và recovery + Giảm chi phí lưu trữ + Đảm bảo tuân thủ và bảo vệ dữ liệu + Hiểu chính sách Lifecycle: EBS Snapshot Policy, EBS-backed AMI Policy, Cross-region/Cross-account Copy Policy 03/11/2025 05/11/2025 https://000088.awsstudygroup.com/ 4 Thực hành: + Khởi tạo EC2 instance và cấu hình lifecycle policies + Chọn tài nguyên cần backup, lịch trình, và thời gian lưu trữ Học AWS GuardDuty: + Phát hiện mối đe dọa dựa trên ML và phân tích hành vi 03/11/2025 06/11/2025 https://000098.awsstudygroup.com/ 5 Học AWS Macie: + Tính năng chính: Khám phá dữ liệu, phân loại, hiển thị cấp bucket + Cách Macie hoạt động + Trường hợp sử dụng và mô hình giá 14/08/2025 15/08/2025 https://000090.awsstudygroup.com/ 6 Thực hành AWS Macie: + Tạo và cấu hình S3 bucket + Kích hoạt dịch vụ Macie + Tạo các job để quét và phân loại dữ liệu 15/08/2025 15/08/2025 https://000090.awsstudygroup.com/ Thành tựu Tuần 9 AWS AppSync: Xây dựng GraphQL API tích hợp DynamoDB và Lambda, hiểu cách thiết kế schema, resolvers và mapping templates. AWS EBS Lifecycle Manager: Cấu hình chính sách backup tự động, tăng độ tin cậy và tối ưu chi phí trên EC2. AWS GuardDuty: Hiểu cách phát hiện mối đe dọa liên tục dựa trên ML và phân tích hành vi bất thường. AWS Macie: Kích hoạt Macie trên S3, chạy job phân loại dữ liệu và đánh giá các rủi ro dữ liệu nhạy cảm, đảm bảo tuân thủ. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.11-week11/",
	"title": "Báo cáo Tuần 11",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 11 Hiểu và thực hành các dịch vụ AWS: ElastiCache (Redis), Certificate Manager, Inspector, Detective, Systems Manager Tìm hiểu cấu trúc, hoạt động và lợi ích của từng dịch vụ Thực hành kết nối, cấp quyền, triển khai và kiểm tra bảo mật trên AWS Nhiệm vụ tuần này Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học Amazon ElastiCache - Redis: + Redis là gì + Các thành phần: Cluster, node ElastiCache, Redis shard + Cách hoạt động + Lợi ích khi sử dụng ElastiCache 17/11/2025 18/11/2025 https://000061.awsstudygroup.com/1-introduce/ 3 Học AWS Certificate Manager Thực hành: + Tạo cluster + Cấp quyền truy cập cluster + Kết nối tới node cluster 18/11/2025 19/11/2025 https://000061.awsstudygroup.com/ https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html 4 Học AWS Inspector: + Phát hiện lỗ hổng bảo mật + Tìm cấu hình sai + Thực hiện các bước khắc phục 19/11/2025 20/11/2025 https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html 5 Học AWS Detective 21/11/2025 22/11/2025 https://docs.aws.amazon.com/detective/latest/userguide/what-is-detective.html 6 Thực hành với AWS Systems Manager 22/11/2025 24/11/2025 https://000031.awsstudygroup.com/1-introduce/ Thành tựu Tuần 11 Hoàn thành lý thuyết và thực hành với ElastiCache (Redis): hiểu Redis, cluster, node, shard, cách hoạt động và lợi ích. Thực hành tạo cluster Redis, cấp quyền truy cập và kết nối tới các node. Học và thực hành AWS Certificate Manager: tạo và triển khai chứng chỉ trên cluster. Học và thực hành AWS Inspector: phát hiện lỗ hổng, cấu hình sai và thực hiện các bước khắc phục. Học AWS Detective: phát hiện và điều tra các sự kiện bảo mật. Thực hành AWS Systems Manager: quản lý và vận hành tập trung các tài nguyên AWS. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/2-proposal/",
	"title": "Đề xuất",
	"tags": [],
	"description": "",
	"content": "HỆ THỐNG DỊCH VỤ ĐƯỜNG SẮT ĐÔ THỊ TRÊN NỀN TẢNG AWS Tóm tắt điều hành Sự phát triển nhanh của đô thị làm gia tăng nhu cầu về hệ thống giao thông công cộng thông minh và bền vững. Một hệ thống Đường Sắt Đô Thị hiện đại cần một nền tảng số ổn định, mở rộng linh hoạt và sẵn sàng 24/7.\nTài liệu này trình bày một kiến trúc AWS xây dựng trên Amazon EC2, mang lại khả năng kiểm soát hoàn toàn, hiệu suất cao và độ linh hoạt cho các hệ thống quan trọng.\nNền tảng hỗ trợ:\nĐặt vé, lập lịch, thanh toán và giám sát lưu lượng Thu thập dữ liệu hành khách thời gian thực với Amazon Kinesis Dashboard BI và phân tích dự đoán bằng QuickSight và SageMaker Tự động hóa CI/CD và giám sát toàn diện Điểm nhấn chính Kiến trúc EC2 với Auto Scaling \u0026amp; ALB Chuỗi bảo mật đầu-cuối: Route53 → CloudFront → WAF → ALB → Private EC2 Tự động hóa CI/CD với CodePipeline, CodeBuild, CodeDeploy Xử lý thời gian thực bằng Kinesis Data Streams Phân tích dữ liệu với QuickSight Đảm bảo HA \u0026amp; khả năng mở rộng trên nhiều AZ 1. Mục tiêu dự án Mục tiêu chính Xây dựng nền tảng số cho hệ thống Đường Sắt Đô Thị với khả năng mở rộng, bảo mật và vận hành ổn định dài hạn.\nMục tiêu cụ thể Triển khai dịch vụ đặt vé, lập lịch, thanh toán, thông báo Kích hoạt vận hành tự động và giám sát toàn hệ thống Xây dựng pipeline CI/CD với khả năng triển khai không downtime lên EC2 Hỗ trợ thu thập và phân tích dữ liệu thời gian thực Cung cấp kiến trúc đa-AZ với độ sẵn sàng ≥ 99.95% 2. Phạm vi dự án Thành phần Mô tả Region AWS Singapore (ap-southeast-1) Người dùng Hành khách, nhân viên vận hành, quản trị Kiến trúc Multi-tier trên EC2 Giai đoạn 1 Đặt vé, lập lịch, thông báo Giai đoạn 2 Analytics, dashboard BI 3. Kiến trúc AWS đề xuất 3.1 Tổng quan kiến trúc Kiến trúc đa tầng bao gồm:\nTầng Edge: Route53, CloudFront, AWS WAF Tầng Ứng dụng: ALB → EC2 Auto Scaling Group Tầng Dữ liệu: RDS SQL Server, ElastiCache Redis Tầng sự kiện: EventBridge, SNS, SQS Tầng phân tích: Kinesis → S3 → QuickSight → SageMaker Giám sát: CloudWatch, CloudTrail CI/CD: CodePipeline, CodeBuild, CodeDeploy 3.2 Tầng mạng và truy cập Route 53: DNS toàn cầu CloudFront: CDN tăng tốc và cache nội dung AWS WAF: Lọc bảo mật (SQLi, XSS, bot) Application Load Balancer: Điều phối traffic vào EC2 Luồng truy cập:\nNgười dùng → Route53 → CloudFront → WAF → ALB → Private Subnet → EC2\n3.3 Tầng ứng dụng — EC2 Auto Scaling Vì sao chọn EC2? Kiểm soát toàn bộ hệ điều hành và môi trường chạy Phù hợp cho cả monolithic và microservice Ổn định cho backend chạy dài hạn Tự động scaling theo CPU, network, request count EC2 role đảm bảo truy cập an toàn vào tài nguyên AWS Mô hình triển khai ứng dụng Backend chạy trên Auto Scaling Group EC2 đặt trong Private Subnet để tăng bảo mật Triển khai qua CodeDeploy (Blue/Green hoặc Rolling) Các backend service: Booking Service Schedule Service Payment Service Notification Service User Service Operation \u0026amp; Reporting Service 3.4 Tầng dữ liệu Amazon RDS (SQL Server) Lưu vé, lịch, tài khoản người dùng, thanh toán Multi-AZ đảm bảo HA Tự động backup, tự động failover Hỗ trợ IAM authentication \u0026amp; mã hóa dữ liệu Amazon S3 Lưu tài liệu, báo cáo, log, dữ liệu phân tích Lưu dữ liệu stream từ Kinesis Tự động tối ưu chi phí nhờ lifecycle 3.5 Tầng sự kiện \u0026amp; nhắn tin Amazon EventBridge Tự động hóa quy trình, ví dụ:\nPaymentSuccess → CreateInvoice → NotifyUser TrainDelay → PushNotifications → Cập nhật dashboard Amazon SQS Hàng đợi buffer cho workload lớn Ngăn hệ thống quá tải giờ cao điểm Amazon SNS Gửi thông báo đa kênh (SMS, email, push) 3.6 Phân tích thời gian thực Kinesis Data Streams Thu thập dữ liệu hành khách theo thời gian thực Ứng dụng ghi log → Kinesis → S3 QuickSight Dashboard kinh doanh: doanh thu vé, giờ cao điểm, sự cố 3.7 Giám sát \u0026amp; quan sát CloudWatch Metrics: Theo dõi EC2, ALB, RDS CloudWatch Logs: Nhận log từ EC2 SNS Alerts: Cảnh báo sự cố CloudTrail: Giám sát thao tác hệ thống 3.8 CI/CD Pipeline Developer Commit\n→ CodePipeline\n→ CodeBuild\n→ Artifact Storage\n→ CodeDeploy\n→ EC2 Auto Scaling Group\nTính năng triển khai Blue/Green hoặc Rolling Kiểm tra sức khỏe qua ALB Tự động rollback Cập nhật không downtime 4. Kế hoạch triển khai Giai đoạn Thời gian Kết quả bàn giao 1 1 tuần Route53, CloudFront, WAF, VPC, ALB 2 3 tuần EC2 ASG, RDS 3 1 tuần EventBridge, SQS, SNS 4 3 tuần Kinesis, S3, QuickSight 5 2 tuần CI/CD (CodeDeploy) 6 2 tuần Hardening \u0026amp; tối ưu chi phí 5. Ước tính chi phí hàng tháng (Kiến trúc EC2) Dịch vụ Chi phí/tháng CloudFront + Route53 + WAF $24 EC2 ASG (t3.micro x 2 AZ) $38 RDS $19 S3 + Kinesis $1 Monitoring $37 CI/CD $18 Tổng cộng $210 6. Kết quả kỳ vọng Hệ thống đường sắt vận hành ổn định 24/7 Backend EC2 HA với autoscaling Bảo mật thanh toán và endpoint Phân tích dữ liệu hành khách thời gian thực Tối ưu chi phí vận hành Triển khai không downtime + dễ bảo trì Phụ lục A — Danh sách dịch vụ AWS Nhóm Dịch vụ AWS Edge Route53, CloudFront, WAF Networking VPC, ALB Compute EC2, Auto Scaling Group Database RDS SQL Server Event EventBridge, SNS, SQS Analytics Kinesis, S3, QuickSight Monitoring CloudWatch, CloudTrail CI/CD CodePipeline, CodeBuild, CodeDeploy "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.3-s3-secrets-manager/5.3.2-test-gwe/",
	"title": "Kiểm tra Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Tạo S3 bucket Đi đến S3 management console Trong Bucket console, chọn Create bucket Trong Create bucket console Đặt tên bucket: chọn 1 tên mà không bị trùng trong phạm vi toàn cầu (gợi ý: lab\u0026lt;số-lab\u0026gt;\u0026lt;tên-bạn\u0026gt;) Giữ nguyên giá trị của các fields khác (default) Kéo chuột xuống và chọn Create bucket Tạo thành công S3 bucket Kết nối với EC2 bằng session manager Trong workshop này, bạn sẽ dùng AWS Session Manager để kết nối đến các EC2 instances. Session Manager là 1 tính năng trong dịch vụ Systems Manager được quản lý hoàn toàn bởi AWS. System manager cho phép bạn quản lý Amazon EC2 instances và các máy ảo on-premises (VMs)thông qua 1 browser-based shell. Session Manager cung cấp khả năng quản lý phiên bản an toàn và có thể kiểm tra mà không cần mở cổng vào, duy trì máy chủ bastion host hoặc quản lý khóa SSH.\nFirst cloud journey Lab để hiểu sâu hơn về Session manager.\nTrong AWS Management Console, gõ Systems Manager trong ô tìm kiếm và nhấn Enter: Từ Systems Manager menu, tìm Node Management ở thanh bên trái và chọn Session Manager: Click Start Session, và chọn EC2 instance tên Test-Gateway-Endpoint. Phiên bản EC2 này đã chạy trong \u0026ldquo;VPC cloud\u0026rdquo; và sẽ được dùng để kiểm tra khả năng kết nối với Amazon S3 thông qua điểm cuối Cổng mà bạn vừa tạo (s3-gwe).\nSession Manager sẽ mở browser tab mới với shell prompt: sh-4.2 $\nBạn đã bắt đầu phiên kết nối đến EC2 trong VPC Cloud thành công. Trong bước tiếp theo, chúng ta sẽ tạo một S3 bucket và một tệp trong đó.\nCreate a file and upload to s3 bucket Đổi về ssm-user\u0026rsquo;s thư mục bằng lệnh \u0026ldquo;cd ~\u0026rdquo; Tạo 1 file để kiểm tra bằng lệnh \u0026ldquo;fallocate -l 1G testfile.xyz\u0026rdquo;, 1 file tên \u0026ldquo;testfile.xyz\u0026rdquo; có kích thước 1GB sẽ được tạo. Tải file mình vừa tạo lên S3 với lệnh \u0026ldquo;aws s3 cp testfile.xyz s3://your-bucket-name\u0026rdquo;. Thay your-bucket-name bằng tên S3 bạn đã tạo. Bạn đã tải thành công tệp lên bộ chứa S3 của mình. Bây giờ bạn có thể kết thúc session.\nKiểm tra object trong S3 bucket Đi đến S3 console. Click tên s3 bucket của bạn Trong Bucket console, bạn sẽ thấy tệp bạn đã tải lên S3 bucket của mình Tóm tắt Chúc mừng bạn đã hoàn thành truy cập S3 từ VPC. Trong phần này, bạn đã tạo gateway endpoint cho Amazon S3 và sử dụng AWS CLI để tải file lên. Quá trình tải lên hoạt động vì gateway endpoint cho phép giao tiếp với S3 mà không cần Internet gateway gắn vào \u0026ldquo;VPC Cloud\u0026rdquo;. Điều này thể hiện chức năng của gateway endpoint như một đường dẫn an toàn đến S3 mà không cần đi qua pub lic Internet.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Tạo một S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "Trong phần này, bạn sẽ tạo và kiểm tra Interface Endpoint S3 bằng cách sử dụng môi trường truyền thống mô phỏng.\nQuay lại Amazon VPC menu. Trong thanh điều hướng bên trái, chọn Endpoints, sau đó click Create Endpoint.\nTrong Create endpoint console:\nĐặt tên interface endpoint Trong Service category, chọn aws services Trong Search box, gõ S3 và nhấn Enter. Chọn endpoint có tên com.amazonaws.us-east-1.s3. Đảm bảo rằng cột Type có giá trị Interface. Đối với VPC, chọn VPC Cloud từ drop-down. Đảm bảo rằng bạn chọn \u0026ldquo;VPC Cloud\u0026rdquo; và không phải \u0026ldquo;VPC On-prem\u0026rdquo;\nMở rộng Additional settings và đảm bảo rằng Enable DNS name không được chọn (sẽ sử dụng điều này trong phần tiếp theo của workshop) Chọn 2 subnets trong AZs sau: us-east-1a and us-east-1b Đối với Security group, chọn SGforS3Endpoint: Giữ default policy - full access và click Create endpoint Chúc mừng bạn đã tạo thành công S3 interface endpoint. Ở bước tiếp theo, chúng ta sẽ kiểm tra interface endpoint.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.12-week12/",
	"title": "Tuần 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 12 Hiểu và thực hành AWS QuickSight và AWS Athena. Xây dựng data lake từ nhiều nguồn dữ liệu. Triển khai ứng dụng web với Elastic Beanstalk và pipeline CDK CI/CD. Nhiệm vụ trong tuần Ngày Nhiệm vụ Ngày bắt đầu Ngày kết thúc Tài liệu tham khảo 2 Học AWS QuickSight:\n+ Tổng quan QuickSight + Kiến trúc + Thành phần + SPICE 25/11/2025 26/11/2025 https://000073.awsstudygroup.com/ 3 Thực hành với AWS QuickSight 26/11/2025 27/11/2025 https://000073.awsstudygroup.com/ 4 Học và thực hành AWS Athena 27/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Thực hành xây dựng data lake 29/11/2025 30/11/2025 https://000070.awsstudygroup.com/ 6 Thực hành triển khai web app với Elastic Beanstalk và CDK pipeline 01/12/2025 02/12/2025 https://000113.awsstudygroup.com/ Thành tích Tuần 12 AWS QuickSight\nHiểu tổng quan, kiến trúc và các thành phần chính (Dashboard, Analysis, Dataset). Hiểu cách hoạt động của SPICE engine giúp tăng tốc truy vấn. Thực hành tạo dashboard cơ bản và kết nối dữ liệu từ S3/Athena. AWS Athena\nHọc cách truy vấn dữ liệu trên S3 bằng SQL. Thực hành tạo bảng, view và chạy truy vấn mẫu. Data Lake\nXây dựng thử nghiệm data lake trên S3 để tổng hợp nhiều nguồn dữ liệu. Làm sạch và chuyển đổi dữ liệu trước khi phân tích. Elastic Beanstalk \u0026amp; CDK\nTriển khai demo web app trên Elastic Beanstalk. Cấu hình pipeline CDK cơ bản để tự động triển khai code. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.13-week13/",
	"title": "Tuần 13 Worklog",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 12 Trong tuần này, mục tiêu chính là học và thực hành các dịch vụ và công cụ liên quan đến việc di chuyển hạ tầng (Migration), phục hồi thảm họa (Disaster Recovery), và chuyển đổi cơ sở dữ liệu (Database Conversion). Cụ thể gồm:\nTìm hiểu và thực hành AWS VM Import/Export. Nghiên cứu và sử dụng AWS Database Migration Service (DMS) và AWS Schema Conversion Tool (SCT). Tìm hiểu và thực hành AWS Elastic Disaster Recovery (DRS). Nhiệm vụ trong tuần Day Task Start Date Completion Date Reference Material 2 Learn AWS VM Import/Export 02/12/2025 03/12/2025 https://000014.awsstudygroup.com/ 3 Practice AWS VM Import/Export 03/12/2025 04/12/2025 https://000014.awsstudygroup.com/ 4 Learn and Practice AWS Database Migration Service (DMS) and Schema Conversion Tool (SCT) 04/12/2025 05/12/2025 https://000043.awsstudygroup.com/ 5 Learn AWS Elastic Disaster Recovery 05/12/2025 06/12/2025 https://000100.awsstudygroup.com/ 6 Practice AWS Elastic Disaster Recovery 06/12/2025 07/12/2025 https://000113.awsstudygroup.com/ Thành tích Tuần 13 1. AWS VM Import/Export Hiểu quy trình import máy ảo (OVA/VMDK) lên AWS. Tạo IAM role vmimport, cấu hình S3 bucket, và thực hiện import AMI thành công. Thực hành export AMI về lại dạng OVA cho môi trường on-prem. 2. AWS Database Migration Service + Schema Conversion Tool Dùng SCT để phân tích và chuyển đổi schema (vd: SQL Server → PostgreSQL). Xử lý các phần không tương thích do SCT báo cáo. Thiết lập DMS: replication instance + source/target endpoints. Thực hiện full load + CDC (Change Data Capture) để đồng bộ dữ liệu realtime. 3. AWS Elastic Disaster Recovery Nắm kiến trúc đầy đủ: replication agent, replication server, staging area subnet. Cài đặt DRS Agent trên máy nguồn và replicate dữ liệu sang AWS. Thực hiện test failover → launch server trên AWS → đánh giá RTO/RPO. Khôi phục lại (failback) thành công. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.10-week10/",
	"title": "Worklog tuần 10",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 10 Thực hành với các dịch vụ cơ bản của AWS (VPC, EC2, EBS, Postgres) Học AWS SageMaker cho workflow AI/ML Thực hành phân tích dữ liệu AI/ML với SageMaker Học các thành phần AWS Bedrock AgentCore Học AWS Glue và Amazon Athena cho ETL và phân tích dữ liệu Nhiệm vụ tuần này Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Thực hành: + Tạo VPCs + Khởi tạo EC2 instances + Tạo EBS volume + Gắn EBS volume + Cài đặt Postgres + Mount EBS trên EC2 production + Backup Postgres + Mount EBS trên EC2 test + Phục hồi dữ liệu 08/11/2025 10/11/2025 https://100000.awsstudygroup.com/ 3 Học AWS SageMaker AI: + SageMaker là gì + Các thành phần + End-to-End Workflow 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 4 Thực hành: Sử dụng SageMaker AI để phân tích dữ liệu, nhập file Excel, chọn kiểu dữ liệu, xuất kết quả bao gồm biểu đồ 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 5 Học AWS Bedrock AgentCore: + Identity, Memory, Code Interpreter, Browser, Gateway, Observability + Trường hợp sử dụng: trang bị công cụ cho agent, triển khai an toàn ở quy mô lớn, kiểm tra và giám sát agent 12/11/2025 14/11/2025 https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html 6 Học AWS Glue và Amazon Athena 14/11/2025 16/11/2025 https://000040.awsstudygroup.com/ Thành tựu Tuần 10 1. Thực hành các dịch vụ cơ bản của AWS: Tạo VPCs với subnet, route table, Internet Gateway đầy đủ. Khởi tạo EC2 instances cho môi trường production và test. Tạo và gắn EBS volume cho EC2 instances. Cài đặt và cấu hình PostgreSQL trên EC2. Mount EBS trên cả production và test instances. Thực hiện backup và phục hồi PostgreSQL thành công. 2. Học \u0026amp; Thực hành AWS SageMaker AI: Hiểu các thành phần SageMaker: Notebook, Training, Endpoint, Model, Pipelines. Thực hiện workflow AI/ML từ đầu đến cuối: nhập dữ liệu Excel, phân tích dữ liệu, trực quan hóa biểu đồ. Chọn kiểu dữ liệu phù hợp, làm sạch dữ liệu và xuất kết quả cho các bước tiếp theo. 3. Học AWS Bedrock AgentCore: Hiểu các thành phần cốt lõi: Identity, Memory, Code Interpreter, Browser, Gateway, Observability. Khám phá các trường hợp sử dụng: trang bị công cụ cho agent, triển khai an toàn, giám sát và kiểm tra agent. 4. Học AWS Glue \u0026amp; Amazon Athena: Thực hành xây dựng pipeline ETL với Glue. Catalog dữ liệu và chuyển đổi dữ liệu để phân tích. Truy vấn dữ liệu trên S3 với Athena và kiểm tra kết quả. Kết nối kết quả truy vấn Athena với QuickSight để báo cáo và trực quan hóa. 5. Thành thạo AWS CLI \u0026amp; Management Console: Cấu hình AWS CLI với Access Key, Secret Key, và Default Region. Sử dụng CLI để lấy danh sách region, liệt kê dịch vụ, và kiểm tra cấu hình. Quản lý tài nguyên AWS bằng CLI và Console linh hoạt. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.2-prerequiste/",
	"title": "Yêu cầu tiên quyết",
	"tags": [],
	"description": "",
	"content": "3.1 Tài khoản AWS, Region \u0026amp; IAM Region + Chọn region để triển khai (ap-southeast-1) Cấu hình IAM cơ bản + Nhóm Infra/Admin: quyền Administrator, nên tách riêng theo từng môi trường (dev/stg/prod).\n+ Stakeholders: quyền ReadOnlyAccess\n+ CI/CD (service roles): CodePipeline, CodeBuild, CodeDeploy\n+ EC2 instance profile (quyền chạy runtime): đọc secrets từ Secrets Manager theo prefix (ví dụ: metro//*), đẩy log/metrics vào CloudWatch, quyền truy cập S3/Kinesis/SNS cần thiết cho chức năng ứng dụng\nGhi chú bảo mật + Không cấp AdministratorAccess cho EC2 Role\n+ Áp dụng nguyên tắc least privilege: phân quyền theo ARN và prefix rõ ràng\n+ Bật MFA cho các IAM User/Role quan trọng\n+ Kích hoạt baseline guardrails (Control Tower / Security Hub nếu có)\nVí dụ chính sách IAM Dưới đây là ví dụ policy dùng cho vai trò vận hành hạ tầng (Infrastructure role):\n{ \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;VisualEditor0\u0026rdquo;, \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: [ \u0026ldquo;cloudformation:\u0026rdquo;, \u0026ldquo;cloudwatch:\u0026rdquo;, \u0026ldquo;ec2:AcceptTransitGatewayPeeringAttachment\u0026rdquo;, \u0026ldquo;ec2:AcceptTransitGatewayVpcAttachment\u0026rdquo;, \u0026ldquo;ec2:AllocateAddress\u0026rdquo;, \u0026ldquo;ec2:AssociateAddress\u0026rdquo;, \u0026ldquo;ec2:AssociateIamInstanceProfile\u0026rdquo;, \u0026ldquo;ec2:AssociateRouteTable\u0026rdquo;, \u0026ldquo;ec2:AssociateSubnetCidrBlock\u0026rdquo;, \u0026ldquo;ec2:AssociateTransitGatewayRouteTable\u0026rdquo;, \u0026ldquo;ec2:AssociateVpcCidrBlock\u0026rdquo;, \u0026ldquo;ec2:AttachInternetGateway\u0026rdquo;, \u0026ldquo;ec2:AttachNetworkInterface\u0026rdquo;, \u0026ldquo;ec2:AttachVolume\u0026rdquo;, \u0026ldquo;ec2:AttachVpnGateway\u0026rdquo;, \u0026ldquo;ec2:AuthorizeSecurityGroupEgress\u0026rdquo;, \u0026ldquo;ec2:AuthorizeSecurityGroupIngress\u0026rdquo;, \u0026ldquo;ec2:CreateClientVpnEndpoint\u0026rdquo;, \u0026ldquo;ec2:CreateClientVpnRoute\u0026rdquo;, \u0026ldquo;ec2:CreateCustomerGateway\u0026rdquo;, \u0026ldquo;ec2:CreateDhcpOptions\u0026rdquo;, \u0026ldquo;ec2:CreateFlowLogs\u0026rdquo;, \u0026ldquo;ec2:CreateInternetGateway\u0026rdquo;, \u0026ldquo;ec2:CreateLaunchTemplate\u0026rdquo;, \u0026ldquo;ec2:CreateNetworkAcl\u0026rdquo;, \u0026ldquo;ec2:CreateNetworkInterface\u0026rdquo;, \u0026ldquo;ec2:CreateNetworkInterfacePermission\u0026rdquo;, \u0026ldquo;ec2:CreateRoute\u0026rdquo;, \u0026ldquo;ec2:CreateRouteTable\u0026rdquo;, \u0026ldquo;ec2:CreateSecurityGroup\u0026rdquo;, \u0026ldquo;ec2:CreateSubnet\u0026rdquo;, \u0026ldquo;ec2:CreateSubnetCidrReservation\u0026rdquo;, \u0026ldquo;ec2:CreateTags\u0026rdquo;, \u0026ldquo;ec2:CreateTransitGateway\u0026rdquo;, \u0026ldquo;ec2:CreateTransitGatewayPeeringAttachment\u0026rdquo;, \u0026ldquo;ec2:CreateTransitGatewayPrefixListReference\u0026rdquo;, \u0026ldquo;ec2:CreateTransitGatewayRoute\u0026rdquo;, \u0026ldquo;ec2:CreateTransitGatewayRouteTable\u0026rdquo;, \u0026ldquo;ec2:CreateTransitGatewayVpcAttachment\u0026rdquo;, \u0026ldquo;ec2:CreateVpc\u0026rdquo;, \u0026ldquo;ec2:CreateVpcEndpoint\u0026rdquo;, \u0026ldquo;ec2:CreateVpcEndpointConnectionNotification\u0026rdquo;, \u0026ldquo;ec2:CreateVpcEndpointServiceConfiguration\u0026rdquo;, \u0026ldquo;ec2:CreateVpnConnection\u0026rdquo;, \u0026ldquo;ec2:CreateVpnConnectionRoute\u0026rdquo;, \u0026ldquo;ec2:CreateVpnGateway\u0026rdquo;, \u0026ldquo;ec2:DeleteCustomerGateway\u0026rdquo;, \u0026ldquo;ec2:DeleteFlowLogs\u0026rdquo;, \u0026ldquo;ec2:DeleteInternetGateway\u0026rdquo;, \u0026ldquo;ec2:DeleteNetworkInterface\u0026rdquo;, \u0026ldquo;ec2:DeleteNetworkInterfacePermission\u0026rdquo;, \u0026ldquo;ec2:DeleteRoute\u0026rdquo;, \u0026ldquo;ec2:DeleteRouteTable\u0026rdquo;, \u0026ldquo;ec2:DeleteSecurityGroup\u0026rdquo;, \u0026ldquo;ec2:DeleteSubnet\u0026rdquo;, \u0026ldquo;ec2:DeleteSubnetCidrReservation\u0026rdquo;, \u0026ldquo;ec2:DeleteTags\u0026rdquo;, \u0026ldquo;ec2:DeleteTransitGateway\u0026rdquo;, \u0026ldquo;ec2:DeleteTransitGatewayPeeringAttachment\u0026rdquo;, \u0026ldquo;ec2:DeleteTransitGatewayPrefixListReference\u0026rdquo;, \u0026ldquo;ec2:DeleteTransitGatewayRoute\u0026rdquo;, \u0026ldquo;ec2:DeleteTransitGatewayRouteTable\u0026rdquo;, \u0026ldquo;ec2:DeleteTransitGatewayVpcAttachment\u0026rdquo;, \u0026ldquo;ec2:DeleteVpc\u0026rdquo;, \u0026ldquo;ec2:DeleteVpcEndpoints\u0026rdquo;, \u0026ldquo;ec2:DeleteVpcEndpointServiceConfigurations\u0026rdquo;, \u0026ldquo;ec2:DeleteVpnConnection\u0026rdquo;, \u0026ldquo;ec2:DeleteVpnConnectionRoute\u0026rdquo;, \u0026ldquo;ec2:Describe*\u0026rdquo;, \u0026ldquo;ec2:DetachInternetGateway\u0026rdquo;, \u0026ldquo;ec2:DisassociateAddress\u0026rdquo;, \u0026ldquo;ec2:DisassociateRouteTable\u0026rdquo;, \u0026ldquo;ec2:GetLaunchTemplateData\u0026rdquo;, \u0026ldquo;ec2:GetTransitGatewayAttachmentPropagations\u0026rdquo;, \u0026ldquo;ec2:ModifyInstanceAttribute\u0026rdquo;, \u0026ldquo;ec2:ModifySecurityGroupRules\u0026rdquo;, \u0026ldquo;ec2:ModifyTransitGatewayVpcAttachment\u0026rdquo;, \u0026ldquo;ec2:ModifyVpcAttribute\u0026rdquo;, \u0026ldquo;ec2:ModifyVpcEndpoint\u0026rdquo;, \u0026ldquo;ec2:ReleaseAddress\u0026rdquo;, \u0026ldquo;ec2:ReplaceRoute\u0026rdquo;, \u0026ldquo;ec2:RevokeSecurityGroupEgress\u0026rdquo;, \u0026ldquo;ec2:RevokeSecurityGroupIngress\u0026rdquo;, \u0026ldquo;ec2:RunInstances\u0026rdquo;, \u0026ldquo;ec2:StartInstances\u0026rdquo;, \u0026ldquo;ec2:StopInstances\u0026rdquo;, \u0026ldquo;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026rdquo;, \u0026ldquo;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026rdquo;, \u0026ldquo;iam:AddRoleToInstanceProfile\u0026rdquo;, \u0026ldquo;iam:AttachRolePolicy\u0026rdquo;, \u0026ldquo;iam:CreateInstanceProfile\u0026rdquo;, \u0026ldquo;iam:CreatePolicy\u0026rdquo;, \u0026ldquo;iam:CreateRole\u0026rdquo;, \u0026ldquo;iam:DeleteInstanceProfile\u0026rdquo;, \u0026ldquo;iam:DeletePolicy\u0026rdquo;, \u0026ldquo;iam:DeleteRole\u0026rdquo;, \u0026ldquo;iam:DeleteRolePolicy\u0026rdquo;, \u0026ldquo;iam:DetachRolePolicy\u0026rdquo;, \u0026ldquo;iam:GetInstanceProfile\u0026rdquo;, \u0026ldquo;iam:GetPolicy\u0026rdquo;, \u0026ldquo;iam:GetRole\u0026rdquo;, \u0026ldquo;iam:GetRolePolicy\u0026rdquo;, \u0026ldquo;iam:ListPolicyVersions\u0026rdquo;, \u0026ldquo;iam:ListRoles\u0026rdquo;, \u0026ldquo;iam:PassRole\u0026rdquo;, \u0026ldquo;iam:PutRolePolicy\u0026rdquo;, \u0026ldquo;iam:RemoveRoleFromInstanceProfile\u0026rdquo;, \u0026ldquo;lambda:CreateFunction\u0026rdquo;, \u0026ldquo;lambda:DeleteFunction\u0026rdquo;, \u0026ldquo;lambda:DeleteLayerVersion\u0026rdquo;, \u0026ldquo;lambda:GetFunction\u0026rdquo;, \u0026ldquo;lambda:GetLayerVersion\u0026rdquo;, \u0026ldquo;lambda:InvokeFunction\u0026rdquo;, \u0026ldquo;lambda:PublishLayerVersion\u0026rdquo;, \u0026ldquo;logs:CreateLogGroup\u0026rdquo;, \u0026ldquo;logs:DeleteLogGroup\u0026rdquo;, \u0026ldquo;logs:DescribeLogGroups\u0026rdquo;, \u0026ldquo;logs:PutRetentionPolicy\u0026rdquo;, \u0026ldquo;route53:ChangeTagsForResource\u0026rdquo;, \u0026ldquo;route53:CreateHealthCheck\u0026rdquo;, \u0026ldquo;route53:CreateHostedZone\u0026rdquo;, \u0026ldquo;route53:CreateTrafficPolicy\u0026rdquo;, \u0026ldquo;route53:DeleteHostedZone\u0026rdquo;, \u0026ldquo;route53:DisassociateVPCFromHostedZone\u0026rdquo;, \u0026ldquo;route53:GetHostedZone\u0026rdquo;, \u0026ldquo;route53:ListHostedZones\u0026rdquo;, \u0026ldquo;route53domains:ListDomains\u0026rdquo;, \u0026ldquo;route53domains:ListOperations\u0026rdquo;, \u0026ldquo;route53domains:ListTagsForDomain\u0026rdquo;, \u0026ldquo;route53resolver:AssociateResolverEndpointIpAddress\u0026rdquo;, \u0026ldquo;route53resolver:AssociateResolverRule\u0026rdquo;, \u0026ldquo;route53resolver:CreateResolverEndpoint\u0026rdquo;, \u0026ldquo;route53resolver:CreateResolverRule\u0026rdquo;, \u0026ldquo;route53resolver:DeleteResolverEndpoint\u0026rdquo;, \u0026ldquo;route53resolver:DeleteResolverRule\u0026rdquo;, \u0026ldquo;route53resolver:DisassociateResolverEndpointIpAddress\u0026rdquo;, \u0026ldquo;route53resolver:DisassociateResolverRule\u0026rdquo;, \u0026ldquo;route53resolver:GetResolverEndpoint\u0026rdquo;, \u0026ldquo;route53resolver:GetResolverRule\u0026rdquo;, \u0026ldquo;route53resolver:ListResolverEndpointIpAddresses\u0026rdquo;, \u0026ldquo;route53resolver:ListResolverEndpoints\u0026rdquo;, \u0026ldquo;route53resolver:ListResolverRuleAssociations\u0026rdquo;, \u0026ldquo;route53resolver:ListResolverRules\u0026rdquo;, \u0026ldquo;route53resolver:ListTagsForResource\u0026rdquo;, \u0026ldquo;route53resolver:UpdateResolverEndpoint\u0026rdquo;, \u0026ldquo;route53resolver:UpdateResolverRule\u0026rdquo;, \u0026ldquo;s3:AbortMultipartUpload\u0026rdquo;, \u0026ldquo;s3:CreateBucket\u0026rdquo;, \u0026ldquo;s3:DeleteBucket\u0026rdquo;, \u0026ldquo;s3:DeleteObject\u0026rdquo;, \u0026ldquo;s3:GetAccountPublicAccessBlock\u0026rdquo;, \u0026ldquo;s3:GetBucketAcl\u0026rdquo;, \u0026ldquo;s3:GetBucketOwnershipControls\u0026rdquo;, \u0026ldquo;s3:GetBucketPolicy\u0026rdquo;, \u0026ldquo;s3:GetBucketPolicyStatus\u0026rdquo;, \u0026ldquo;s3:GetBucketPublicAccessBlock\u0026rdquo;, \u0026ldquo;s3:GetObject\u0026rdquo;, \u0026ldquo;s3:GetObjectVersion\u0026rdquo;, \u0026ldquo;s3:GetBucketVersioning\u0026rdquo;, \u0026ldquo;s3:ListAccessPoints\u0026rdquo;, \u0026ldquo;s3:ListAccessPointsForObjectLambda\u0026rdquo;, \u0026ldquo;s3:ListAllMyBuckets\u0026rdquo;, \u0026ldquo;s3:ListBucket\u0026rdquo;, \u0026ldquo;s3:ListBucketMultipartUploads\u0026rdquo;, \u0026ldquo;s3:ListBucketVersions\u0026rdquo;, \u0026ldquo;s3:ListJobs\u0026rdquo;, \u0026ldquo;s3:ListMultipartUploadParts\u0026rdquo;, \u0026ldquo;s3:ListMultiRegionAccessPoints\u0026rdquo;, \u0026ldquo;s3:ListStorageLensConfigurations\u0026rdquo;, \u0026ldquo;s3:PutAccountPublicAccessBlock\u0026rdquo;, \u0026ldquo;s3:PutBucketAcl\u0026rdquo;, \u0026ldquo;s3:PutBucketPolicy\u0026rdquo;, \u0026ldquo;s3:PutBucketPublicAccessBlock\u0026rdquo;, \u0026ldquo;s3:PutObject\u0026rdquo;, \u0026ldquo;secretsmanager:CreateSecret\u0026rdquo;, \u0026ldquo;secretsmanager:DeleteSecret\u0026rdquo;, \u0026ldquo;secretsmanager:DescribeSecret\u0026rdquo;, \u0026ldquo;secretsmanager:GetSecretValue\u0026rdquo;, \u0026ldquo;secretsmanager:ListSecrets\u0026rdquo;, \u0026ldquo;secretsmanager:ListSecretVersionIds\u0026rdquo;, \u0026ldquo;secretsmanager:PutResourcePolicy\u0026rdquo;, \u0026ldquo;secretsmanager:TagResource\u0026rdquo;, \u0026ldquo;secretsmanager:UpdateSecret\u0026rdquo;, \u0026ldquo;sns:ListTopics\u0026rdquo;, \u0026ldquo;ssm:DescribeInstanceProperties\u0026rdquo;, \u0026ldquo;ssm:DescribeSessions\u0026rdquo;, \u0026ldquo;ssm:GetConnectionStatus\u0026rdquo;, \u0026ldquo;ssm:GetParameters\u0026rdquo;, \u0026ldquo;ssm:ListAssociations\u0026rdquo;, \u0026ldquo;ssm:ResumeSession\u0026rdquo;, \u0026ldquo;ssm:StartSession\u0026rdquo;, \u0026ldquo;ssm:TerminateSession\u0026rdquo; ], \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;*\u0026rdquo; } ] }\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/",
	"title": "Các bài blogs đã dịch",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTại đây sẽ là phần liệt kê, giới thiệu các blogs mà các bạn đã dịch. Ví dụ:\nBlog 1 - Các dịch vụ AWS vươn lên tầm cao mới trong Prime Day 2025: các số liệu và cột mốc chính Prime Day 2025 lập kỷ lục mới nhờ sức mạnh công nghệ của AWS. Các trung tâm hoàn tất đơn hàng với hơn 7.000 robot ASRS được vận hành bởi AWS Outposts, xử lý hơn 524 triệu lệnh và đạt đỉnh 8 triệu lệnh/giờ (tăng 160% so với 2024). Nhờ hạ tầng AWS kết hợp kinh nghiệm bán lẻ của Amazon, khách hàng dễ dàng tìm ưu đãi, nhận thông tin sản phẩm nhanh chóng và được giao hàng trong ngày hoặc hôm sau.\nBlog 2 - AWS được công nhận là Nhà lãnh đạo trong báo cáo Gartner Magic Quadrant 2025 về Nền tảng Ứng dụng Cloud-Native và Quản lý Container Trong kỷ nguyên điện toán đám mây và trí tuệ nhân tạo bùng nổ, doanh nghiệp liên tục tìm kiếm nền tảng công nghệ giúp họ đổi mới nhanh hơn, vận hành linh hoạt hơn và tối ưu chi phí tốt hơn. Với hệ sinh thái dịch vụ toàn diện, AWS không chỉ đồng hành cùng hàng triệu khách hàng toàn cầu trong hành trình chuyển đổi số, mà còn tiếp tục khẳng định vị thế tiên phong khi được Gartner công nhận là Nhà lãnh đạo (Leader) trong nhiều hạng mục quan trọng của Magic Quadrant 2025.\nBlog 3 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 4 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 5 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 6 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Kiểm tra Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Lấy regional DNS name (tên DNS khu vực) của S3 interface endpoint Trong Amazon VPC menu, chọn Endpoints.\nClick tên của endpoint chúng ta mới tạo ở mục 4.2: s3-interface-endpoint. Click details và lưu lại regional DNS name của endpoint (cái đầu tiên) vào text-editor của bạn để dùng ở các bước sau.\nKết nối đến EC2 instance ở trong \u0026ldquo;VPC On-prem\u0026rdquo; (giả lập môi trường truyền thống) Đi đến Session manager bằng cách gõ \u0026ldquo;session manager\u0026rdquo; vào ô tìm kiếm\nClick Start Session, chọn EC2 instance có tên Test-Interface-Endpoint. EC2 instance này đang chạy trên \u0026ldquo;VPC On-prem\u0026rdquo; và sẽ được sử dụng để kiểm tra kết nối đến Amazon S3 thông qua Interface endpoint. Session Manager sẽ mở 1 browser tab mới với shell prompt: sh-4.2 $\nĐi đến ssm-user\u0026rsquo;s home directory với lệnh \u0026ldquo;cd ~\u0026rdquo;\nTạo 1 file tên testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file vào S3 bucket mình tạo ở section 4.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; Câu lệnh này yêu cầu thông số \u0026ndash;endpoint-url, bởi vì bạn cần sử dụng DNS name chỉ định cho endpoint để truy cập vào S3 thông qua Interface endpoint. Không lấy \u0026rsquo; * \u0026rsquo; khi copy/paste tên DNS khu vực. Cung cấp tên S3 bucket của bạn Bây giờ tệp đã được thêm vào bộ chứa S3 của bạn. Hãy kiểm tra bộ chứa S3 của bạn trong bước tiếp theo.\nKiểm tra Object trong S3 bucket Đi đến S3 console Click Buckets Click tên bucket của bạn và bạn sẽ thấy testfile2.xyz đã được thêm vào s3 bucket của bạn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.3-s3-secrets-manager/",
	"title": "Tạo Secret Manager",
	"tags": [],
	"description": "",
	"content": "Sử dụng AWS Secrets Manager Trong phần này, bạn sẽ tạo Secrets Manager để lưu một secret. Secrets Manager sẽ lưu các key bí mật từ dự án của bạn như username và password của cơ sở dữ liệu, jwt secret key, và VNPAY key. Điều này hữu ích để lưu trữ trên AWS mà không cần lo lắng về việc các bí mật này bị lộ.\nNội dung Tạo gateway endpoint Kiểm tra gateway endpoint "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/4-eventparticipated/4.3-event3/",
	"title": "Sự kiện 3",
	"tags": [],
	"description": "",
	"content": "Bài thu hoạch: AWS Cloud Mastery Series #2 – DevOps on AWS Mục tiêu sự kiện Hiểu rõ các nguyên tắc cốt lõi của văn hóa DevOps và các chỉ số hiệu suất chính (DORA). Làm chủ hệ sinh thái DevOps của AWS để xây dựng pipeline CI/CD hoàn chỉnh (end-to-end). So sánh và lựa chọn công cụ Infrastructure as Code (IaC) phù hợp (CloudFormation vs. AWS CDK). Khám phá các chiến lược container hóa sử dụng Amazon ECS, EKS và App Runner. Triển khai khả năng quan sát (Observability) toàn diện sử dụng CloudWatch và AWS X-Ray. Thời gian \u0026amp; Địa điểm Thời gian: 08:30 – 17:00, Thứ Hai, 17 tháng 11\nĐịa điểm: Tầng 26, Tòa nhà tài chính Bitexco, 2 Đ. Hải Triều, Bến Nghé, Quận 1, TP. Hồ Chí Minh\nDiễn giả Truong Quang Tinh - DevOps Engineer, TymeX | AWS Community Builder Kha Van - Cloud Security Engineer | AWS Community Builders Bao Huynh – AWS Community Builder Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Nội dung nổi bật Văn hóa DevOps \u0026amp; Các chỉ số đo lường Chuyển dịch văn hóa: Chuyển từ việc phát triển (Dev) và vận hành (Ops) riêng biệt sang một văn hóa thống nhất với trách nhiệm được chia sẻ. Các chỉ số chính (DORA): Tập trung vào 4 chỉ số quan trọng để đo lường hiệu suất: Tần suất triển khai (Deployment Frequency - DF) Thời gian hoàn thành thay đổi (Lead Time for Changes - LT) Thời gian trung bình để khôi phục (Mean Time to Restore - MTTR) Tỷ lệ thay đổi thất bại (Change Failure Rate - CFR) Dịch vụ AWS DevOps – CI/CD Pipeline Quản lý mã nguồn (Source Control): Các chiến lược quản lý Git (GitFlow vs. Trunk-based development) sử dụng AWS CodeCommit. Điều phối (Orchestration): Tự động hóa quy trình phát hành với AWS CodePipeline. Chiến lược triển khai (Deployment Strategies): Blue/Green: Giảm thiểu thời gian chết (downtime) và rủi ro bằng cách chạy hai môi trường song song. Canary: Triển khai thay đổi cho một nhóm nhỏ người dùng trước. Rolling: Cập nhật các instances dần dần theo từng đợt. Quản lý hạ tầng bằng mã (IaC) AWS CloudFormation: Phương pháp khai báo (declarative) sử dụng JSON/YAML template để định nghĩa hạ tầng; sử dụng tính năng phát hiện sai lệch (drift detection) để duy trì tính nhất quán. AWS CDK (Cloud Development Kit): Phương pháp mệnh lệnh (imperative) cho phép lập trình viên định nghĩa tài nguyên đám mây bằng ngôn ngữ lập trình quen thuộc (TypeScript, Python, Java). Các cấu trúc (constructs) của CDK cho phép tái sử dụng các mẫu hạ tầng. Dịch vụ Container \u0026amp; Khả năng quan sát (Observability) Sự tiến hóa của tài nguyên tính toán: Tiêu chí lựa chọn giữa Amazon ECS (kiểm soát chặt chẽ), Amazon EKS (chuẩn Kubernetes), và AWS App Runner (đơn giản hóa cho lập trình viên). Observability: Vượt xa việc giám sát đơn thuần bằng cách tích hợp CloudWatch (Logs, Metrics, Alarms) với AWS X-Ray để truy vết phân tán (distributed tracing), cho phép phân tích nguyên nhân gốc rễ trong microservices. Bài học đúc kết 1. Tư duy thiết kế Tự động hóa là ưu tiên: Các quy trình thủ công rất dễ gây lỗi; mọi thứ từ kiểm thử đến cung cấp hạ tầng đều cần được tự động hóa. Bảo mật dịch chuyển sang trái (Shift-Left Security): Tích hợp kiểm tra bảo mật ngay từ đầu pipeline CI/CD thay vì đợi đến cuối. Đo lường mọi thứ: Bạn không thể cải thiện những gì bạn không đo lường. DORA metrics là kim chỉ nam cho sự trưởng thành của DevOps. 2. Kiến trúc kỹ thuật Hạ tầng bất biến (Immutable Infrastructure): Máy chủ không bao giờ được sửa đổi sau khi triển khai; chúng sẽ được thay thế hoàn toàn bằng máy chủ mới. Điều phối Pipeline: Một pipeline mạnh mẽ phải bao gồm các bước build, unit test, integration test và các cổng phê duyệt (approval gates) trước khi lên production. Khả năng truy vết (Traceability): Trong hệ thống phân tán, việc tương quan giữa logs và traces (thông qua X-Ray) là bắt buộc để debug. 3. Chiến lược hiện đại hóa Lựa chọn quy mô phù hợp (Right-sizing): Sử dụng App Runner cho các web app đơn giản để giảm chi phí vận hành, và dành EKS cho các hệ thống microservices phức tạp cần điều phối cao. Áp dụng IaC: Chuyển từ việc thao tác tay trên console sang định nghĩa hạ tầng bằng code để đảm bảo khả năng tái tạo và phục hồi sau thảm họa. Ứng dụng vào công việc Áp dụng Trunk-based Development: Tinh gọn quy trình Git trong dự án nhóm để giảm xung đột khi merge code và tăng tốc độ tích hợp. Triển khai CI/CD: Xây dựng pipeline sử dụng AWS CodePipeline để tự động deploy backend quản lý sinh viên ngay khi push code lên nhánh chính. Chuyển đổi sang IaC: Refactor lại các thiết lập thủ công hiện tại của DynamoDB và Lambda functions thành các script AWS CDK để dễ bảo trì hơn. Tăng cường giám sát: Thiết lập CloudWatch Alarms cho các lỗi API nghiêm trọng (mã lỗi 5xx) để chủ động phát hiện sự cố trước buổi demo. Trải nghiệm tham dự Tham dự buổi workshop “DevOps on AWS” là cầu nối quan trọng giúp em lấp đầy khoảng cách giữa việc viết code (Dev) và vận hành hệ thống (Ops). Sự kiện đã cung cấp một lộ trình rõ ràng để xây dựng các hệ thống tin cậy và có khả năng mở rộng. Những trải nghiệm chính bao gồm:\nHọc hỏi từ người thực chiến: Các diễn giả là những AWS Community Builders tích cực, chia sẻ những \u0026ldquo;bài học xương máu\u0026rdquo; và các cạm bẫy thực tế trong DevOps chứ không chỉ là lý thuyết sách vở. Em đã hiểu rõ hơn về Mô hình Trách nhiệm Chia sẻ trong bối cảnh DevOps. Tiếp cận kỹ thuật thực tế: Tham gia live CI/CD walkthrough, chứng kiến code thô được chuyển đổi thành ứng dụng hoàn chỉnh thông qua pipeline tự động chỉ trong vài phút. Trải nghiệm sức mạnh của AWS CDK khi triển khai VPC và ECS cluster với số dòng code ít hơn đáng kể so với CloudFormation template thuần túy. Trực quan hóa việc truy vết phân tán với AWS X-Ray, hiểu cách xác định điểm nghẽn (bottlenecks) trong microservices. Kết nối và thảo luận: Thảo luận về sự đánh đổi giữa GitFlow và Trunk-based development cho các nhóm nhỏ và nhận được hướng dẫn về lộ trình chứng chỉ AWS Certified DevOps Engineer – Professional. Bài học rút ra IaC là không thể thương lượng: Đối với bất kỳ dự án dài hạn nào, hạ tầng bắt buộc phải là code. Observability là sống còn: Xây dựng hệ thống chỉ là một nửa chặng đường; biết được hệ thống có \u0026ldquo;khỏe\u0026rdquo; hay không là nửa còn lại. Văn hóa hơn công cụ: Các công cụ như Jenkins hay CodePipeline sẽ vô dụng nếu thiếu văn hóa hợp tác và cải tiến liên tục. Hình ảnh sự kiện Tổng kết lại, sự kiện không chỉ cung cấp kỹ năng kỹ thuật về các công cụ AWS mà còn rèn luyện tư duy DevOps chuyên nghiệp, chuẩn bị cho em khả năng xây dựng các hệ thống chuẩn production một cách hiệu quả.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/4-eventparticipated/",
	"title": "Các events đã tham gia",
	"tags": [],
	"description": "",
	"content": "Trong quá trình thực tập, em đã tham gia 2 events, với mỗi event là một trải nghiệm đáng nhớ với những kiến thức mới, hay và bổ ích, cùng với đó là nhứng món quà và những khoảnh khắc rất tuyệt vời.\nEvent 1 Tên sự kiện: GenAI-powered App-DB Modernization workshop\nThời gian: 09:00 ngày 13/08/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 2 Tên sự kiện: GenAI-powered App-DB Modernization workshop\nThời gian: 09:00 ngày 13/08/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 3 Tên sự kiện: AWS Cloud Mastery Series #2 – DevOps on AWS\nThời gian: 09:00 ngày 17/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 4 Tên sự kiện: AWS Cloud Mastery Series #3 - ​Theo AWS Well-Architected Security Pillar\nThời gian: 09:00 ngày 29/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "Mô phỏng On-premises DNS ",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoint có một địa chỉ IP cố định trong từng AZ nơi chúng được triển khai, trong suốt thời gian tồn tại của endpoint (cho đến khi endpoint bị xóa). Các địa chỉ IP này được gắn vào Elastic network interface (ENI). AWS khuyến nghị sử dụng DNS để resolve địa chỉ IP cho endpoint để các ứng dụng downstream sử dụng địa chỉ IP mới nhất khi ENIs được thêm vào AZ mới hoặc bị xóa theo thời gian.\nTrong phần này, bạn sẽ tạo một quy tắc chuyển tiếp (forwarding rule) để gửi các yêu cầu resolve DNS từ môi trường truyền thống (mô phỏng) đến Private Hosted Zone trên Route 53. Phần này tận dụng cơ sở hạ tầng do CloudFormation triển khai trong phần Chuẩn bị môi trường.\nTạo DNS Alias Records cho Interface endpoint Click link để đi đến Route 53 management console (Hosted Zones section). Mẫu CloudFormation mà bạn triển khai trong phần Chuẩn bị môi trường đã tạo Private Hosted Zone này. Nhấp vào tên của Private Hosted Zone, s3.us-east-1.amazonaws.com: Tạo 1 record mới trong Private Hosted Zone: Giữ nguyên Record name và record type Alias Button: click để enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Chọn endpoint: Paste tên (Regional VPC Endpoint DNS) bạn đã lưu lại ở phần 4.3 Click Add another record, và add 1 cái record thứ 2 sử dụng những thông số sau: Record name: *. Record type: giữ giá trị default (type A) Alias Button: Click để enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Chọn endpoint: Paste tên (Regional VPC Endpoint DNS) bạn đã lưu lại ở phần 4.3 Click Create records Record mới xuất hiện trên giao diện Route 53.\nTạo một Resolver Forwarding Rule Route 53 Resolver Forwarding Rules cho phép bạn chuyển tiếp các DNS queries từ VPC của bạn đến các nguồn khác để resolve name. Bên ngoài môi trường workshop, bạn có thể sử dụng tính năng này để chuyển tiếp các DNS queries từ VPC của bạn đến các máy chủ DNS chạy trên on-premises. Trong phần này, bạn sẽ mô phỏng một on-premises conditional forwarder bằng cách tạo một forwarding rule để chuyển tiếp các DNS queries for Amazon S3 đến một Private Hosted Zone chạy trong \u0026ldquo;VPC Cloud\u0026rdquo; để resolve PrivateLink interface endpoint regional DNS name.\nTừ giao diện Route 53, chọn Inbound endpoints trên thanh bên trái\nTrong giao diện Inbound endpoint, Chọn ID của Inbound endpoint.\nSao chép 2 địa chỉ IP trong danh sách vào trình chỉnh sửa. Từ giao diện Route 53, chọn Resolver \u0026gt; Rules và chọn Create rule Trong giao diện Create rule Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: điền cả hai IP bạn đã lưu trữ trên trình soạn thảo (inbound endpoint addresses) và sau đó chọn Submit Bạn đã tạo thành công resolver forwarding rule.\nKiểm tra on-premises DNS mô phỏng. Kết nối đến Test-Interface-Endpoint EC2 instance với Session Manager Kiểm tra DNS resolution. Lệnh dig sẽ trả về địa chỉ IP được gán cho VPC endpoint interface đang chạy trên VPC (địa chỉ IP của bạn sẽ khác): dig +short s3.us-east-1.amazonaws.com Các địa chỉ IP được trả về là các địa chỉ IP VPC enpoint, KHÔNG phải là các địa chỉ IP Resolver mà bạn đã dán từ trình chỉnh sửa văn bản của mình. Các địa chỉ IP của Resolver endpoint và VPC endpoin trông giống nhau vì chúng đều từ khối CIDR VPC Cloud.\nTruy cập vào menu VPC (phần Endpoints), chọn S3 interface endpoint. Nhấp vào tab Subnets và xác nhận rằng các địa chỉ IP được trả về bởi lệnh Dig khớp với VPC endpoint: Hãy quay lại shell của bạn và sử dụng AWS CLI để kiểm tra danh sách các bucket S3 của bạn: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Kết thúc phiên làm việc của Session Manager của bạn: Trong phần này, bạn đã tạo một Interface Endpoint cho Amazon S3. Điểm cuối này có thể được truy cập từ on-premises thông qua Site-to-Site VPN hoặc AWS Direct Connect. Các điểm cuối Route 53 Resolver outbound giả lập chuyển tiếp các yêu cầu DNS từ on-premises đến một Private Hosted Zone đang chạy trên đám mây. Các điểm cuối Route 53 inbound nhận yêu cầu giải quyết và trả về một phản hồi chứa địa chỉ IP của Interface Endpoint VPC. Sử dụng DNS để giải quyết các địa chỉ IP của điểm cuối cung cấp tính sẵn sàng cao trong trường hợp một Availability Zone gặp sự cố.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/",
	"title": "Truy cập S3 từ môi trường truyền thống",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần này, bạn sẽ tạo một Interface Endpoint để truy cập Amazon S3 từ môi trường truyền thống mô phỏng. Interface Endpoint sẽ cho phép bạn định tuyến đến Amazon S3 qua kết nối VPN từ môi trường truyền thống mô phỏng của bạn.\nTại sao nên sử dụng Interface Endpoint:\nCác Gateway endpoints chỉ hoạt động với các tài nguyên đang chạy trong VPC nơi chúng được tạo. Interface Endpoint hoạt động với tài nguyên chạy trong VPC và cả tài nguyên chạy trong môi trường truyền thống. Khả năng kết nối từ môi trường truyền thống của bạn với aws cloud có thể được cung cấp bởi AWS Site-to-Site VPN hoặc AWS Direct Connect. Interface Endpoint cho phép bạn kết nối với các dịch vụ do AWS PrivateLink cung cấp. Các dịch vụ này bao gồm một số dịch vụ AWS, dịch vụ do các đối tác và khách hàng AWS lưu trữ trong VPC của riêng họ (gọi tắt là Dịch vụ PrivateLink endpoints) và các dịch vụ Đối tác AWS Marketplace. Đối với workshop này, chúng ta sẽ tập trung vào việc kết nối với Amazon S3. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/4-eventparticipated/4.4-event4/",
	"title": "Sự kiện 4",
	"tags": [],
	"description": "",
	"content": "Bài thu hoạch: AWS Cloud Mastery Series #3 - ​Theo AWS Well-Architected Security Pillar Mục tiêu sự kiện Hiểu sâu về Trụ cột Bảo mật (Security Pillar) trong khung kiến trúc AWS Well-Architected. Nắm vững các nguyên tắc cốt lõi: Đặc quyền tối thiểu (Least Privilege), Zero Trust, và Phòng thủ chiều sâu (Defense in Depth). Học cách triển khai các biện pháp kiểm soát an ninh qua 5 lĩnh vực chính: Quản lý danh tính (IAM), Phát hiện, Bảo vệ hạ tầng, Bảo vệ dữ liệu, và Ứng phó sự cố. Nhận diện các mối đe dọa an ninh mạng phổ biến trong môi trường Cloud tại doanh nghiệp Việt Nam. Thời gian \u0026amp; Địa điểm Thời gian: 08:30 – 12:00, Thứ Bảy, ngày 29/11/2025\nĐịa điểm: Tầng 26, Tòa nhà tài chính Bitexco, 2 Đ. Hải Triều, Bến Nghé, Quận 1, TP. Hồ Chí Minh\nDiễn giả Le Vu Xuan An - Software \u0026amp; Cloud Engineer | AWS Cloud Club Captain HCMUTE Tran Doan Cong Ly - DevOps Engineer | AWS FCAJ Ambassador | AWS Cloud Club Captain PTIT Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud | AWS First Cloud AI Journey | AWS Cloud Club Captain HUFLIT Tran Duc Anh - Cloud Security Engineer Trainee | First Cloud AI Journey | AWS Cloud Club Captain SGU Nguyen Tuan Thinh - Cloud Engineer Trainee | First Cloud AI Journey Nguyen Do Thanh Dat - Cloud Engineer Trainee | First Cloud AI Journey Đinh Le Hoang Anh – Cloud Engineer Trainee | First Cloud AI Journey Kha Van - Cloud Security Engineer | AWS Community Builders Special Guest:\nMendel Grabski - Cloud Security \u0026amp; Solution Architect | Enabling Secure-by-Design Solutions Truong Quang Tinh - DevOps Engineer, TymeX | AWS Community Builder Nội dung nổi bật 1. Nền tảng Bảo mật \u0026amp; Quản lý danh tính (IAM) Nguyên tắc cốt lõi: Chuyển dịch từ bảo mật chu vi (perimeter) sang mô hình Zero Trust, nơi mọi yêu cầu truy cập đều phải được xác thực và cấp quyền. Kiến trúc IAM hiện đại: Loại bỏ việc sử dụng thông tin xác thực dài hạn (Access Keys) và thay thế bằng IAM Roles và Identity Center (SSO). Thực thi Least Privilege (Đặc quyền tối thiểu) và sử dụng Access Analyzer để kiểm tra policy. Sử dụng SCP (Service Control Policies) để thiết lập ranh giới quyền hạn trong môi trường đa tài khoản (Multi-account). 2. Phát hiện \u0026amp; Giám sát liên tục Tầm nhìn tập trung: Sử dụng Security Hub để gom tất cả cảnh báo từ GuardDuty, Inspector và Macie về một nơi. Chiến lược Logging: Bắt buộc ghi log tại mọi tầng: Network (VPC Flow Logs), API (CloudTrail), và Storage (S3 Logs). Detection-as-Code: Định nghĩa các quy tắc phát hiện mối đe dọa bằng mã nguồn để đảm bảo tính nhất quán giữa các môi trường. 3. Bảo vệ Hạ tầng \u0026amp; Dữ liệu An ninh mạng: Áp dụng Defense in Depth với phân đoạn VPC (Private/Public subnets), Security Groups và WAF/Shield tại các điểm biên (edge). Mã hóa: Lưu trữ (At-rest): Sử dụng KMS với cơ chế tự động xoay vòng khóa (key rotation) cho EBS, RDS, S3. Truyền tải (In-transit): Bắt buộc sử dụng TLS 1.2+ cho mọi luồng dữ liệu. Quản lý bí mật: Thay thế thông tin đăng nhập (hardcoded credentials) trong code bằng AWS Secrets Manager hoặc Parameter Store. 4. Tự động hóa Ứng phó sự cố (Incident Response - IR) Vòng đời IR: Chuẩn bị → Phát hiện \u0026amp; Phân tích → Khoanh vùng, Xử lý \u0026amp; Khôi phục → Hoạt động sau sự cố. Playbooks: Quy trình mẫu xử lý các tình huống thực tế: lộ IAM key, S3 bucket bị public, phát hiện malware trên EC2. Tự động hóa: Sử dụng Lambda và Step Functions để tự động khắc phục (ví dụ: tự động thu hồi quyền của user bị xâm nhập). Bài học đúc kết Tư duy Bảo mật Bảo mật là trách nhiệm của mọi người: Không chỉ riêng đội Security, lập trình viên phải thực hành \u0026ldquo;Security by Design\u0026rdquo; ngay từ khi viết dòng code đầu tiên. Tư duy \u0026ldquo;Assume Breach\u0026rdquo;: Luôn thiết kế hệ thống với giả định rằng một thành phần có thể bị xâm nhập, từ đó giới hạn \u0026ldquo;phạm vi ảnh hưởng\u0026rdquo; (blast radius). Kiến trúc Kỹ thuật Identity là \u0026ldquo;bức tường lửa\u0026rdquo; mới: Trong môi trường Cloud-native, việc quản lý IAM quan trọng hơn cả firewall mạng. Hạ tầng bất biến (Immutable Infrastructure): Hạn chế vá lỗi trực tiếp trên server; thay vào đó hãy thay thế bằng các server mới đã được vá lỗi để tránh malware ẩn mình. Ứng dụng vào công việc Refactor IAM: Rà soát ngay các IAM policy của dự án nhóm. Loại bỏ các quyền *:* dư thừa và thay thế hardcoded Access Keys bằng IAM Roles cho EC2/Lambda. Bảo vệ Secrets: Di chuyển các thông tin nhạy cảm (DB password, API Key) từ file .env lên AWS Systems Manager Parameter Store. Kích hoạt GuardDuty: Bật GuardDuty trên tài khoản dự án (tận dụng Free Tier) để phát hiện các hành vi bất thường như đào coin trái phép. Mã hóa mặc định: Bật Server-Side Encryption (SSE-S3) cho tất cả S3 bucket trong dự án Quản lý sinh viên. Trải nghiệm tham dự Buổi workshop tập trung cao độ vào tính thực chiến của bảo mật đám mây. Khác với các buổi giới thiệu chung, sự kiện này cung cấp các mô hình hành động cụ thể:\nHọc tập tương tác: Phần demo về Validate IAM Policy giúp em thấy rõ việc cấu hình sai quyền hạn dễ dàng xảy ra như thế nào. Tính thực tế cao: Việc thảo luận về \u0026ldquo;Top threats tại Việt Nam\u0026rdquo; làm nội dung trở nên rất gần gũi, nhấn mạnh sự cần thiết phải bảo vệ chống lại việc lộ lọt thông tin xác thực. Tự động hóa: Chứng kiến một Playbook IR được kích hoạt bởi EventBridge và xử lý bởi Lambda đã thay đổi hoàn toàn cách nhìn của em về việc vận hành bảo mật ở quy mô lớn. Hình ảnh sự kiện Sự kiện này đã thay đổi tư duy của em từ \u0026ldquo;xây dựng nhanh\u0026rdquo; sang \u0026ldquo;xây dựng an toàn\u0026rdquo;. Em đã có một lộ trình rõ ràng để gia cố bảo mật cho ứng dụng trước khi đưa vào môi trường production.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "Khi bạn tạo một Interface Endpoint hoặc cổng, bạn có thể đính kèm một chính sách điểm cuối để kiểm soát quyền truy cập vào dịch vụ mà bạn đang kết nối. Chính sách VPC Endpoint là chính sách tài nguyên IAM mà bạn đính kèm vào điểm cuối. Nếu bạn không đính kèm chính sách khi tạo điểm cuối, thì AWS sẽ đính kèm chính sách mặc định cho bạn để cho phép toàn quyền truy cập vào dịch vụ thông qua điểm cuối.\nBạn có thể tạo chính sách chỉ hạn chế quyền truy cập vào các S3 bucket cụ thể. Điều này hữu ích nếu bạn chỉ muốn một số Bộ chứa S3 nhất định có thể truy cập được thông qua điểm cuối.\nTrong phần này, bạn sẽ tạo chính sách VPC Endpoint hạn chế quyền truy cập vào S3 bucket được chỉ định trong chính sách VPC Endpoint.\nKết nối tới EC2 và xác minh kết nối tới S3. Bắt đầu một phiên AWS Session Manager mới trên máy chủ có tên là Test-Gateway-Endpoint. Từ phiên này, xác minh rằng bạn có thể liệt kê nội dung của bucket mà bạn đã tạo trong Phần 1: Truy cập S3 từ VPC. aws s3 ls s3://\u0026lt;your-bucket-name\u0026gt; Nội dung của bucket bao gồm hai tệp có dung lượng 1GB đã được tải lên trước đó.\nTạo một bucket S3 mới; tuân thủ mẫu đặt tên mà bạn đã sử dụng trong Phần 1, nhưng thêm \u0026lsquo;-2\u0026rsquo; vào tên. Để các trường khác là mặc định và nhấp vào Create. Tạo bucket thành công. Policy mặc định cho phép truy cập vào tất cả các S3 Buckets thông qua VPC endpoint.\nTrong giao diện Edit Policy, sao chép và dán theo policy sau, thay thế yourbucketname-2 với tên bucket thứ hai của bạn. Policy này sẽ cho phép truy cập đến bucket mới thông qua VPC endpoint, nhưng không cho phép truy cập đến các bucket còn lại. Chọn Save để kích hoạt policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Cấu hình policy thành công.\nTừ session của bạn trên Test-Gateway-Endpoint instance, kiểm tra truy cập đến S3 bucket bạn tạo ở bước đầu aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; Câu lệnh trả về lỗi bởi vì truy cập vào S3 bucket không có quyền trong VPC endpoint policy.\nTrở lại home directory của bạn trên EC2 instance cd~ Tạo file fallocate -l 1G test-bucket2.xyz Sao chép file lên bucket thứ 2 aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; Thao tác này được cho phép bởi VPC endpoint policy.\nSau đó chúng ta kiểm tra truy cập vào S3 bucket đầu tiên\naws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt;\nCâu lệnh xảy ra lỗi bởi vì bucket không có quyền truy cập bởi VPC endpoint policy.\nTrong phần này, bạn đã tạo chính sách VPC Endpoint cho Amazon S3 và sử dụng AWS CLI để kiểm tra chính sách. Các hoạt động AWS CLI liên quan đến bucket S3 ban đầu của bạn thất bại vì bạn áp dụng một chính sách chỉ cho phép truy cập đến bucket thứ hai mà bạn đã tạo. Các hoạt động AWS CLI nhắm vào bucket thứ hai của bạn thành công vì chính sách cho phép chúng. Những chính sách này có thể hữu ích trong các tình huống khi bạn cần kiểm soát quyền truy cập vào tài nguyên thông qua VPC Endpoint.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Set up The project Metropolitano Tổng quan AWS PrivateLink cung cấp kết nối riêng tư đến các dịch vụ aws từ VPCs hoặc trung tâm dữ liệu (on-premise) mà không làm lộ lưu lượng truy cập ra ngoài public internet.\nTrong bài lab này, chúng ta sẽ học cách tạo, cấu hình, và kiểm tra VPC endpoints để cho phép workload của bạn tiếp cận các dịch vụ AWS mà không cần đi qua Internet công cộng.\nChúng ta sẽ tạo hai loại endpoints để truy cập đến Amazon S3: gateway vpc endpoint và interface vpc endpoint. Hai loại vpc endpoints này mang đến nhiều lợi ích tùy thuộc vào việc bạn truy cập đến S3 từ môi trường cloud hay từ trung tâm dữ liệu (on-premise).\nGateway - Tạo gateway endpoint để gửi lưu lượng đến Amazon S3 hoặc DynamoDB using private IP addresses. Bạn điều hướng lưu lượng từ VPC của bạn đến gateway endpoint bằng các bảng định tuyến (route tables) Interface - Tạo interface endpoint để gửi lưu lượng đến các dịch vụ điểm cuối (endpoints) sử dụng Network Load Balancer để phân phối lưu lượng. Lưu lượng dành cho dịch vụ điểm cuối được resolved bằng DNS. Nội dung Tổng quan về workshop Chuẩn bị Truy cập đến S3 từ VPC Truy cập đến S3 từ TTDL On-premises VPC Endpoint Policies (làm thêm) Dọn dẹp tài nguyên "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.6-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Dọn dẹp tài nguyên Xin chúc mừng bạn đã hoàn thành xong lab này! Trong lab này, bạn đã học về các mô hình kiến trúc để truy cập Amazon S3 mà không sử dụng Public Internet.\nBằng cách tạo Gateway endpoint, bạn đã cho phép giao tiếp trực tiếp giữa các tài nguyên EC2 và Amazon S3, mà không đi qua Internet Gateway. Bằng cách tạo Interface endpoint, bạn đã mở rộng kết nối S3 đến các tài nguyên chạy trên trung tâm dữ liệu trên chỗ của bạn thông qua AWS Site-to-Site VPN hoặc Direct Connect. Dọn dẹp Điều hướng đến Hosted Zones trên phía trái của bảng điều khiển Route 53. Nhấp vào tên của s3.us-east-1.amazonaws.com zone. Nhấp vào Delete và xác nhận việc xóa bằng cách nhập từ khóa \u0026ldquo;delete\u0026rdquo;. Disassociate Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. 4.Mở console của CloudFormation và xóa hai stack CloudFormation mà bạn đã tạo cho bài thực hành này:\nPLOnpremSetup PLCloudSetup Xóa các S3 bucket Mở bảng điều khiển S3 Chọn bucket chúng ta đã tạo cho lab, nhấp chuột và xác nhận là empty. Nhấp Delete và xác nhận delete. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/6-self-evaluation/",
	"title": "Tự Đánh Giá",
	"tags": [],
	"description": "",
	"content": "Trong thời gian thực tập tại Công ty TNHH Amazon Web Services Vietnam từ 09/2025 đến 12/2025, tôi đã có cơ hội học hỏi, thực hành và áp dụng kiến thức đã học ở trường vào môi trường làm việc thực tế.\nTôi tham gia vào việc học cách sử dụng tất cả các dịch vụ và cách chúng hoạt động trên AWS, qua đó tôi cải thiện kỹ năng thực hành workshop, áp dụng các dịch vụ AWS vào ứng dụng của mình. Đồng thời, tôi còn phải tối ưu hóa chi phí, hiệu năng và tăng cường các lớp bảo mật.\nKhi ở công ty, tôi luôn kiểm tra danh sách công việc (to-do list) và kiểm tra tất cả các nhiệm vụ chưa hoàn thành từ hôm trước. Bên cạnh đó, tôi có cơ hội gặp gỡ mentor và nhờ họ giúp đỡ khi gặp vấn đề. Trong quá trình học tập, tôi cùng các đồng đội đã xây dựng các website và sử dụng các dịch vụ AWS.\nĐể phản ánh khách quan về thời gian thực tập, tôi đánh giá bản thân dựa trên các tiêu chí sau:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức \u0026amp; kỹ năng chuyên môn Hiểu biết về lĩnh vực, áp dụng kiến thức vào thực tế, thành thạo công cụ, chất lượng công việc ☐ ✅ ☐ 2 Khả năng học hỏi Khả năng tiếp thu kiến thức mới và học nhanh ☐ ✅ ☐ 3 Tính chủ động Chủ động, tìm kiếm nhiệm vụ mà không chờ hướng dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn và đảm bảo chất lượng ☐ ✅ ☐ 5 Kỷ luật Tuân thủ lịch trình, quy tắc và quy trình làm việc ✅ ☐ ☐ 6 Tư duy tiến bộ Sẵn sàng nhận phản hồi và cải thiện bản thân ☐ ☐ ✅ 7 Kỹ năng giao tiếp Trình bày ý tưởng và báo cáo công việc rõ ràng ☐ ✅ ☐ 8 Làm việc nhóm Làm việc hiệu quả với đồng nghiệp và tham gia nhóm ☐ ✅ ☐ 9 Hành vi chuyên nghiệp Tôn trọng đồng nghiệp, đối tác và môi trường làm việc ✅ ☐ ☐ 10 Kỹ năng giải quyết vấn đề Xác định vấn đề, đề xuất giải pháp và thể hiện sự sáng tạo ☐ ✅ ☐ 11 Đóng góp cho dự án/nhóm Hiệu quả công việc, ý tưởng sáng tạo, được đồng đội công nhận ✅ ☐ ☐ 12 Tổng quan Đánh giá tổng thể về toàn bộ thời gian thực tập ☐ ✅ ☐ Những điểm cần cải thiện Tôi là người hướng nội, nên giao tiếp với mentor còn hạn chế. Tôi cần thay đổi cách giao tiếp và dành thời gian gặp mentor để hỏi về các vấn đề của mình. Tôi cần cải thiện kỹ năng quản lý dự án vì phải hướng dẫn đồng đội và giao nhiệm vụ cho họ, nhưng hiện tại tôi chưa áp dụng phương pháp như Agile hoặc Scrum để phát triển phần mềm cùng đồng đội. Tôi luôn suy nghĩ về cách các dịch vụ hoạt động và áp dụng vào ứng dụng của mình nhưng vẫn chưa hiểu cách thức hoạt động của những dịch vụ đó, do đó tôi cần dành thời gian cải thiện tư duy logic để hiểu rõ hơn. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/7-feedback/",
	"title": "Chia sẻ, đóng góp ý kiến",
	"tags": [],
	"description": "",
	"content": " Tại đây bạn có thể tự do đóng góp ý kiến cá nhân về những trải nghiệm khi tham gia chương trình First Cloud Journey, giúp team FCJ cải thiện những vấn đề còn thiếu sót dựa trên các hạng mục sau:\nĐánh giá chung 1. Môi trường làm việc\nMôi trường làm việc rất thân thiện và cởi mở. Các thành viên trong FCJ luôn sẵn sàng hỗ trợ khi mình gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn. Tuy nhiên, mình nghĩ có thể bổ sung thêm một số buổi giao lưu hoặc team bonding để mọi người hiểu nhau hơn.\n2. Sự hỗ trợ của mentor / team admin\nMentor hướng dẫn rất chi tiết, giải thích rõ ràng khi mình chưa hiểu và luôn khuyến khích mình đặt câu hỏi. Team admin hỗ trợ các thủ tục, tài liệu và tạo điều kiện để mình làm việc thuận lợi. Mình đánh giá cao việc mentor cho phép mình thử và tự xử lý vấn đề thay vì chỉ đưa đáp án.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCông việc mình được giao phù hợp với kiến thức mình đã học ở trường, đồng thời mở rộng thêm những mảng mới mà mình chưa từng được tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, kỹ năng làm việc nhóm, và cả cách giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp mình định hướng tốt hơn cho sự nghiệp.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty rất tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí. Điều này giúp mình cảm thấy mình là một phần của tập thể, dù chỉ là thực tập sinh.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty có hỗ trợ phụ cấp thực tập và tạo điều kiện về thời gian linh hoạt khi cần thiết. Ngoài ra, việc được tham gia các buổi đào tạo nội bộ là một điểm cộng lớn.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập? Trong quá trình học tập thì em cảm thấy mình thích nghi với văn hóa làm việc của công ty và cảm thấy hài lòng khi được các anh mentor hướng dẫn. Điều bạn nghĩ công ty cần cải thiện cho các thực tập sinh sau? Công ty cần cải thiện về việc cho sinh viên có thể lên công ty hằng ngày Nếu giới thiệu cho bạn bè, bạn có khuyên họ thực tập ở đây không? Vì sao? Nếu như có giới thiệu bạn bè thì em có lời khuyên cho họ là nên đi với công ty vì công ty TNHH Amazon Web Services vietnam không chỉ là 1 công ty về lĩnh vực công nghệ điện toán đám mây, mà còn là 1 công ty có cộng đồng AWS study group lớn trong việt nam để tất cả mọi sinh viên có thể học hỏi về điện toán đám mây. Đề xuất \u0026amp; mong muốn Bạn có lời khuyên nào để nâng cao trải nghiệm kì thực tập? Em muốn được tham gia câu lạc bộ công ty để học hỏi thêm cách anh chị mentor và cách họ lãnh đạo 1 đội nhóm Bạn có muốn tiếp tục chương trình này trong tương lai? Em muốn được gắn bó thêm công ty thêm Góp ý khác (tự do chia sẻ): "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.14-week14/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]