[
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/",
	"title": "Báo cáo thực tập",
	"tags": [],
	"description": "",
	"content": "Báo cáo thực tập Thông tin sinh viên: Họ và tên: Tào Bảo Thành\nSố điện thoại: 0901452366\nEmail: taobaothanh365@gmail.com\nTrường: Đại học FPT Hồ Chí Minh\nNgành: Kỹ Thuật Phần Mềm\nLớp: OJT202\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 12/08/2025 đến ngày 12/11/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Các dịch vụ AWS vươn lên tầm cao mới trong Prime Day 2025: các số liệu và cột mốc chính Amazon Prime Day 2025 là sự kiện mua sắm Prime Day lớn nhất từ trước đến nay, thiết lập kỷ lục cả về khối lượng bán hàng và tổng số sản phẩm được bán ra trong suốt 4 ngày diễn ra sự kiện. Các thành viên Prime đã tiết kiệm hàng tỷ đô la khi mua sắm hàng triệu ưu đãi trên Amazon trong sự kiện này.\nNăm nay đánh dấu một sự chuyển đổi đáng kể trong trải nghiệm Prime Day nhờ những tiến bộ trong các dịch vụ AI tạo sinh của Amazon và AWS. Khách hàng đã sử dụng Alexa+ — trợ lý cá nhân thế hệ tiếp theo của Amazon hiện có sẵn trong giai đoạn truy cập sớm cho hàng triệu khách hàng — cùng với trợ lý mua sắm được hỗ trợ AI, Rufus, và Hướng dẫn mua sắm AI. Những tính năng này, được xây dựng dựa trên hơn 15 năm đổi mới đám mây và chuyên môn về học máy từ AWS, kết hợp với kinh nghiệm bán lẻ và người tiêu dùng sâu rộng từ Amazon, đã giúp khách hàng nhanh chóng khám phá các ưu đãi và có được thông tin sản phẩm, bổ sung cho dịch vụ giao hàng nhanh, miễn phí mà thành viên Prime tận hưởng quanh năm.\nNhư một phần trong truyền thống hằng năm của chúng tôi nhằm chia sẻ về cách AWS đã hỗ trợ Prime Day với doanh số kỷ lục, tôi muốn giới thiệu các dịch vụ và những số liệu ấn tượng từ AWS đã tạo nên trải nghiệm mua sắm tuyệt vời của bạn.\nPrime Day 2025 – tất cả các con số Trong những tuần trước các sự kiện mua sắm lớn như Prime Day, các trung tâm hoàn thiện đơn hàng và trạm giao hàng của Amazon hoạt động để chuẩn bị sẵn sàng và đảm bảo vận hành hiệu quả, an toàn. Ví dụ, hệ thống lưu trữ và truy xuất tự động (ASRS) của Amazon vận hành một đội ngũ robot di động công nghiệp toàn cầu để di chuyển hàng hóa trong các trung tâm hoàn thiện đơn hàng.\nAWS Outposts, một dịch vụ được quản lý toàn phần mở rộng trải nghiệm AWS đến môi trường tại chỗ, cung cấp sức mạnh cho các ứng dụng phần mềm quản lý điều khiển và vận hành ASRS, và hỗ trợ giao hàng trong ngày và ngày kế tiếp thông qua xử lý độ trễ thấp cho các lệnh điều khiển robot quan trọng.\nTrong Prime Day 2025, AWS Outposts tại một trong những trung tâm hoàn thiện đơn hàng lớn nhất đã gửi hơn 524 triệu lệnh đến hơn 7.000 robot, đạt đỉnh 8 triệu lệnh mỗi giờ — tăng 160% so với Prime Day 2024.\nDưới đây là một số số liệu thú vị và ấn tượng khác:\nAmazon Elastic Compute Cloud (Amazon EC2) – Trong Prime Day 2025, AWS Graviton, một bộ vi xử lý được thiết kế để mang lại hiệu suất/chi phí tốt nhất cho các khối lượng công việc trên Amazon EC2, đã cung cấp hơn 40% năng lực tính toán Amazon EC2 được Amazon.com sử dụng. Amazon cũng đã triển khai hơn 87.000 chip AWS Inferentia và AWS Trainium — chip silicon tùy chỉnh dành cho huấn luyện và suy luận AI/học sâu — để hỗ trợ Amazon Rufus trong Prime Day.\nAmazon SageMaker AI — Amazon SageMaker AI, một dịch vụ được quản lý toàn phần tập hợp nhiều công cụ cho học máy hiệu suất cao và chi phí thấp, đã xử lý hơn 626 tỷ yêu cầu suy luận trong Prime Day 2025.\nAmazon Elastic Container Service (Amazon ECS) và AWS Fargate – Amazon ECS, dịch vụ điều phối container được quản lý toàn phần, kết hợp với AWS Fargate, một công cụ tính toán serverless cho container, đã khởi chạy trung bình 18,4 triệu tác vụ mỗi ngày trên AWS Fargate trong Prime Day 2025, tăng 77% so với mức trung bình của Prime Day năm trước.\nAWS Fault Injection Service (AWS FIS) – Chúng tôi đã chạy hơn 6.800 thí nghiệm AWS FIS — nhiều gấp tám lần so với năm 2024 — để kiểm tra khả năng chịu lỗi và đảm bảo Amazon.com luôn khả dụng trong Prime Day. Sự gia tăng đáng kể này có được nhờ hai cải tiến: hỗ trợ mới cho các thí nghiệm lỗi mạng trên Amazon ECS với AWS Fargate, và việc tích hợp thử nghiệm FIS vào pipeline CI/CD.\nAWS Lambda – AWS Lambda đã xử lý hơn 1,7 nghìn tỷ lượt gọi mỗi ngày trong Prime Day 2025.\nAmazon API Gateway – Trong Prime Day 2025, Amazon API Gateway đã xử lý hơn 1 nghìn tỷ yêu cầu dịch vụ nội bộ — tăng trung bình 30% mỗi ngày so với Prime Day 2024.\nAmazon CloudFront – Amazon CloudFront đã phân phát hơn 3 nghìn tỷ yêu cầu HTTP trong tuần Prime Day toàn cầu 2025, tăng 43% so với Prime Day 2024.\nAmazon Elastic Block Store (Amazon EBS) – Trong Prime Day 2025, Amazon EBS đã đạt đỉnh 20,3 nghìn tỷ thao tác I/O, di chuyển đến 1 exabyte dữ liệu mỗi ngày.\nAmazon Aurora – Trong Prime Day, Amazon Aurora đã xử lý 500 tỷ giao dịch, lưu trữ 4.071 terabyte dữ liệu, và truyền 999 terabyte dữ liệu.\nAmazon DynamoDB – Amazon DynamoDB đã xử lý hàng chục nghìn tỷ cuộc gọi API trong Prime Day, duy trì tính khả dụng cao với thời gian phản hồi đơn vị mili-giây, và đạt đỉnh 151 triệu yêu cầu mỗi giây.\nAmazon ElastiCache – Trong Prime Day, Amazon ElastiCache đã đạt đỉnh hơn 1,5 triệu tỷ yêu cầu mỗi ngày và hơn 1,4 nghìn tỷ yêu cầu trong một phút.\nAmazon Kinesis Data Streams – Đã xử lý đỉnh 807 triệu bản ghi mỗi giây trong Prime Day 2025.\nAmazon Simple Queue Service (Amazon SQS) – Trong Prime Day 2025, Amazon SQS đã thiết lập kỷ lục lưu lượng mới với 166 triệu tin nhắn mỗi giây.\nAmazon GuardDuty – Trong Prime Day 2025, Amazon GuardDuty đã giám sát trung bình 8,9 nghìn tỷ sự kiện log mỗi giờ, tăng 48,9% so với Prime Day năm ngoái.\nAWS CloudTrail – AWS CloudTrail đã xử lý hơn 2,5 nghìn tỷ sự kiện trong Prime Day 2025, so với 976 tỷ sự kiện trong năm 2024.\nSẵn sàng để mở rộng quy mô\nNếu bạn đang chuẩn bị cho các sự kiện quan trọng đối với doanh nghiệp như ra mắt sản phẩm, di chuyển hệ thống, và các cuộc di cư khác, tôi khuyên bạn nên tận dụng AWS Countdown (trước đây gọi là AWS Infrastructure Event Management, hay IEM). Đây là một chương trình hỗ trợ toàn diện giúp đánh giá mức độ sẵn sàng vận hành, xác định và giảm thiểu rủi ro, và lập kế hoạch năng lực, sử dụng các playbook đã được chứng minh do các chuyên gia AWS phát triển.Chúng tôi đã mở rộng để bao gồm: generative AI implementation support nhằm giúp bạn tự tin ra mắt và mở rộng các sáng kiến AI; migration and modernization support, bao gồm mainframe modernization; và tối ưu hóa hạ tầng cho các lĩnh vực chuyên biệt bao gồm election systems, retail operations, healthcare services, và sports and gaming events.\nTôi mong được chứng kiến những kỷ lục nào khác sẽ bị phá vỡ vào năm tới!\n— Channy\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\rAWS được công nhận là Nhà lãnh đạo trong báo cáo Gartner Magic Quadrant 2025 về Nền tảng Ứng dụng Cloud-Native và Quản lý Container Một tháng trước, mình đã chia sẻ rằng Amazon Web Services (AWS) được công nhận làLeader in 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services (SCPS),đánh dấu năm thứ mười lăm liên tiếp AWS được Gartner xếp hạng là Nhà lãnh đạo.\nNăm 2024, AWS được công nhận là Nhà lãnh đạo trong nhiều báo cáo Gartner Magic Quadrant, bao gồm: AI Code Assistants, Cloud-Native Application Platforms, Cloud Database Management Systems, Container Management, Data Integration Tools, Desktop as a Service (DaaS), and Data Science and Machine Learning Platforms as cũng như SCPS.Sang năm 2025, AWS tiếp tục được công nhận là Nhà lãnh đạo trong Gartner Magic Quadrant cho Contact Center as a Service (CCaaS), Desktop as a Service và Data Science and Machine Learning (DSML) nền tảng. Chúng tôi tin tưởng rằng điều này cho thấy AWS đang cung cấp danh mục dịch vụ rộng và sâu nhất cho khách hàng.\nHôm nay, mình vui mừng chia sẻ các báo cáo Magic Quadrant mới nhất, trong đó AWS được vinh danh là Nhà lãnh đạo trong nhiều thị trường công nghệ đám mây: Nền tảng Ứng dụng Cloud-Native (còn gọi là Nền tảng Ứng dụng Đám mây) và Quản lý Container.\n2025 Gartner Magic Quadrant for Cloud-Native Application Platforms AWS đã được công nhận là Nhà lãnh đạo trong Gartner Magic Quadrant cho Nền tảng Ứng dụng Cloud-Native trong 2 năm liên tiếp. AWS được xếp hạng cao nhất về “Khả năng Thực thi”.\nGartner định nghĩa nền tảng ứng dụng cloud-native là những dịch vụ cung cấp môi trường runtime được quản lý cho ứng dụng, cùng các khả năng tích hợp để quản lý vòng đời của ứng dụng hoặc thành phần ứng dụng trong môi trường đám mây.\nDanh mục ứng dụng cloud-native toàn diện của chúng tôi—AWS Lambda, AWS App Runner, AWS Amplify, and AWS Elastic Beanstalk—mang đến sự linh hoạt để xây dựng các ứng dụng hiện đại với khả năng AI mạnh mẽ, nhờ đổi mới liên tục và tích hợp sâu rộng trong toàn bộ danh mục dịch vụ AWS.\nKhách hàng có thể dễ dàng lựa chọn dịch vụ thông qua tài liệu chi tiết, kiến trúc tham chiếu, và hướng dẫn có sẵn trong AWS Solutions Library, cùng với khuyến nghị ngữ cảnh được hỗ trợ bởi AI từ Amazon Q dựa trên yêu cầu cụ thể. Mặc dù AWS Lambda được tối ưu hóa cho AWS để mang lại trải nghiệm serverless (không máy chủ) tốt nhất, nhưng nó vẫn tuân theo các tiêu chuẩn của ngành về điện toán serverless và hỗ trợ các ngôn ngữ lập trình cũng như framework phổ biến. Bạn có thể tìm thấy tất cả các khả năng cần thiết trong AWS, bao gồm các tính năng nâng cao cho AI/ML, điện toán biên (edge computing) và tích hợp doanh nghiệp.\nBạn có thể xây dựng, triển khai, và mở rộng các ứng dụng generative AI bằng cách tích hợp các dịch vụ compute này với Amazon Bedrock để thực hiện suy luận serverless, vàAmazon SageMaker cho cho việc huấn luyện và quản lý artificial intelligence and machine learning (AI/ML).\nTruy cập 2025 Gartner Magic Quadrant for Cloud-Native Application Platforms để xem thêm thông tin.\nGartner Magic Quadrant 2025 cho Quản lý Container Trong báo cáo Gartner Magic Quadrant 2025 cho Quản lý Container, AWS tiếp tục được công nhận là Nhà lãnh đạo trong 3 năm liên tiếp và được xếp hạng xa nhất về “Độ hoàn thiện Tầm nhìn”.\nGartner định nghĩa quản lý container là các dịch vụ hỗ trợ triển khai và vận hành workloads được container hóa. Quá trình này bao gồm việc điều phối và giám sát toàn bộ vòng đời container—từ triển khai, mở rộng, đến vận hành—để đảm bảo hiệu năng và tính nhất quán trên nhiều môi trường khác nhau.\nAWS container services cung cấp quản lý container được AWS vận hành toàn phần, kết hợp giữa công nghệ gốc của AWS và mã nguồn mở, mang lại nhiều lựa chọn triển khai từ Kubernetes cho đến bộ điều phối native.\nBạn có thể sử dụng Amazon Elastic Container Service (Amazon ECS) và Amazon Elastic Kubernetes Service (Amazon EKS). cả hai đều có thể chạy với AWS Fargate để triển khai container serverless. Ngoài ra, EKS Auto Mode đơn giản hóa việc quản lý Kubernetes bằng cách tự động cung cấp hạ tầng, chọn instance tối ưu, và mở rộng tài nguyên động cho ứng dụng container.\nĐể kết nối hạ tầng on-premises hoặc edge với dịch vụ container của AWS, bạn có thể dùng EKS Hybrid Nodes and ECS Anywhere, hoặc dùng EKS Anywhere cho trải nghiệm Kubernetes tách biệt hoàn toàn nhưng vẫn được AWS hỗ trợ. Với các tùy chọn compute và triển khai linh hoạt, bạn có thể giảm chi phí vận hành, tập trung đổi mới, và mang lại giá trị kinh doanh nhanh hơn.\nTruy cập vào 2025 Gartner Magic Quadrant for Container Management để xem thêm thông tin.\n— Channy\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Chuẩn bị tài nguyên",
	"tags": [],
	"description": "",
	"content": "Để chuẩn bị cho phần này của workshop, bạn sẽ cần phải:\nTriển khai CloudFormation stack Sửa đổi bảng định tuyến VPC. Các thành phần này hoạt động cùng nhau để mô phỏng DNS forwarding và name resolution.\nTriển khai CloudFormation stack Mẫu CloudFormation sẽ tạo các dịch vụ bổ sung để hỗ trợ mô phỏng môi trường truyền thống:\nMột Route 53 Private Hosted Zone lưu trữ các bản ghi Bí danh (Alias records) cho điểm cuối PrivateLink S3 Một Route 53 Inbound Resolver endpoint cho phép \u0026ldquo;VPC Cloud\u0026rdquo; giải quyết các yêu cầu resolve DNS gửi đến Private Hosted Zone Một Route 53 Outbound Resolver endpoint cho phép \u0026ldquo;VPC On-prem\u0026rdquo; chuyển tiếp các yêu cầu DNS cho S3 sang \u0026ldquo;VPC Cloud\u0026rdquo; Click link sau để mở AWS CloudFormation console. Mẫu yêu cầu sẽ được tải sẵn vào menu. Chấp nhận tất cả mặc định và nhấp vào Tạo stack. Có thể mất vài phút để triển khai stack hoàn tất. Bạn có thể tiếp tục với bước tiếp theo mà không cần đợi quá trình triển khai kết thúc.\nCập nhật bảng định tuyến private on-premise Workshop này sử dụng StrongSwan VPN chạy trên EC2 instance để mô phỏng khả năng kết nối giữa trung tâm dữ liệu truyền thống và môi trường cloud AWS. Hầu hết các thành phần bắt buộc đều được cung cấp trước khi bạn bắt đầu. Để hoàn tất cấu hình VPN, bạn sẽ sửa đổi bảng định tuyến \u0026ldquo;VPC on-prem\u0026rdquo; để hướng lưu lượng đến cloud đi qua StrongSwan VPN instance.\nMở Amazon EC2 console\nChọn instance tên infra-vpngw-test. Từ Details tab, copy Instance ID và paste vào text editor của bạn để sử dụng ở những bước tiếp theo\nĐi đến VPC menu bằng cách gõ \u0026ldquo;VPC\u0026rdquo; vào Search box\nClick vào Route Tables, chọn RT Private On-prem route table, chọn Routes tab, và click Edit Routes.\nClick Add route. Destination: CIDR block của Cloud VPC Target: ID của infra-vpngw-test instance (bạn đã lưu lại ở bước trên) Click Save changes "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Báo cáo Tóm tắt: “Generative AI with Amazon Bedrock” Mục tiêu của sự kiện Tìm hiểu Amazon Bedrock là gì và các mô hình AI đang được sử dụng Học sâu hơn về Prompt Engineering: Zero-Shot, Few-Shot, Chain of Thought (CoT) Tìm hiểu Retrieval Augmented Generation và các trường hợp sử dụng RAG Khám phá thêm các dịch vụ AI pretrained Tìm hiểu Amazon Bedrock AgentCore và xem demo trực tiếp Diễn giả Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Các điểm nổi bật Foundation Models Prompt Engineering Retrieval-Augmented Generation (RAG) Bedrock AgentCore Foundation Models Mô hình ML truyền thống Foundation Models Mục đích Hẹp, chuyên biệt từng tác vụ (classification, regression, forecasting, clustering,\u0026hellip;) Đa dụng, đa miền, đa tác vụ Dữ liệu huấn luyện Thường là dữ liệu structured/tabular hoặc domain-specific Tập dữ liệu khổng lồ (text, image, code,\u0026hellip;) Yêu cầu huấn luyện Phải huấn luyện thủ công với dữ liệu gán nhãn Không cần — đã được pretrained Tùy chỉnh Cần tinh chỉnh hyperparameter, feature, dataset Tùy chọn — fine-tuning, RAG, prompt engineering Ví dụ Random Forest, XGBoost, SVM, Linear Regression Claude, Llama, Cohere Command, Titan Kỹ năng cần thiết Chuyên môn ML cao (feature engineering, training pipeline) Rất thấp — chỉ cần viết prompt Dịch vụ AWS SageMaker, Amazon ML, custom training Amazon Bedrock, Bedrock Studio Use cases Dự đoán, Forecasting doanh số, Phân loại ảnh, Fraud detection, anomaly detection Chatbot, Tóm tắt văn bản, Sinh mã, Sinh ảnh, Ứng dụng hội thoại, Tìm kiếm tài liệu với RAG, Sentiment analysis, Dịch đa ngôn ngữ Supported foundation models in Amazon Bedrock\nPrompt Engineering - Tạo và tinh chỉnh hướng dẫn (prompt) Prompt là gì? Là đầu vào bạn cung cấp cho mô hình AI (như ChatGPT, Claude, Gemini, hoặc các mô hình trên Amazon Bedrock) để nhận được đầu ra mong muốn. Ví dụ Sự khác nhau giữa Zero-Shot, Few-Shot và Chain of Thought trong Prompting Techniques Kỹ thuật Mô tả Khi nào dùng Ưu điểm Hạn chế Ví dụ Zero-Shot Model chỉ nhận instruction, không có ví dụ mẫu Nhiệm vụ đơn giản; model đã quen miền kiến thức Prompt ngắn, nhanh, không cần chuẩn bị nhiều Chính xác thấp với tác vụ phức tạp \u0026ldquo;Hãy phân loại đoạn văn thành neutral, negative hoặc positive.\u0026rdquo; Few-Shot Cung cấp một vài ví dụ để dạy model pattern Yêu cầu output theo format riêng; tác vụ theo mẫu Chính xác cao hơn; model học phong cách bạn muốn Prompt dài hơn; tốn token Ví dụ có mô tả về \u0026ldquo;whatpu\u0026rdquo; Chain-of-Thought Yêu cầu model giải thích từng bước Bài toán logic, toán, code, multi-step Lý luận tốt hơn, output chính xác hơn Chậm hơn; trả lời dài \u0026ldquo;Hãy cho thấy từng bước suy luận.\u0026rdquo; Retrieval Augmented Generation (RAG) RAG là gì? Retrieval: Truy xuất dữ liệu liên quan từ nguồn ngoài (database, document store, API,…) Augmentation: Thêm ngữ cảnh vào prompt gửi cho mô hình Generation: Mô hình trả lời dựa trên ngữ cảnh đã bổ sung Use cases RAG Cải thiện chất lượng nội dung: giảm hallucination, dùng kiến thức cập nhật Chatbot theo ngữ cảnh: tích hợp dữ liệu doanh nghiệp Tìm kiếm cá nhân hóa Tóm tắt dữ liệu real-time Embeddings là gì? Biểu diễn văn bản dưới dạng vector số Giúp mô hình hiểu ngữ nghĩa và mối quan hệ giữa các từ So sánh độ tương đồng giữa các đoạn văn Hỗ trợ đa ngôn ngữ Amazon Titan Embedding RAG in Action Data Ingestion Workflow RetrieveAndGenerate API Các dịch vụ AI Pretrained khác (hình ảnh Amazon Polly)\n(hình ảnh Amazon Comprehend)\n(hình ảnh Amazon Kendra)\n(hình ảnh Amazon Lookout Family)\n(hình ảnh Amazon Personalize)\nAmazon Bedrock AgentCore Sự phát triển của Agentic Generative AI assistants: theo quy tắc có sẵn, tự động hoá tác vụ lặp lại Generative AI agents: đạt mục tiêu, xử lý tác vụ rộng hơn, tự động hoá quy trình hoàn chỉnh Agentic AI systems: đa agent, tự điều phối, mô phỏng tư duy con người Framework xây dựng agent Strands agents SDK Langgraph, Langchain OpenAI Agents SDK Crew.AI Google ADK LlamaIndex “Vực sâu” từ prototype → production AgentCore — giải pháp giúp agent vận hành ở quy mô lớn Trải nghiệm Sự kiện (Event Experience) Tham dự workshop “Generative AI with Amazon Bedrock” mang lại nhiều giá trị giúp tôi hiểu sâu hơn cách AI hiện đại và các công nghệ cloud được ứng dụng trong thực tế.\nHọc hỏi từ chuyên gia Diễn giả chia sẻ các tình huống thực tế về ứng dụng AI trong doanh nghiệp Hiểu vì sao foundation models, RAG và agentic workflow quan trọng đối với scalability Trải nghiệm kỹ thuật thực hành Học trực tiếp về Zero-Shot / Few-Shot / Chain-of-Thought Demo RAG giúp thấy rõ cách giảm hallucination Quan sát Bedrock AgentCore vận hành tool, lập kế hoạch, xử lý quy trình Hiểu thêm về hiện đại hóa hệ thống Nắm rõ sự khác nhau giữa ML truyền thống và foundation models Hiểu các mô hình hiện đại hóa như Event-driven Architecture, DDD, AI-augmented workflows Khám phá công cụ AI hiện đại Titan Embedding, Textract, Comprehend, Kendra, Polly… Amazon Q Developer hỗ trợ coding, debugging, refactoring Mở rộng kết nối Giao lưu với cloud engineers, AI specialists, developers Hiểu thêm các use cases thực tế trong doanh nghiệp Những điều rút ra RAG là chìa khóa cho AI doanh nghiệp Prompt engineering rất quan trọng Agentic AI là tương lai của workflow automation Hiện đại hoá hệ thống cần roadmap rõ ràng + đo lường ROI AI trong developer workflow tăng năng suất đáng kể "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.1-workshop-overview/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Giới thiệu về VPC Endpoint Điểm cuối VPC (endpoint) là thiết bị ảo. Chúng là các thành phần VPC có thể mở rộng theo chiều ngang, dự phòng và có tính sẵn sàng cao. Chúng cho phép giao tiếp giữa tài nguyên điện toán của bạn và dịch vụ AWS mà không gây ra rủi ro về tính sẵn sàng. Tài nguyên điện toán đang chạy trong VPC có thể truy cập Amazon S3 bằng cách sử dụng điểm cuối Gateway. Interface Endpoint PrivateLink có thể được sử dụng bởi tài nguyên chạy trong VPC hoặc tại TTDL. Tổng quan về workshop Trong workshop này, bạn sẽ sử dụng hai VPC.\n\u0026ldquo;VPC Cloud\u0026rdquo; dành cho các tài nguyên cloud như Gateway endpoint và EC2 instance để kiểm tra. \u0026ldquo;VPC On-Prem\u0026rdquo; mô phỏng môi trường truyền thống như nhà máy hoặc trung tâm dữ liệu của công ty. Một EC2 Instance chạy phần mềm StrongSwan VPN đã được triển khai trong \u0026ldquo;VPC On-prem\u0026rdquo; và được cấu hình tự động để thiết lập đường hầm VPN Site-to-Site với AWS Transit Gateway. VPN này mô phỏng kết nối từ một vị trí tại TTDL (on-prem) với AWS cloud. Để giảm thiểu chi phí, chỉ một phiên bản VPN được cung cấp để hỗ trợ workshop này. Khi lập kế hoạch kết nối VPN cho production workloads của bạn, AWS khuyên bạn nên sử dụng nhiều thiết bị VPN để có tính sẵn sàng cao. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/",
	"title": "Nhật ký công việc",
	"tags": [],
	"description": "",
	"content": "Tuần 1: Làm quen với AWS và các dịch vụ cơ bản trong AWS\nTuần 2: Làm công việc với Amazon RDS,AWS DynamoDB, LightSail và EC2 Auto Scaling\nTuần 3: Học về AWS Security\nTuần 4: Học về AWS DynamoDB và Relational Database Services\nTuần 5: Phân quyền và lưu trữ\nTuần 6: Mã hóa dữ liệu\nTuần 7: Tối ưu hiệu năng\nTuần 8: setup networking, Phân quyền, và chia sẻ file\nTuần 9: Thực hành với bảo mật và vận hành\nTuần 10: Học về quan sát hiệu năng\nTuần 11: Học về Redis và những dịch vụ bảo mật\nTuần 12: Làm công việc N\u0026hellip;\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/4-eventparticipated/4.1-event1/",
	"title": "Sự kiện 1",
	"tags": [],
	"description": "",
	"content": "Báo cáo tổng kết: “Data Science on AWS” Mục tiêu sự kiện Khám phá cách AWS giải quyết các bài toán dữ liệu bằng dịch vụ của mình Giới thiệu tổng quan về các Managed AI Services và các trường hợp sử dụng thực tế Chuẩn bị dữ liệu với Amazon SageMaker Ứng dụng XGBoost trong SageMaker Studio Notebooks Khai thác AutoML không cần code với SageMaker Canvas Diễn giả Văn Hoàng Kha – Cloud Solutions Architect, AWS User Group Leader Bạch Doãn Vương – Cloud DevOps Engineer, AWS Community Builder Đoàn Nguyễn Thanh Hòa – Giảng viên CF, Đại học FPT TP.HCM Nội dung nổi bật (Key Highlights) Amazon Comprehend và Amazon Translate Phân tích và dịch văn bản bằng công nghệ học sâu (Deep Learning)\nXử lý nhiều loại tài liệu như email, chat, mạng xã hội, cuộc gọi\u0026hellip; và tự động trích xuất thông tin hữu ích. Các trường hợp sử dụng phổ biến của Amazon Comprehend: Xử lý tài liệu thông minh Tự động hóa quy trình email Phân loại và định tuyến ticket hỗ trợ khách hàng Gắn thẻ tài liệu và nội dung media Phân tích cảm xúc khách hàng Phân tích cuộc gọi tổng đài Phát hiện và ẩn thông tin nhạy cảm (PII) Amazon Translate Dịch máy thần kinh (Neural Machine Translation)\nTính năng chính:\nHỗ trợ ngôn ngữ rộng: 4970 cặp ngôn ngữ dịch X↔Y Độ trễ thấp: \u0026lt;150ms cho mỗi câu Bảo mật dữ liệu: mã hóa và quản lý truy cập đầy đủ Phủ sóng khu vực rộng: 17 vùng AWS Tùy chỉnh dịch: dùng Custom Terminologies và Active Custom Translation Dịch hàng loạt: hỗ trợ định dạng DOCX, PPTX, XLSX, XML, HTML, TXT Mô hình huấn luyện đa lĩnh vực: 11 domain khác nhau Tính phí theo sử dụng: dễ tích hợp qua API Trường hợp sử dụng:\nBản địa hóa nội dung: tài liệu doanh nghiệp, phụ đề video, lưu trữ Giao tiếp: tương tác khách hàng, chat trong game, bài viết mạng xã hội Phân tích văn bản: Voice of Customer, phân tích media, eDiscovery Amazon Polly Dịch vụ chuyển văn bản thành giọng nói (Text-to-Speech)\nAmazon Polly sử dụng công nghệ học sâu để tổng hợp giọng nói tự nhiên như con người.\nTính năng:\nText-to-Speech (TTS) Ngôn ngữ đánh dấu SSML Tùy chỉnh từ vựng (Lexicons) Dấu giọng nói (Speech Marks) Tạo giọng thương hiệu (Brand Voice) Ứng dụng thực tế:\nĐọc tin tức, tài liệu đào tạo Tổng đài thoại/IVR Podcast, học ngoại ngữ Dẫn đường, nhắc việc, công cụ hỗ trợ người khuyết tật Amazon Transcribe Dịch vụ nhận dạng giọng nói tự động (ASR)\nChuyển nội dung âm thanh/video thành văn bản Hỗ trợ cả ghi âm sẵn và phát trực tiếp theo thời gian thực Amazon Lex Dịch vụ xây dựng chatbot và giao diện hội thoại thông minh\nTính năng:\nDễ sử dụng Hiểu ngôn ngữ tự nhiên chính xác Tích hợp sẵn với hệ sinh thái AWS Tiết kiệm chi phí Amazon Rekognition Phân tích hình ảnh và video để phát hiện đối tượng, khuôn mặt, cảnh vật và nội dung không phù hợp.\nAmazon Personalize Cá nhân hóa trải nghiệm người dùng\nTriển khai hệ thống gợi ý nhanh chóng Phản ứng theo hành vi người dùng theo thời gian thực Dễ dàng tích hợp với hệ thống hiện có Giảm thời gian ra thị trường nhờ dịch vụ ML được quản lý Feature Engineering Chuẩn bị dữ liệu với Amazon SageMaker Canvas Ứng dụng vào công việc — Bài học rút ra từ Key Highlights 1. Hiểu dữ liệu \u0026amp; Tự động hóa (Amazon Comprehend, Translate) Bài học:\nKhai thác dữ liệu phi cấu trúc là nền tảng cho việc ra quyết định thông minh.\nỨng dụng:\nSử dụng Amazon Comprehend để phân tích cảm xúc khách hàng, phân loại tài liệu hoặc email. Tự động định tuyến ticket hỗ trợ dựa trên nội dung và cảm xúc. Dùng Amazon Translate để dịch nhanh tài liệu trong các dự án đa ngôn ngữ. 2. Tăng tương tác \u0026amp; Giao diện thoại (Amazon Polly, Lex, Transcribe) Bài học:\nAI giọng nói giúp nâng cao trải nghiệm người dùng và khả năng tiếp cận dịch vụ.\nỨng dụng:\nKết hợp Lex + Transcribe + Polly để xây dựng chatbot hỗ trợ khách hàng bằng giọng nói 24/7. Sử dụng Polly tạo giọng đọc tự nhiên cho nội dung đào tạo hoặc podcast. Dùng Transcribe để ghi âm, phân tích và tóm tắt nội dung cuộc họp hoặc cuộc gọi. 3. Phân tích hình ảnh \u0026amp; video (Amazon Rekognition) Bài học:\nThị giác máy tính giúp tự động hóa việc phân tích và kiểm duyệt nội dung đa phương tiện.\nỨng dụng:\nÁp dụng Rekognition để tự động gắn thẻ, phân loại hình ảnh/video. Phát hiện khuôn mặt hoặc hành vi bất thường trong hệ thống an ninh hoặc bán lẻ. 4. Cá nhân hóa trải nghiệm người dùng (Amazon Personalize) Bài học:\nCá nhân hóa là yếu tố cốt lõi để giữ chân người dùng và nâng cao trải nghiệm.\nỨng dụng:\nTích hợp Personalize để gợi ý sản phẩm/dịch vụ theo hành vi người dùng. Xây dựng hệ thống gợi ý phản hồi theo thời gian thực, thích ứng liên tục. 5. Đơn giản hóa Machine Learning (Amazon SageMaker, Canvas) Bài học:\nMachine Learning không còn là đặc quyền của chuyên gia — AWS giúp AI trở nên dễ tiếp cận.\nỨng dụng:\nDùng SageMaker Canvas để huấn luyện và dự đoán mà không cần viết code. Ứng dụng SageMaker Studio để thử nghiệm các mô hình như XGBoost. Hiểu rõ vai trò của Feature Engineering để cải thiện độ chính xác của mô hình. 6. Tư duy hiện đại hóa dựa trên dữ liệu Bài học:\nCác dịch vụ AI của AWS thúc đẩy tư duy kiến trúc hướng sự kiện (event-driven) và tập trung vào dữ liệu.\nỨng dụng:\nKết hợp Domain-Driven Design (DDD) với quy trình AI để xây dựng hệ thống linh hoạt, dễ mở rộng. Sử dụng serverless (Lambda, API Gateway) để triển khai nhanh các pipeline AI. Thiết kế hệ thống xử lý dữ liệu bất đồng bộ theo mô hình event streaming để tăng hiệu năng. 🌟 Tổng kết Qua hội thảo “Data Science on AWS”, tôi không chỉ học về từng dịch vụ AI riêng lẻ mà còn hiểu được cách liên kết chúng thành một hệ sinh thái dữ liệu thông minh.\nMỗi công cụ — từ Comprehend, Translate, Polly, Rekognition, Personalize đến SageMaker — đều góp phần tạo nên hệ thống tự động, linh hoạt và hướng người dùng.\nNhững kiến thức và bài học này có thể áp dụng trực tiếp vào dự án thực tế, giúp chuyển đổi quy trình thủ công sang mô hình vận hành thông minh dựa trên AI và dữ liệu.\nHình ảnh sự kiện (Thêm ảnh sự kiện của bạn tại đây)\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Tạo một Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": " Mở Amazon VPC console Trong thanh điều hướng, chọn Endpoints, click Create Endpoint: Bạn sẽ thấy 6 điểm cuối VPC hiện có hỗ trợ AWS Systems Manager (SSM). Các điểm cuối này được Mẫu CloudFormation triển khai tự động cho workshop này.\nTrong Create endpoint console: Đặt tên cho endpoint: s3-gwe Trong service category, chọn aws services Trong Services, gõ \u0026ldquo;s3\u0026rdquo; trong hộp tìm kiếm và chọn dịch vụ với loại gateway Đối với VPC, chọn VPC Cloud từ drop-down menu. Đối với Route tables, chọn bảng định tuyến mà đã liên kết với 2 subnets (lưu ý: đây không phải là bảng định tuyến chính cho VPC mà là bảng định tuyến thứ hai do CloudFormation tạo). Đối với Policy, để tùy chọn mặc định là Full access để cho phép toàn quyền truy cập vào dịch vụ. Bạn sẽ triển khai VPC endpoint policy trong phần sau để chứng minh việc hạn chế quyền truy cập vào S3 bucket dựa trên các policies. Không thêm tag vào VPC endpoint. Click Create endpoint, click x sau khi nhận được thông báo tạo thành công. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.1-week1/",
	"title": "Worklog Tuần 1",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 1 Làm quen với các thành viên trong First Cloud Journey. Hiểu các dịch vụ cơ bản của AWS và cách sử dụng AWS Console \u0026amp; CLI. Các nhiệm vụ thực hiện trong tuần Ngày Nhiệm vụ Bắt đầu Hoàn thành Tài liệu tham khảo 2 - Làm quen với thành viên FCJ.\n- Tìm hiểu nội quy trong quá trình đào tạo.\nThực hành:\n+ Tạo tài khoản AWS Free Tier 08/09/2025 09/09/2025 — 3 Học về AWS và các dịch vụ AWS:\n+ EC2\n+ Lambda\n+ SQS\n+ SNS\n+ CLI \u0026amp; SDKs 09/09/2025 10/09/2025 — 4 Học về ECS, EKS, VPC, CloudFormation.\nHọc Cost Management.\nThực hành:\n+ Tạo EC2 instance\n+ Tạo Lambda function\n+ Sử dụng AWS CLI 10/09/2025 11/09/2025 — 5 Học VPC cơ bản và S3.\nThực hành:\n+ Tạo Security Group\n+ Tạo Internet Gateway\n+ Tạo Subnet\n+ Tạo Route Table 11/09/2025 12/09/2025 — 6 Thực hành:\n+ Tạo S3 Bucket 12/09/2025 13/09/2025 — Kết quả đạt được trong tuần 1. Kiến thức nền tảng AWS Hiểu được các nhóm dịch vụ chính của AWS: Compute, Storage, Networking, Database. Tạo và cấu hình thành công tài khoản AWS Free Tier. Làm quen với AWS Management Console và biết cách truy cập, tìm kiếm và sử dụng các dịch vụ. 2. Sử dụng AWS CLI Thực hiện được các thao tác cơ bản:\nKiểm tra thông tin tài khoản \u0026amp; cấu hình. Xem danh sách region. Lấy thông tin dịch vụ EC2. Tạo và quản lý Key Pair. Kiểm tra trạng thái dịch vụ đang chạy. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.2-week2/",
	"title": "Worklog Tuần 2",
	"tags": [],
	"description": "",
	"content": "Mục tiêu tuần 2 Tìm hiểu về Amazon RDS, AWS S3, AWS EC2 Auto Scaling, DynamoDB và Amazon Lightsail. Các nhiệm vụ thực hiện trong tuần Ngày Nhiệm vụ Bắt đầu Hoàn thành Tài liệu tham khảo 2 - Học AWS DynamoDB, AWS RDS, Amazon Aurora.\nThực hành:\n+ Tạo CloudFront và S3 14/09/2025 15/09/2025 https://render.skillbuilder.aws 3 Thực hành:\n+ Tạo AWS RDS\n+ Tạo DynamoDB bằng Python và AWS CLI 15/09/2025 16/09/2025 https://render.skillbuilder.aws 4 Dịch Blog 13/09/2025 13/09/2025 https://cloudjourney.awsstudygroup.com/ 5 Học EC2 Auto Scaling, Amazon Lightsail.\nThực hành:\n+ Tạo Database trên RDS\n+ Sử dụng Amazon Lightsail 19/09/2025 20/09/2025 https://000005.awsstudygroup.com/ https://000045.awsstudygroup.com/ 6 Thực hành:\n+ Tạo RDS, VPC, EC2 với EC2 Auto Scaling 20/09/2025 21/09/2025 https://000006.awsstudygroup.com/ Kết quả đạt được trong tuần 1. Static Website Hosting với Amazon S3 Biết cách tạo S3 Bucket, upload file và quản lý truy cập. Cấu hình Static Website Hosting để bucket hoạt động như một website. Thiết lập quyền public giúp website được truy cập từ Internet. Cấu hình Bucket Policy để chỉ cho phép đọc công khai (public read-only). 2. Kiến thức về Amazon RDS – Dịch vụ cơ sở dữ liệu quan hệ Hỗ trợ các hệ quản trị phổ biến: MySQL, PostgreSQL, Oracle, SQL Server. Biết cách kết nối RDS từ EC2. Nắm rõ endpoint, port, username/password để đăng nhập DB. Tự tạo và cấu hình VPC, subnet, security group để bảo vệ RDS Instance. 3. Mở rộng ứng dụng với EC2 Auto Scaling Hiểu EC2 Auto Scaling là dịch vụ tự động tăng/giảm số lượng EC2 dựa trên nhu cầu thực tế. Thành phần chính của Auto Scaling: Launch Configuration / Launch Template Auto Scaling Group (ASG) Scaling Policies: Dynamic Scaling và Scheduled Scaling Health Checks Biết cách sử dụng Amazon CloudWatch để thu thập các metric như CPU Utilization, Network Traffic, Request Count… giúp ASG đưa ra quyết định scale hợp lý. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.3-week3/",
	"title": "Worklog Tuần 3",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 3 Hiểu các khái niệm bảo mật cơ bản của AWS. Tìm hiểu các dịch vụ bảo mật quan trọng như AWS KMS, Amazon Macie, AWS Certificate Manager. Hiểu cách bảo vệ hạ tầng AWS. Công việc đã thực hiện trong tuần Ngày Công việc Bắt đầu Hoàn thành Tài liệu tham khảo 2 Tìm hiểu cách bảo vệ dữ liệu bằng các cơ chế sẵn có của AWS (S3, EBS, DynamoDB) và các dịch vụ bảo mật (KMS, Macie, Certificate Manager). 22/09/2025 23/09/2025 Link SkillBuilder 3 Tìm hiểu cách bảo vệ mạng và ứng dụng thông qua bảo mật hạ tầng (Security Groups, ELB, Regions) và dịch vụ bảo vệ (AWS Shield, AWS WAF). 23/09/2025 24/09/2025 Link SkillBuilder 4 Tìm hiểu quy trình phát hiện và phản hồi sự cố bảo mật với Amazon Inspector, GuardDuty, Amazon Detective, AWS Security Hub. 24/09/2025 25/09/2025 Link SkillBuilder 5 Tìm hiểu cách ngăn truy cập trái phép bằng các dịch vụ như IAM Identity Center, Secrets Manager, Systems Manager. 25/09/2025 26/09/2025 Link SkillBuilder 6 Thực hành: + Cấu hình liên kết danh tính (Identity Federation) với AWS Single Sign-On. 26/09/2025 29/09/2025 https://000012.awsstudygroup.com/ Kết quả đạt được 1. Bảo vệ dữ liệu và quản trị bảo mật Hiểu: Mã hóa \u0026amp; quản lý khóa bằng AWS KMS Quét dữ liệu nhạy cảm với Amazon Macie Quản lý chứng chỉ bằng AWS Certificate Manager Các chế độ bảo mật có sẵn của S3, EBS, DynamoDB 2. Bảo mật mạng và ứng dụng Nắm được các khái niệm bảo mật nhiều lớp: Security Groups Elastic Load Balancer Chiến lược bảo vệ đa vùng (Multi-Region) Hiểu cách AWS Shield và AWS WAF giảm thiểu tấn công DDoS và tấn công web. 3. Phát hiện \u0026amp; ứng phó sự cố bảo mật Hiểu vai trò của: Amazon Inspector – kiểm tra lỗ hổng Amazon GuardDuty – phát hiện mối đe dọa Amazon Detective – phân tích điều tra AWS Security Hub – quản lý tuân thủ \u0026amp; tổng hợp cảnh báo 4. Ngăn truy cập trái phép Hiểu cơ bản về: IAM Identity Center (SSO) Secrets Manager Systems Manager "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.4-week4/",
	"title": "Worklog tuần 4",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 4 Luyện tập tạo, quản lý và triển khai tài nguyên AWS thông qua nhiều công cụ (Console, CLI, SDK, Elastic Beanstalk\u0026hellip;). Xây dựng một ứng dụng web mẫu sử dụng Lambda, S3, DynamoDB và API Gateway. Công việc đã thực hiện trong tuần Ngày Công việc Bắt đầu Hoàn thành Tài liệu tham khảo 2 Thực hành: Tạo bảng trong AWS DynamoDB bằng AWS CLI hoặc Python SDK thông qua Access Key. 28/09/2025 29/09/2025 3 Thực hành: Xây dựng ứng dụng Book Store sử dụng Lambda, S3 và DynamoDB. 30/09/2025 01/10/2025 https://000078.awsstudygroup.com/ 4 Học mô hình dữ liệu JSON \u0026amp; Document. 01/10/2025 02/10/2025 5 Thực hành: + Triển khai và kiểm tra tài nguyên AWS bằng CloudFormation Template.\n+ Sử dụng AWS Tools for Eclipse để deploy ứng dụng Java lên Elastic Beanstalk.\n+ Cài đặt và cấu hình Elastic Beanstalk CLI.\n+ Dùng EB CLI để deploy bản cập nhật cho môi trường có sẵn.\n+ Dùng AWS SDK để truy vấn và chỉnh sửa môi trường AWS bằng code. 03/10/2025 04/10/2025 https://000050.awsstudygroup.com/ 6 Thực hành: Xây dựng web frontend kết nối đến database qua Lambda và API Gateway. 04/10/2025 05/10/2025 https://000079.awsstudygroup.com/ Kết quả đạt được 1. Làm việc với AWS DynamoDB Tạo và quản lý các bảng DynamoDB bằng CLI và Python SDK. Hiểu cách định nghĩa Partition Key, Sort Key và cấu hình Read/Write Capacity. 2. Xây dựng ứng dụng Book Store (Lambda + S3 + DynamoDB) Tạo Lambda function xử lý CRUD cho dữ liệu sách. Kết nối API Gateway với Lambda để xây dựng REST API. Lưu ảnh bìa sách và file tĩnh trong Amazon S3. 3. Hiểu mô hình JSON \u0026amp; Document Nắm cách DynamoDB lưu trữ dữ liệu bán cấu trúc. Biết cách truy vấn và cập nhật dữ liệu ở dạng JSON. 4. Triển khai ứng dụng Java với Elastic Beanstalk Triển khai hạ tầng qua Console và CloudFormation. Cài đặt EB CLI và deploy bản cập nhật. Sử dụng AWS SDK để thao tác với tài nguyên AWS bằng mã nguồn Java. 5. Xây dựng frontend kết nối backend serverless Tạo giao diện cho phép thêm, sửa, xóa dữ liệu trong DynamoDB. Hiểu quy trình: Fr "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.5-week5/",
	"title": "Worklog tuần 5",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 5 Khám phá và thực hành xác thực người dùng và lưu trữ dữ liệu sử dụng AWS Amplify. Hiểu và triển khai ứng dụng serverless sử dụng AWS SAM (Serverless Application Model). Học và triển khai xác thực người dùng với Amazon Cognito. Nhiệm vụ đã hoàn thành trong tuần Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học AWS Amplify. 05/10/2025 06/10/2025 https://aws.amazon.com/vi/amplify/ 3 Thực hành: Sử dụng Amplify Authentication và Amplify Storage cho ứng dụng serverless. 06/10/2025 09/10/2025 https://000134.awsstudygroup.com/ 4 Học AWS SAM: + Cấu trúc template SAM và SAM CLI + SAM Accelerate và tích hợp SAM CLI 09/10/2025 10/10/2025 https://aws.amazon.com/vi/serverless/sam/ 5 Thực hành: + Cài đặt SAM CLI + Triển khai frontend và Lambda function + Cấu hình API Gateway và kiểm thử API bằng Postman 10/10/2025 11/10/2025 https://000080.awsstudygroup.com/ 6 Học Amazon Cognito và Thực hành: Triển khai xác thực người dùng với Cognito. 11/10/2025 13/10/2025 https://000081.awsstudygroup.com/ Thành tựu Tuần 5 1. Làm việc với AWS Amplify Hiểu kiến trúc Amplify và cách tích hợp với hệ thống serverless. Triển khai Amplify Authentication cho luồng đăng ký và đăng nhập người dùng. Sử dụng Amplify Storage để quản lý upload file và truy cập dữ liệu trên Amazon S3. Quản lý môi trường Amplify bằng CLI và tích hợp với ứng dụng frontend. 2. Học và áp dụng AWS SAM (Serverless Application Model) Hiểu cấu trúc template SAM; nắm cách SAM sử dụng CloudFormation. Cài đặt và cấu hình SAM CLI cho phát triển và triển khai cục bộ. Thực hành triển khai ứng dụng serverless sử dụng SAM Accelerate. Triển khai Lambda function và kiểm thử endpoint API qua API Gateway bằng Postman. Hiểu quy trình làm việc: SAM → Lambda → API Gateway → CloudFormation. 3. Học và thực hành Amazon Cognito Hiểu Cognito User Pools (xác thực) và Identity Pools (phân quyền + truy cập dịch vụ AWS). Triển khai luồng xác thực người dùng bằng Cognito. Tích hợp Cognito với AWS Amplify để xác thực liền mạch trên frontend. Thử nghiệm thành công các luồng: đăng ký, đăng nhập, xác nhận và xác thực token. 4. Kết hợp AWS Amplify, SAM và Cognito Xây dựng ứng dụng serverless hoàn chỉnh, hỗ trợ xác thực người dùng, tương tác API và triển khai trên cloud. Nâng cao hiểu biết về kiến trúc serverless, chuẩn bị cho các dự án phát triển cloud-native nâng cao trong các tuần tới. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.6-week6/",
	"title": "Worklog tuần 6",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 6 Hiểu và thực hành AWS Backup, bao gồm tạo Backup Vault, Backup Plan, và triển khai cấu hình sao lưu bằng AWS CloudFormation. Tìm hiểu hoạt động của AWS WAF và AWS PrivateLink, gồm các thành phần: ACL, Rules, Rule Groups, VPC Endpoint Services, và Network Load Balancer. Nghiên cứu AWS KMS (Dịch vụ quản lý khóa) — quản lý khóa đối xứng và khóa bất đối xứng, và vai trò của chúng trong mã hóa dữ liệu. Nắm vững Containerization với Docker, cách xây dựng và triển khai ứng dụng sử dụng Docker Images và Docker Compose. Nâng cao kỹ năng Infrastructure as Code (IaC) và triển khai dựa trên container để xây dựng môi trường cloud an toàn và mở rộng. Nhiệm vụ trong tuần Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học AWS Backup: Backup vault, Backup plan, Sử dụng CloudFormation để tạo backup plan 13/10/2025 15/10/2025 https://000013.awsstudygroup.com/ 3 Thực hành: Tạo backup plan, on-demand backup, backup vaults 13/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 4 Học AWS WAF và AWS PrivateLink: ACL, Rules, Rule Groups, VPC Endpoint Service, VPC Endpoint, Network Load Balancer 15/10/2025 17/10/2025 https://000026.awsstudygroup.com/ https://000111.awsstudygroup.com/ 5 Học AWS KMS: Khóa đối xứng, Khóa bất đối xứng; Thực hành: Tạo ACLs, rules và rule groups 17/10/2025 19/10/2025 https://000033.awsstudygroup.com/ 6 Học Containerization với Docker: Docker là gì, Triển khai dùng Docker Image, Triển khai bằng Docker Compose và push image 19/10/2025 21/10/2025 https://000015.awsstudygroup.com/ Thành tựu Tuần 6 Học và thực hành thành công AWS Backup, tạo Backup Vault, Backup Plans, và On-demand Backups qua AWS Console và CloudFormation. Cấu hình và kiểm thử AWS WAF, tạo ACLs, Rules, và Rule Groups để bảo vệ ứng dụng web. Thực hành AWS PrivateLink, triển khai VPC Endpoint Services và VPC Endpoints kết nối qua Network Load Balancer. Thực hành sử dụng AWS KMS, tạo và quản lý cả Khóa đối xứng và Khóa bất đối xứng cho mã hóa dữ liệu. Xây dựng và triển khai ứng dụng container hóa với Docker, bao gồm Docker Images, containers, Docker Compose, và push images lên Docker Hub. Nâng cao hiểu biết về Bảo mật Cloud, Mã hóa, và Containerization, chuẩn bị triển khai ứng dụng thực tế trên AWS Cloud. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.7-week7/",
	"title": "Worklog tuần 7",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 7: Tìm hiểu các dịch vụ AWS tối ưu hiệu năng. Nắm các dịch vụ cần thiết cho tối ưu hiệu năng như ECS, EKS, CodePipeline, Storage Gateway. Tìm hiểu về Docker, Kubernetes và mối quan hệ giữa Docker và Kubernetes. Công việc thực hiện trong tuần: Ngày Công việc Ngày bắt đầu Ngày kết thúc Tài liệu tham khảo 2 - Học AWS EKS: + Control Plane (AWS quản lý) + Worker Nodes (người dùng quản lý) + Các thành phần trong EKS: Cluster, Node Group, Pod, Deployment, Service 20/10/2025 21/10/2025 https://000126.awsstudygroup.com/1-introduce/ 3 - Thực hành: + Tạo mạng: VPC, Subnets, Internet Gateway + Cấu hình Auth cho Control Plane + Tạo EKS Cluster + Cài addon: VPC-CNI, kube-proxy + Tạo Auth cho Worker Node + Tạo Worker Node + Cài addon: CoreDNS + Triển khai Nginx Deployment 21/10/2025 22/10/2025 https://000126.awsstudygroup.com/1-introduce/ 4 - Học AWS ECS: + Cluster, Task Definition, Task, Service, Container Agent + ECS Launch Types + Networking trong ECS + Tích hợp với các dịch vụ khác + ECS Auto Scaling 22/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Học AWS Storage Gateway: - Thực hành: + Tạo ECS Cluster + Cấu hình Docker Image + Tạo Task Definition + Đăng ký Namespace trên Cloud Map 23/10/2025 25/10/2025 https://000016.awsstudygroup.com/1-introduction/ 6 - Học AWS CodePipeline: + Source Stage: Lấy code từ GitHub, CodeCommit hoặc S3 + Build Stage: Gọi CodeBuild để build code + Deploy Stage: Gọi CodeDeploy để deploy ứng dụng 25/10/2025 27/10/2025 https://000017.awsstudygroup.com/ Thành tựu Tuần 7: 1. Học AWS EKS: Hiểu kiến trúc EKS bao gồm Control Plane (AWS quản lý) và Worker Nodes (người dùng quản lý). Hiểu định nghĩa Cluster, Node Group, Pod, Deployment, Service. Hiểu cách kết nối kubectl với EKS Cluster và quản lý workloads. 2. Thực hành trên EKS: Tạo VPC, Subnets, Internet Gateway, Route Table. Cấu hình Authentication cho EKS Control Plane (IAM Role). Tạo EKS Cluster và cài addon: VPC-CNI, kube-proxy. Tạo Node Group cho Worker Node và cài addon CoreDNS. Triển khai thành công Nginx Deployment trên EKS và truy cập qua LoadBalancer. 3. Học AWS ECS: Hiểu cách ECS Cluster hoạt động, Task Definition, Service, Container Agent. Phân biệt ECS Launch Type (EC2 / Fargate). Hiểu cách cấu hình mạng trong ECS (bridge, host, awsvpc). Hiểu cách tích hợp ECS với CloudWatch, Load Balancer và Auto Scaling. 4. Thực hành triển khai ECS: Tạo ECS Cluster (Fargate). Build và push Docker Image lên ECR. Tạo Task Definition và Service chạy container. Đăng ký Namespace trên Cloud Map để ECS có thể thực hiện Service Discovery. Hiểu cơ bản về AWS Storage Gateway và mô hình lưu trữ hybrid. 5. Học AWS CodePipeline: Hiểu pipeline CI/CD gồm 3 stage: Source → Build → Deploy. Hiểu cách tích hợp CodePipeline với GitHub, CodeBuild và CodeDeploy. Hiểu tự động build và deploy ứng dụng Spring Boot hoặc React lên EC2/S3. Viết file buildspec.yml và appspec.yml theo template chuẩn. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.8-week8/",
	"title": "Worklog Tuần 8",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 8 Hiểu cơ bản và các trường hợp sử dụng của AWS Step Functions, bao gồm 7 loại trạng thái chính và cách điều phối các workflow phức tạp. Thực hành tạo và kiểm thử workflow trong AWS Cloud9, tập trung vào điều phối tác vụ và xử lý lỗi. Học các tính năng chính và các bước triển khai của Amazon FSx, bao gồm các biến thể khác nhau (Windows File Server, Lustre, NetApp ONTAP, OpenZFS). Thực hành cấu hình FSx tích hợp với AWS Managed Microsoft AD, đảm bảo cấu hình mạng, xác thực và chia sẻ file đúng. Khám phá AWS X-Ray để theo dõi và trực quan hóa các yêu cầu trong ứng dụng phân tán nhằm tối ưu hiệu năng và debug. Nhiệm vụ tuần này Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học AWS Step Functions: + 7 trạng thái: Task, Choice, Fail/Success, Pass, Wait, Parallel, Map + Các trường hợp sử dụng Step Functions + Lợi ích của Step Functions 26/10/2025 27/10/2025 https://000047.awsstudygroup.com/1-intro/ 3 Thực hành: + Tạo môi trường Cloud9 + Tạo dịch vụ mẫu + Khởi tạo workflow + Xử lý lỗi 27/10/2025 28/10/2025 https://000047.awsstudygroup.com/1-intro/ 4 Học Amazon FSx: + FSx cho Windows File Server + FSx cho Lustre + FSx cho NetApp ONTAP + FSx cho OpenZFS 28/10/2025 29/10/2025 https://000025.awsstudygroup.com/ 5 Thực hành: + Cấu hình chi tiết hệ thống file + Chọn VPC hiện có + Chọn AWS Managed Microsoft AD (cung cấp DNS, username/password của Service Account) + Đặt tên chia sẻ file Windows + Kiểm tra và tạo 29/10/2025 30/10/2025 https://000025.awsstudygroup.com/ 6 Học AWS X-Ray: + Trace + Segment + Subsegment + Annotation / Metadata + Service Map 30/10/2025 31/10/2025 https://000140.awsstudygroup.com/ Thành tựu Tuần 8 Hiểu và giải thích 7 trạng thái của AWS Step Functions, bao gồm vai trò của từng trạng thái trong tự động hóa workflow và chuyển trạng thái. Thực hành tạo workflow mẫu trong Cloud9 với Step Functions, bao gồm xử lý lỗi và xác nhận luồng thực thi. Nắm vững các tùy chọn của Amazon FSx và các trường hợp sử dụng phù hợp cho Windows workloads, HPC và lưu trữ doanh nghiệp. Hoàn thành cấu hình FSx cho Windows File Server, bao gồm cấu hình hệ thống file, tích hợp VPC/AD và tạo chia sẻ file. Học cách AWS X-Ray thu thập trace, segment, subsegment và trực quan hóa Service Map để nhận diện bottleneck và lỗi trong hệ thống. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.9-week9/",
	"title": "Worklog tuần 9",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 9 Hiểu và áp dụng AWS AppSync để xây dựng GraphQL API với nhiều nguồn dữ liệu khác nhau. Học và cấu hình AWS EBS Data Lifecycle Manager để tự động hóa snapshot và backup. Khám phá AWS GuardDuty cho việc phát hiện mối đe dọa thông minh dựa trên ML và phân tích hành vi. Học AWS Macie để nhận diện và bảo vệ dữ liệu nhạy cảm trong S3 bằng machine learning. Nhiệm vụ tuần này Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học AWS AppSync: + GraphQL APIs: Query, Mutation, Subscription + Nguồn dữ liệu: DynamoDB, RDS/Aurora, Lambda, HTTP Endpoints, OpenSearch + Xác thực và phân quyền: IAM, API Keys, Cognito User Pools, OpenID Connect + Hỗ trợ realtime subscription, caching, offline Thực hành: + Tạo GraphQL API + Định nghĩa schema và gắn với nguồn dữ liệu + Cấu hình request/response mapping template 02/11/2025 03/11/2025 https://000086.awsstudygroup.com/1-introduction/ 3 Học AWS EBS Data Lifecycle Manager: + Tự động backup và recovery + Giảm chi phí lưu trữ + Đảm bảo tuân thủ và bảo vệ dữ liệu + Hiểu chính sách Lifecycle: EBS Snapshot Policy, EBS-backed AMI Policy, Cross-region/Cross-account Copy Policy 03/11/2025 05/11/2025 https://000088.awsstudygroup.com/ 4 Thực hành: + Khởi tạo EC2 instance và cấu hình lifecycle policies + Chọn tài nguyên cần backup, lịch trình, và thời gian lưu trữ Học AWS GuardDuty: + Phát hiện mối đe dọa dựa trên ML và phân tích hành vi 03/11/2025 06/11/2025 https://000098.awsstudygroup.com/ 5 Học AWS Macie: + Tính năng chính: Khám phá dữ liệu, phân loại, hiển thị cấp bucket + Cách Macie hoạt động + Trường hợp sử dụng và mô hình giá 14/08/2025 15/08/2025 https://000090.awsstudygroup.com/ 6 Thực hành AWS Macie: + Tạo và cấu hình S3 bucket + Kích hoạt dịch vụ Macie + Tạo các job để quét và phân loại dữ liệu 15/08/2025 15/08/2025 https://000090.awsstudygroup.com/ Thành tựu Tuần 9 AWS AppSync: Xây dựng GraphQL API tích hợp DynamoDB và Lambda, hiểu cách thiết kế schema, resolvers và mapping templates. AWS EBS Lifecycle Manager: Cấu hình chính sách backup tự động, tăng độ tin cậy và tối ưu chi phí trên EC2. AWS GuardDuty: Hiểu cách phát hiện mối đe dọa liên tục dựa trên ML và phân tích hành vi bất thường. AWS Macie: Kích hoạt Macie trên S3, chạy job phân loại dữ liệu và đánh giá các rủi ro dữ liệu nhạy cảm, đảm bảo tuân thủ. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.11-week11/",
	"title": "Báo cáo Tuần 11",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 11 Hiểu và thực hành các dịch vụ AWS: ElastiCache (Redis), Certificate Manager, Inspector, Detective, Systems Manager Tìm hiểu cấu trúc, hoạt động và lợi ích của từng dịch vụ Thực hành kết nối, cấp quyền, triển khai và kiểm tra bảo mật trên AWS Nhiệm vụ tuần này Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Học Amazon ElastiCache - Redis: + Redis là gì + Các thành phần: Cluster, node ElastiCache, Redis shard + Cách hoạt động + Lợi ích khi sử dụng ElastiCache 17/11/2025 18/11/2025 https://000061.awsstudygroup.com/1-introduce/ 3 Học AWS Certificate Manager Thực hành: + Tạo cluster + Cấp quyền truy cập cluster + Kết nối tới node cluster 18/11/2025 19/11/2025 https://000061.awsstudygroup.com/ https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html 4 Học AWS Inspector: + Phát hiện lỗ hổng bảo mật + Tìm cấu hình sai + Thực hiện các bước khắc phục 19/11/2025 20/11/2025 https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html 5 Học AWS Detective 21/11/2025 22/11/2025 https://docs.aws.amazon.com/detective/latest/userguide/what-is-detective.html 6 Thực hành với AWS Systems Manager 22/11/2025 24/11/2025 https://000031.awsstudygroup.com/1-introduce/ Thành tựu Tuần 11 Hoàn thành lý thuyết và thực hành với ElastiCache (Redis): hiểu Redis, cluster, node, shard, cách hoạt động và lợi ích. Thực hành tạo cluster Redis, cấp quyền truy cập và kết nối tới các node. Học và thực hành AWS Certificate Manager: tạo và triển khai chứng chỉ trên cluster. Học và thực hành AWS Inspector: phát hiện lỗ hổng, cấu hình sai và thực hiện các bước khắc phục. Học AWS Detective: phát hiện và điều tra các sự kiện bảo mật. Thực hành AWS Systems Manager: quản lý và vận hành tập trung các tài nguyên AWS. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "IAM permissions Gắn IAM permission policy sau vào tài khoản aws user của bạn để triển khai và dọn dẹp tài nguyên trong workshop này.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Khởi tạo tài nguyên bằng CloudFormation Trong lab này, chúng ta sẽ dùng N.Virginia region (us-east-1).\nĐể chuẩn bị cho môi trường làm workshop, chúng ta deploy CloudFormation template sau (click link): PrivateLinkWorkshop . Để nguyên các lựa chọn mặc định.\nLựa chọn 2 mục acknowledgement Chọn Create stack Quá trình triển khai CloudFormation cần khoảng 15 phút để hoàn thành.\n2 VPCs đã được tạo 3 EC2s đã được tạo "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Kiểm tra Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Tạo S3 bucket Đi đến S3 management console Trong Bucket console, chọn Create bucket Trong Create bucket console Đặt tên bucket: chọn 1 tên mà không bị trùng trong phạm vi toàn cầu (gợi ý: lab\u0026lt;số-lab\u0026gt;\u0026lt;tên-bạn\u0026gt;) Giữ nguyên giá trị của các fields khác (default) Kéo chuột xuống và chọn Create bucket Tạo thành công S3 bucket Kết nối với EC2 bằng session manager Trong workshop này, bạn sẽ dùng AWS Session Manager để kết nối đến các EC2 instances. Session Manager là 1 tính năng trong dịch vụ Systems Manager được quản lý hoàn toàn bởi AWS. System manager cho phép bạn quản lý Amazon EC2 instances và các máy ảo on-premises (VMs)thông qua 1 browser-based shell. Session Manager cung cấp khả năng quản lý phiên bản an toàn và có thể kiểm tra mà không cần mở cổng vào, duy trì máy chủ bastion host hoặc quản lý khóa SSH.\nFirst cloud journey Lab để hiểu sâu hơn về Session manager.\nTrong AWS Management Console, gõ Systems Manager trong ô tìm kiếm và nhấn Enter: Từ Systems Manager menu, tìm Node Management ở thanh bên trái và chọn Session Manager: Click Start Session, và chọn EC2 instance tên Test-Gateway-Endpoint. Phiên bản EC2 này đã chạy trong \u0026ldquo;VPC cloud\u0026rdquo; và sẽ được dùng để kiểm tra khả năng kết nối với Amazon S3 thông qua điểm cuối Cổng mà bạn vừa tạo (s3-gwe).\nSession Manager sẽ mở browser tab mới với shell prompt: sh-4.2 $\nBạn đã bắt đầu phiên kết nối đến EC2 trong VPC Cloud thành công. Trong bước tiếp theo, chúng ta sẽ tạo một S3 bucket và một tệp trong đó.\nCreate a file and upload to s3 bucket Đổi về ssm-user\u0026rsquo;s thư mục bằng lệnh \u0026ldquo;cd ~\u0026rdquo; Tạo 1 file để kiểm tra bằng lệnh \u0026ldquo;fallocate -l 1G testfile.xyz\u0026rdquo;, 1 file tên \u0026ldquo;testfile.xyz\u0026rdquo; có kích thước 1GB sẽ được tạo. Tải file mình vừa tạo lên S3 với lệnh \u0026ldquo;aws s3 cp testfile.xyz s3://your-bucket-name\u0026rdquo;. Thay your-bucket-name bằng tên S3 bạn đã tạo. Bạn đã tải thành công tệp lên bộ chứa S3 của mình. Bây giờ bạn có thể kết thúc session.\nKiểm tra object trong S3 bucket Đi đến S3 console. Click tên s3 bucket của bạn Trong Bucket console, bạn sẽ thấy tệp bạn đã tải lên S3 bucket của mình Tóm tắt Chúc mừng bạn đã hoàn thành truy cập S3 từ VPC. Trong phần này, bạn đã tạo gateway endpoint cho Amazon S3 và sử dụng AWS CLI để tải file lên. Quá trình tải lên hoạt động vì gateway endpoint cho phép giao tiếp với S3 mà không cần Internet gateway gắn vào \u0026ldquo;VPC Cloud\u0026rdquo;. Điều này thể hiện chức năng của gateway endpoint như một đường dẫn an toàn đến S3 mà không cần đi qua pub lic Internet.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "URBAN RAIL TRANSIT SERVICE SYSTEM ON AWS Executive Summary Rapid urban development has increased the need for intelligent and sustainable public transportation. A modern Urban Rail Transit System requires a reliable, scalable, and highly available digital platform operating 24/7.\nThis proposal presents an AWS microservices architecture using Amazon ECS Fargate, enabling:\nTicket booking, scheduling, payments, and traffic monitoring Real-time passenger data ingestion through Amazon Kinesis BI dashboards and predictive analytics with QuickSight and SageMaker Full CI/CD automation and comprehensive monitoring Key Highlights Microservices container architecture powered by Amazon ECS Fargate End-to-end security: Route53 → CloudFront → WAF → ALB → Private Subnets Fully automated CI/CD using CodePipeline, CodeBuild, CodeDeploy, ECR Real-time processing using Kinesis Data Streams Analytics \u0026amp; forecasting with QuickSight + SageMaker Scalability for hundreds of thousands of daily ticket requests 1. Project Objectives Primary Objective Build a digital platform for the Urban Rail Transit System with scalability, security, and long-term operational stability.\nSpecific Objectives Implement ticketing, scheduling, payment, and notification services as microservices Enable automated operations and system-wide monitoring Establish a full CI/CD pipeline with zero downtime Support real-time passenger data ingestion and analytics Provide multi-AZ architecture with ≥ 99.95% system availability 2. Project Scope Component Description Region AWS Singapore (ap-southeast-1) Users Passengers, operators, administrators Architecture Style Microservices on ECS Fargate Phase 1 Ticketing, scheduling, notifications Phase 2 Analytics, BI dashboards, AI forecasting 3. Proposed AWS Architecture 3.1 Architecture Overview A multi-tier microservices architecture will be deployed, including:\nEdge Layer: Route53, CloudFront, AWS WAF Application Layer: ALB → ECS Fargate → ECR Data Layer: RDS SQL Server, ElastiCache Redis Event Layer: EventBridge, SNS, SQS Analytics Layer: Kinesis → S3 → QuickSight → SageMaker Monitoring Layer: CloudWatch, CloudTrail CI/CD Layer: CodePipeline, CodeBuild, CodeDeploy 3.2 Networking \u0026amp; Access Layer Route 53: Global DNS routing CloudFront: CDN caching and low-latency access AWS WAF: Protection from DDoS, SQL injection, XSS Application Load Balancer: Routes traffic to microservices Traffic Flow:\nUser → Route53 → CloudFront → WAF → ALB → Private Subnet → ECS Fargate\n3.3 Application Layer — Microservices on ECS Fargate Why Fargate? No server management Autoscaling based on CPU/Memory High security in private subnets Microservices Booking Service Schedule Service Payment Service Notification Service User Service Staff \u0026amp; Operation Service Docker Images stored in Amazon ECR\n3.4 Data Layer Amazon RDS (SQL Server) Stores tickets, schedules, user accounts, payments Multi-AZ high availability Automated backup and failover ElastiCache Redis Cache schedules → reduce RDS load Increase API performance up to 10× Amazon S3 Stores reports, invoices, files Destination for streaming data from Kinesis 3.5 Event \u0026amp; Messaging Layer Amazon EventBridge Automates workflows, including:\nPaymentSuccess → CreateInvoice → NotifyUser ScheduleUpdate → BroadcastToMobile Amazon SQS Queue system for notifications and ticket processing Protects services during traffic spikes Amazon SNS Sends SMS, email, and push notifications 3.6 Real-Time Analytics Kinesis Data Streams Ingests passenger traffic in real time Application logs → Kinesis → S3 QuickSight Dashboards for daily sales, station load, peak hours SageMaker Predict passenger demand Optimize train frequencies during peak hours 3.7 Monitoring \u0026amp; Observability CloudWatch Metrics: CPU, Memory, ALB latency CloudWatch Logs: Fargate application logs SNS Alerts: Error notifications CloudTrail: Governance \u0026amp; admin activity tracking 3.8 CI/CD Pipeline Developer Commit\n→ CodePipeline\n→ CodeBuild\n→ Build Docker Image\n→ ECR\n→ CodeDeploy\n→ ECS Fargate\nFeatures: Rolling deployments with zero downtime ALB health checks Automatic rollback on failure 4. Deployment Plan Phase Duration Deliverables 1 1 week Route53, CloudFront, WAF, VPC, ALB 2 3 weeks ECS, ECR, RDS, ElastiCache 3 1 week EventBridge, SQS, SNS 4 3 weeks Kinesis, S3, QuickSight 5 2 weeks CI/CD Pipeline 6 2 weeks Security hardening, cost tuning 5. Estimated Monthly Operational Cost Service Cost/Month CloudFront + Route53 + WAF $24 ECS Fargate $40 RDS $19 ElastiCache Redis $73 S3 + Kinesis $1 Monitoring $37 CI/CD $18 Total Estimated Cost $212 6. Expected Outcomes Stable 24/7 rail transit operations 1,000+ concurrent users supported Secure and automated payment processing Accurate passenger demand forecasting Reduced operation costs through autoscaling and CI/CD Appendix A — AWS Services Summary Category AWS Services Edge Route53, CloudFront, WAF Networking VPC, ALB Compute ECS Fargate, ECR Database RDS SQL Server, ElastiCache Event EventBridge, SNS, SQS Analytics Kinesis, S3, QuickSight, SageMaker Monitoring CloudWatch, CloudTrail CI/CD CodePipeline, CodeBuild, CodeDeploy "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Tạo một S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "Trong phần này, bạn sẽ tạo và kiểm tra Interface Endpoint S3 bằng cách sử dụng môi trường truyền thống mô phỏng.\nQuay lại Amazon VPC menu. Trong thanh điều hướng bên trái, chọn Endpoints, sau đó click Create Endpoint.\nTrong Create endpoint console:\nĐặt tên interface endpoint Trong Service category, chọn aws services Trong Search box, gõ S3 và nhấn Enter. Chọn endpoint có tên com.amazonaws.us-east-1.s3. Đảm bảo rằng cột Type có giá trị Interface. Đối với VPC, chọn VPC Cloud từ drop-down. Đảm bảo rằng bạn chọn \u0026ldquo;VPC Cloud\u0026rdquo; và không phải \u0026ldquo;VPC On-prem\u0026rdquo;\nMở rộng Additional settings và đảm bảo rằng Enable DNS name không được chọn (sẽ sử dụng điều này trong phần tiếp theo của workshop) Chọn 2 subnets trong AZs sau: us-east-1a and us-east-1b Đối với Security group, chọn SGforS3Endpoint: Giữ default policy - full access và click Create endpoint Chúc mừng bạn đã tạo thành công S3 interface endpoint. Ở bước tiếp theo, chúng ta sẽ kiểm tra interface endpoint.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.10-week10/",
	"title": "Worklog tuần 10",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tuần 10 Thực hành với các dịch vụ cơ bản của AWS (VPC, EC2, EBS, Postgres) Học AWS SageMaker cho workflow AI/ML Thực hành phân tích dữ liệu AI/ML với SageMaker Học các thành phần AWS Bedrock AgentCore Học AWS Glue và Amazon Athena cho ETL và phân tích dữ liệu Nhiệm vụ tuần này Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo 2 Thực hành: + Tạo VPCs + Khởi tạo EC2 instances + Tạo EBS volume + Gắn EBS volume + Cài đặt Postgres + Mount EBS trên EC2 production + Backup Postgres + Mount EBS trên EC2 test + Phục hồi dữ liệu 08/11/2025 10/11/2025 https://100000.awsstudygroup.com/ 3 Học AWS SageMaker AI: + SageMaker là gì + Các thành phần + End-to-End Workflow 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 4 Thực hành: Sử dụng SageMaker AI để phân tích dữ liệu, nhập file Excel, chọn kiểu dữ liệu, xuất kết quả bao gồm biểu đồ 10/11/2025 12/11/2025 https://000200.awsstudygroup.com/ 5 Học AWS Bedrock AgentCore: + Identity, Memory, Code Interpreter, Browser, Gateway, Observability + Trường hợp sử dụng: trang bị công cụ cho agent, triển khai an toàn ở quy mô lớn, kiểm tra và giám sát agent 12/11/2025 14/11/2025 https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html 6 Học AWS Glue và Amazon Athena 14/11/2025 16/11/2025 https://000040.awsstudygroup.com/ Thành tựu Tuần 10 1. Thực hành các dịch vụ cơ bản của AWS: Tạo VPCs với subnet, route table, Internet Gateway đầy đủ. Khởi tạo EC2 instances cho môi trường production và test. Tạo và gắn EBS volume cho EC2 instances. Cài đặt và cấu hình PostgreSQL trên EC2. Mount EBS trên cả production và test instances. Thực hiện backup và phục hồi PostgreSQL thành công. 2. Học \u0026amp; Thực hành AWS SageMaker AI: Hiểu các thành phần SageMaker: Notebook, Training, Endpoint, Model, Pipelines. Thực hiện workflow AI/ML từ đầu đến cuối: nhập dữ liệu Excel, phân tích dữ liệu, trực quan hóa biểu đồ. Chọn kiểu dữ liệu phù hợp, làm sạch dữ liệu và xuất kết quả cho các bước tiếp theo. 3. Học AWS Bedrock AgentCore: Hiểu các thành phần cốt lõi: Identity, Memory, Code Interpreter, Browser, Gateway, Observability. Khám phá các trường hợp sử dụng: trang bị công cụ cho agent, triển khai an toàn, giám sát và kiểm tra agent. 4. Học AWS Glue \u0026amp; Amazon Athena: Thực hành xây dựng pipeline ETL với Glue. Catalog dữ liệu và chuyển đổi dữ liệu để phân tích. Truy vấn dữ liệu trên S3 với Athena và kiểm tra kết quả. Kết nối kết quả truy vấn Athena với QuickSight để báo cáo và trực quan hóa. 5. Thành thạo AWS CLI \u0026amp; Management Console: Cấu hình AWS CLI với Access Key, Secret Key, và Default Region. Sử dụng CLI để lấy danh sách region, liệt kê dịch vụ, và kiểm tra cấu hình. Quản lý tài nguyên AWS bằng CLI và Console linh hoạt. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/1-worklog/1.12-week12/",
	"title": "Worklog Tuần 12",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 12: Kết nối, làm quen với các thành viên trong First Cloud Journey. Hiểu dịch vụ AWS cơ bản, cách dùng console \u0026amp; CLI. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Làm quen với các thành viên FCJ - Đọc và lưu ý các nội quy, quy định tại đơn vị thực tập 11/08/2025 11/08/2025 3 - Tìm hiểu AWS và các loại dịch vụ + Compute + Storage + Networking + Database + \u0026hellip; 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tạo AWS Free Tier account - Tìm hiểu AWS Console \u0026amp; AWS CLI - Thực hành: + Tạo AWS account + Cài AWS CLI \u0026amp; cấu hình + Cách sử dụng AWS CLI 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tìm hiểu EC2 cơ bản: + Instance types + AMI + EBS + \u0026hellip; - Các cách remote SSH vào EC2 - Tìm hiểu Elastic IP 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Thực hành: + Tạo EC2 instance + Kết nối SSH + Gắn EBS volume 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 12: Hiểu AWS là gì và nắm được các nhóm dịch vụ cơ bản:\nCompute Storage Networking Database \u0026hellip; Đã tạo và cấu hình AWS Free Tier account thành công.\nLàm quen với AWS Management Console và biết cách tìm, truy cập, sử dụng dịch vụ từ giao diện web.\nCài đặt và cấu hình AWS CLI trên máy tính bao gồm:\nAccess Key Secret Key Region mặc định \u0026hellip; Sử dụng AWS CLI để thực hiện các thao tác cơ bản như:\nKiểm tra thông tin tài khoản \u0026amp; cấu hình Lấy danh sách region Xem dịch vụ EC2 Tạo và quản lý key pair Kiểm tra thông tin dịch vụ đang chạy \u0026hellip; Có khả năng kết nối giữa giao diện web và CLI để quản lý tài nguyên AWS song song.\n\u0026hellip;\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/3-blogstranslated/",
	"title": "Các bài blogs đã dịch",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTại đây sẽ là phần liệt kê, giới thiệu các blogs mà các bạn đã dịch. Ví dụ:\nBlog 1 - Các dịch vụ AWS vươn lên tầm cao mới trong Prime Day 2025: các số liệu và cột mốc chính Prime Day 2025 lập kỷ lục mới nhờ sức mạnh công nghệ của AWS. Các trung tâm hoàn tất đơn hàng với hơn 7.000 robot ASRS được vận hành bởi AWS Outposts, xử lý hơn 524 triệu lệnh và đạt đỉnh 8 triệu lệnh/giờ (tăng 160% so với 2024). Nhờ hạ tầng AWS kết hợp kinh nghiệm bán lẻ của Amazon, khách hàng dễ dàng tìm ưu đãi, nhận thông tin sản phẩm nhanh chóng và được giao hàng trong ngày hoặc hôm sau.\nBlog 2 - AWS được công nhận là Nhà lãnh đạo trong báo cáo Gartner Magic Quadrant 2025 về Nền tảng Ứng dụng Cloud-Native và Quản lý Container Trong kỷ nguyên điện toán đám mây và trí tuệ nhân tạo bùng nổ, doanh nghiệp liên tục tìm kiếm nền tảng công nghệ giúp họ đổi mới nhanh hơn, vận hành linh hoạt hơn và tối ưu chi phí tốt hơn. Với hệ sinh thái dịch vụ toàn diện, AWS không chỉ đồng hành cùng hàng triệu khách hàng toàn cầu trong hành trình chuyển đổi số, mà còn tiếp tục khẳng định vị thế tiên phong khi được Gartner công nhận là Nhà lãnh đạo (Leader) trong nhiều hạng mục quan trọng của Magic Quadrant 2025.\nBlog 3 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 4 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 5 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 6 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Kiểm tra Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Lấy regional DNS name (tên DNS khu vực) của S3 interface endpoint Trong Amazon VPC menu, chọn Endpoints.\nClick tên của endpoint chúng ta mới tạo ở mục 4.2: s3-interface-endpoint. Click details và lưu lại regional DNS name của endpoint (cái đầu tiên) vào text-editor của bạn để dùng ở các bước sau.\nKết nối đến EC2 instance ở trong \u0026ldquo;VPC On-prem\u0026rdquo; (giả lập môi trường truyền thống) Đi đến Session manager bằng cách gõ \u0026ldquo;session manager\u0026rdquo; vào ô tìm kiếm\nClick Start Session, chọn EC2 instance có tên Test-Interface-Endpoint. EC2 instance này đang chạy trên \u0026ldquo;VPC On-prem\u0026rdquo; và sẽ được sử dụng để kiểm tra kết nối đến Amazon S3 thông qua Interface endpoint. Session Manager sẽ mở 1 browser tab mới với shell prompt: sh-4.2 $\nĐi đến ssm-user\u0026rsquo;s home directory với lệnh \u0026ldquo;cd ~\u0026rdquo;\nTạo 1 file tên testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file vào S3 bucket mình tạo ở section 4.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; Câu lệnh này yêu cầu thông số \u0026ndash;endpoint-url, bởi vì bạn cần sử dụng DNS name chỉ định cho endpoint để truy cập vào S3 thông qua Interface endpoint. Không lấy \u0026rsquo; * \u0026rsquo; khi copy/paste tên DNS khu vực. Cung cấp tên S3 bucket của bạn Bây giờ tệp đã được thêm vào bộ chứa S3 của bạn. Hãy kiểm tra bộ chứa S3 của bạn trong bước tiếp theo.\nKiểm tra Object trong S3 bucket Đi đến S3 console Click Buckets Click tên bucket của bạn và bạn sẽ thấy testfile2.xyz đã được thêm vào s3 bucket của bạn "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.3-s3-vpc/",
	"title": "Truy cập S3 từ VPC",
	"tags": [],
	"description": "",
	"content": "Sử dụng Gateway endpoint Trong phần này, bạn sẽ tạo một Gateway endpoint để truy cập Amazon S3 từ một EC2 instance. Gateway endpoint sẽ cho phép tải một object lên S3 bucket mà không cần sử dụng Internet Công cộng. Để tạo endpoint, bạn phải chỉ định VPC mà bạn muốn tạo endpoint và dịch vụ (trong trường hợp này là S3) mà bạn muốn thiết lập kết nối.\nNội dung Tạo gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/4-eventparticipated/",
	"title": "Các events đã tham gia",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTrong phần này, các bạn cần liệt kê và mô tả chi tiết các sự kiện (event) mà mình đã tham gia trong suốt quá trình thực tập hoặc làm việc.\nMỗi sự kiện nên được trình bày theo định dạng Event 1, Event 2, Event 3…, kèm theo các thông tin:\nTên sự kiện Thời gian tổ chức Địa điểm (nếu có) Vai trò của bạn trong sự kiện (người tham dự, hỗ trợ tổ chức, diễn giả, v.v.) Mô tả ngắn gọn nội dung và hoạt động chính trong sự kiện Kết quả hoặc giá trị đạt được (bài học, kỹ năng mới, đóng góp cho nhóm/dự án) Việc liệt kê này giúp thể hiện rõ sự tham gia thực tế của bạn, cũng như các kỹ năng mềm và kinh nghiệm bạn đã tích lũy qua từng sự kiện. Trong quá trình thực tập, em đã tham gia 2 events, với mỗi event là một trải nghiệm đáng nhớ với những kiến thức mới, hay và bổ ích, cùng với đó là nhứng món quà và những khoảnh khắc rất tuyệt vời.\nEvent 1 Tên sự kiện: GenAI-powered App-DB Modernization workshop\nThời gian: 09:00 ngày 13/08/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 2 Tên sự kiện: GenAI-powered App-DB Modernization workshop\nThời gian: 09:00 ngày 13/08/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "Mô phỏng On-premises DNS ",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoint có một địa chỉ IP cố định trong từng AZ nơi chúng được triển khai, trong suốt thời gian tồn tại của endpoint (cho đến khi endpoint bị xóa). Các địa chỉ IP này được gắn vào Elastic network interface (ENI). AWS khuyến nghị sử dụng DNS để resolve địa chỉ IP cho endpoint để các ứng dụng downstream sử dụng địa chỉ IP mới nhất khi ENIs được thêm vào AZ mới hoặc bị xóa theo thời gian.\nTrong phần này, bạn sẽ tạo một quy tắc chuyển tiếp (forwarding rule) để gửi các yêu cầu resolve DNS từ môi trường truyền thống (mô phỏng) đến Private Hosted Zone trên Route 53. Phần này tận dụng cơ sở hạ tầng do CloudFormation triển khai trong phần Chuẩn bị môi trường.\nTạo DNS Alias Records cho Interface endpoint Click link để đi đến Route 53 management console (Hosted Zones section). Mẫu CloudFormation mà bạn triển khai trong phần Chuẩn bị môi trường đã tạo Private Hosted Zone này. Nhấp vào tên của Private Hosted Zone, s3.us-east-1.amazonaws.com: Tạo 1 record mới trong Private Hosted Zone: Giữ nguyên Record name và record type Alias Button: click để enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Chọn endpoint: Paste tên (Regional VPC Endpoint DNS) bạn đã lưu lại ở phần 4.3 Click Add another record, và add 1 cái record thứ 2 sử dụng những thông số sau: Record name: *. Record type: giữ giá trị default (type A) Alias Button: Click để enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Chọn endpoint: Paste tên (Regional VPC Endpoint DNS) bạn đã lưu lại ở phần 4.3 Click Create records Record mới xuất hiện trên giao diện Route 53.\nTạo một Resolver Forwarding Rule Route 53 Resolver Forwarding Rules cho phép bạn chuyển tiếp các DNS queries từ VPC của bạn đến các nguồn khác để resolve name. Bên ngoài môi trường workshop, bạn có thể sử dụng tính năng này để chuyển tiếp các DNS queries từ VPC của bạn đến các máy chủ DNS chạy trên on-premises. Trong phần này, bạn sẽ mô phỏng một on-premises conditional forwarder bằng cách tạo một forwarding rule để chuyển tiếp các DNS queries for Amazon S3 đến một Private Hosted Zone chạy trong \u0026ldquo;VPC Cloud\u0026rdquo; để resolve PrivateLink interface endpoint regional DNS name.\nTừ giao diện Route 53, chọn Inbound endpoints trên thanh bên trái\nTrong giao diện Inbound endpoint, Chọn ID của Inbound endpoint.\nSao chép 2 địa chỉ IP trong danh sách vào trình chỉnh sửa. Từ giao diện Route 53, chọn Resolver \u0026gt; Rules và chọn Create rule Trong giao diện Create rule Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: điền cả hai IP bạn đã lưu trữ trên trình soạn thảo (inbound endpoint addresses) và sau đó chọn Submit Bạn đã tạo thành công resolver forwarding rule.\nKiểm tra on-premises DNS mô phỏng. Kết nối đến Test-Interface-Endpoint EC2 instance với Session Manager Kiểm tra DNS resolution. Lệnh dig sẽ trả về địa chỉ IP được gán cho VPC endpoint interface đang chạy trên VPC (địa chỉ IP của bạn sẽ khác): dig +short s3.us-east-1.amazonaws.com Các địa chỉ IP được trả về là các địa chỉ IP VPC enpoint, KHÔNG phải là các địa chỉ IP Resolver mà bạn đã dán từ trình chỉnh sửa văn bản của mình. Các địa chỉ IP của Resolver endpoint và VPC endpoin trông giống nhau vì chúng đều từ khối CIDR VPC Cloud.\nTruy cập vào menu VPC (phần Endpoints), chọn S3 interface endpoint. Nhấp vào tab Subnets và xác nhận rằng các địa chỉ IP được trả về bởi lệnh Dig khớp với VPC endpoint: Hãy quay lại shell của bạn và sử dụng AWS CLI để kiểm tra danh sách các bucket S3 của bạn: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Kết thúc phiên làm việc của Session Manager của bạn: Trong phần này, bạn đã tạo một Interface Endpoint cho Amazon S3. Điểm cuối này có thể được truy cập từ on-premises thông qua Site-to-Site VPN hoặc AWS Direct Connect. Các điểm cuối Route 53 Resolver outbound giả lập chuyển tiếp các yêu cầu DNS từ on-premises đến một Private Hosted Zone đang chạy trên đám mây. Các điểm cuối Route 53 inbound nhận yêu cầu giải quyết và trả về một phản hồi chứa địa chỉ IP của Interface Endpoint VPC. Sử dụng DNS để giải quyết các địa chỉ IP của điểm cuối cung cấp tính sẵn sàng cao trong trường hợp một Availability Zone gặp sự cố.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.4-s3-onprem/",
	"title": "Truy cập S3 từ môi trường truyền thống",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần này, bạn sẽ tạo một Interface Endpoint để truy cập Amazon S3 từ môi trường truyền thống mô phỏng. Interface Endpoint sẽ cho phép bạn định tuyến đến Amazon S3 qua kết nối VPN từ môi trường truyền thống mô phỏng của bạn.\nTại sao nên sử dụng Interface Endpoint:\nCác Gateway endpoints chỉ hoạt động với các tài nguyên đang chạy trong VPC nơi chúng được tạo. Interface Endpoint hoạt động với tài nguyên chạy trong VPC và cả tài nguyên chạy trong môi trường truyền thống. Khả năng kết nối từ môi trường truyền thống của bạn với aws cloud có thể được cung cấp bởi AWS Site-to-Site VPN hoặc AWS Direct Connect. Interface Endpoint cho phép bạn kết nối với các dịch vụ do AWS PrivateLink cung cấp. Các dịch vụ này bao gồm một số dịch vụ AWS, dịch vụ do các đối tác và khách hàng AWS lưu trữ trong VPC của riêng họ (gọi tắt là Dịch vụ PrivateLink endpoints) và các dịch vụ Đối tác AWS Marketplace. Đối với workshop này, chúng ta sẽ tập trung vào việc kết nối với Amazon S3. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "Khi bạn tạo một Interface Endpoint hoặc cổng, bạn có thể đính kèm một chính sách điểm cuối để kiểm soát quyền truy cập vào dịch vụ mà bạn đang kết nối. Chính sách VPC Endpoint là chính sách tài nguyên IAM mà bạn đính kèm vào điểm cuối. Nếu bạn không đính kèm chính sách khi tạo điểm cuối, thì AWS sẽ đính kèm chính sách mặc định cho bạn để cho phép toàn quyền truy cập vào dịch vụ thông qua điểm cuối.\nBạn có thể tạo chính sách chỉ hạn chế quyền truy cập vào các S3 bucket cụ thể. Điều này hữu ích nếu bạn chỉ muốn một số Bộ chứa S3 nhất định có thể truy cập được thông qua điểm cuối.\nTrong phần này, bạn sẽ tạo chính sách VPC Endpoint hạn chế quyền truy cập vào S3 bucket được chỉ định trong chính sách VPC Endpoint.\nKết nối tới EC2 và xác minh kết nối tới S3. Bắt đầu một phiên AWS Session Manager mới trên máy chủ có tên là Test-Gateway-Endpoint. Từ phiên này, xác minh rằng bạn có thể liệt kê nội dung của bucket mà bạn đã tạo trong Phần 1: Truy cập S3 từ VPC. aws s3 ls s3://\u0026lt;your-bucket-name\u0026gt; Nội dung của bucket bao gồm hai tệp có dung lượng 1GB đã được tải lên trước đó.\nTạo một bucket S3 mới; tuân thủ mẫu đặt tên mà bạn đã sử dụng trong Phần 1, nhưng thêm \u0026lsquo;-2\u0026rsquo; vào tên. Để các trường khác là mặc định và nhấp vào Create. Tạo bucket thành công. Policy mặc định cho phép truy cập vào tất cả các S3 Buckets thông qua VPC endpoint.\nTrong giao diện Edit Policy, sao chép và dán theo policy sau, thay thế yourbucketname-2 với tên bucket thứ hai của bạn. Policy này sẽ cho phép truy cập đến bucket mới thông qua VPC endpoint, nhưng không cho phép truy cập đến các bucket còn lại. Chọn Save để kích hoạt policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Cấu hình policy thành công.\nTừ session của bạn trên Test-Gateway-Endpoint instance, kiểm tra truy cập đến S3 bucket bạn tạo ở bước đầu aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; Câu lệnh trả về lỗi bởi vì truy cập vào S3 bucket không có quyền trong VPC endpoint policy.\nTrở lại home directory của bạn trên EC2 instance cd~ Tạo file fallocate -l 1G test-bucket2.xyz Sao chép file lên bucket thứ 2 aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; Thao tác này được cho phép bởi VPC endpoint policy.\nSau đó chúng ta kiểm tra truy cập vào S3 bucket đầu tiên\naws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt;\nCâu lệnh xảy ra lỗi bởi vì bucket không có quyền truy cập bởi VPC endpoint policy.\nTrong phần này, bạn đã tạo chính sách VPC Endpoint cho Amazon S3 và sử dụng AWS CLI để kiểm tra chính sách. Các hoạt động AWS CLI liên quan đến bucket S3 ban đầu của bạn thất bại vì bạn áp dụng một chính sách chỉ cho phép truy cập đến bucket thứ hai mà bạn đã tạo. Các hoạt động AWS CLI nhắm vào bucket thứ hai của bạn thành công vì chính sách cho phép chúng. Những chính sách này có thể hữu ích trong các tình huống khi bạn cần kiểm soát quyền truy cập vào tài nguyên thông qua VPC Endpoint.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nĐảm bảo truy cập Hybrid an toàn đến S3 bằng cách sử dụng VPC endpoint Tổng quan AWS PrivateLink cung cấp kết nối riêng tư đến các dịch vụ aws từ VPCs hoặc trung tâm dữ liệu (on-premise) mà không làm lộ lưu lượng truy cập ra ngoài public internet.\nTrong bài lab này, chúng ta sẽ học cách tạo, cấu hình, và kiểm tra VPC endpoints để cho phép workload của bạn tiếp cận các dịch vụ AWS mà không cần đi qua Internet công cộng.\nChúng ta sẽ tạo hai loại endpoints để truy cập đến Amazon S3: gateway vpc endpoint và interface vpc endpoint. Hai loại vpc endpoints này mang đến nhiều lợi ích tùy thuộc vào việc bạn truy cập đến S3 từ môi trường cloud hay từ trung tâm dữ liệu (on-premise).\nGateway - Tạo gateway endpoint để gửi lưu lượng đến Amazon S3 hoặc DynamoDB using private IP addresses. Bạn điều hướng lưu lượng từ VPC của bạn đến gateway endpoint bằng các bảng định tuyến (route tables) Interface - Tạo interface endpoint để gửi lưu lượng đến các dịch vụ điểm cuối (endpoints) sử dụng Network Load Balancer để phân phối lưu lượng. Lưu lượng dành cho dịch vụ điểm cuối được resolved bằng DNS. Nội dung Tổng quan về workshop Chuẩn bị Truy cập đến S3 từ VPC Truy cập đến S3 từ TTDL On-premises VPC Endpoint Policies (làm thêm) Dọn dẹp tài nguyên "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/5-workshop/5.6-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Dọn dẹp tài nguyên Xin chúc mừng bạn đã hoàn thành xong lab này! Trong lab này, bạn đã học về các mô hình kiến trúc để truy cập Amazon S3 mà không sử dụng Public Internet.\nBằng cách tạo Gateway endpoint, bạn đã cho phép giao tiếp trực tiếp giữa các tài nguyên EC2 và Amazon S3, mà không đi qua Internet Gateway. Bằng cách tạo Interface endpoint, bạn đã mở rộng kết nối S3 đến các tài nguyên chạy trên trung tâm dữ liệu trên chỗ của bạn thông qua AWS Site-to-Site VPN hoặc Direct Connect. Dọn dẹp Điều hướng đến Hosted Zones trên phía trái của bảng điều khiển Route 53. Nhấp vào tên của s3.us-east-1.amazonaws.com zone. Nhấp vào Delete và xác nhận việc xóa bằng cách nhập từ khóa \u0026ldquo;delete\u0026rdquo;. Disassociate Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. 4.Mở console của CloudFormation và xóa hai stack CloudFormation mà bạn đã tạo cho bài thực hành này:\nPLOnpremSetup PLCloudSetup Xóa các S3 bucket Mở bảng điều khiển S3 Chọn bucket chúng ta đã tạo cho lab, nhấp chuột và xác nhận là empty. Nhấp Delete và xác nhận delete. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/6-self-evaluation/",
	"title": "Tự đánh giá",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTrong suốt thời gian thực tập tại [Tên công ty/tổ chức] từ [ngày bắt đầu] đến [ngày kết thúc], tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\nTôi đã tham gia [mô tả ngắn gọn dự án hoặc công việc chính], qua đó cải thiện kỹ năng [liệt kê kỹ năng: lập trình, phân tích, viết báo cáo, giao tiếp…].\nVề tác phong, tôi luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, tôi xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ✅ ☐ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ☐ ✅ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ✅ ☐ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ☐ ☐ ✅ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ☐ ✅ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ☐ ✅ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ✅ ☐ ☐ Cần cải thiện Nâng cao tính kỹ luật, chấp hành nghiêm chỉnh nội quy của công ty hoặc bất kỳ trong một tổ chức nào Cải thiện trong cách tư duy giải quyết vấn đề Học cách giao tiếp tốt hơn trong giao tiếp hằng ngày và trong công việc, xử lý tình huống "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/7-feedback/",
	"title": "Chia sẻ, đóng góp ý kiến",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTại đây bạn có thể tự do đóng góp ý kiến cá nhân về những trải nghiệm khi tham gia chương trình First Cloud Journey, giúp team FCJ cải thiện những vấn đề còn thiếu sót dựa trên các hạng mục sau:\nĐánh giá chung 1. Môi trường làm việc\nMôi trường làm việc rất thân thiện và cởi mở. Các thành viên trong FCJ luôn sẵn sàng hỗ trợ khi mình gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn. Tuy nhiên, mình nghĩ có thể bổ sung thêm một số buổi giao lưu hoặc team bonding để mọi người hiểu nhau hơn.\n2. Sự hỗ trợ của mentor / team admin\nMentor hướng dẫn rất chi tiết, giải thích rõ ràng khi mình chưa hiểu và luôn khuyến khích mình đặt câu hỏi. Team admin hỗ trợ các thủ tục, tài liệu và tạo điều kiện để mình làm việc thuận lợi. Mình đánh giá cao việc mentor cho phép mình thử và tự xử lý vấn đề thay vì chỉ đưa đáp án.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCông việc mình được giao phù hợp với kiến thức mình đã học ở trường, đồng thời mở rộng thêm những mảng mới mà mình chưa từng được tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, kỹ năng làm việc nhóm, và cả cách giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp mình định hướng tốt hơn cho sự nghiệp.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty rất tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí. Điều này giúp mình cảm thấy mình là một phần của tập thể, dù chỉ là thực tập sinh.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty có hỗ trợ phụ cấp thực tập và tạo điều kiện về thời gian linh hoạt khi cần thiết. Ngoài ra, việc được tham gia các buổi đào tạo nội bộ là một điểm cộng lớn.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập? Điều bạn nghĩ công ty cần cải thiện cho các thực tập sinh sau? Nếu giới thiệu cho bạn bè, bạn có khuyên họ thực tập ở đây không? Vì sao? Đề xuất \u0026amp; mong muốn Bạn có đề xuất gì để cải thiện trải nghiệm trong kỳ thực tập? Bạn có muốn tiếp tục chương trình này trong tương lai? Góp ý khác (tự do chia sẻ): "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]